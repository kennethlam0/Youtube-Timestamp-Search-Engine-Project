Training and Testing an Italian BERT - Transformers From Scratch #4
Hi welcom to the video So thi is the fourth video in a Transform from Scratch mini seri So if you been follow along essenti cover what you can see on the screen So we got some data We built a token with it And then set up our input pipelin readi to begin actual train our model which is what go to cover in thi video So move over to the code And we see here that we have essenti everyth done so far So built our input data our input pipelin And now at a point where we have a data loader PyTorch data loader readi And we can begin train a model with it So there ar a few thing to be awar of So I mean first just have a quick look at the structur of our data So when train a model for mass languag model we need a few tensor We need three tensor And thi is for train Roberta by the wai as well Same thing with Bert as well We have our input ID attent mask and our label Our input ID have roughli of their valu mask So we can see that here we have these two tensor These ar the label And we have the real token in here the token ID And then in our input ID tensor we have these be replac with mask token the number four So the structur of our input data creat a Torch data set from it and us that to creat a Torch data loader And with that we can actual begin set up our model for train So there ar a few thing to that We just begin train straight awai So the first thing that we need to do is creat a Roberta config object And the config object is someth that we us when initi a transform from scratch in order to initi it with a certain set of paramet So do that first So we want from transform import Roberta config And to creat that config object we do thi So we do Roberta config And then in here we need to specifi differ paramet Now on of the main on is the voc up size Now thi need to match to whichev voc up size we have alreadi creat in our token when initi it In our token when build our token So I mean for me if I go all the wai up here to here thi is where I creat the token I can see OK thi number here So So go to set that But if you have that you can just write token voc up size So here And that will return your voc up size So I mean replac that do thi Now as well as that we want to also set thi So max posit embed And thi need to be set to your max length plu two in thi case So max length is set up here So where is it Max length here Plu two becaus we have these ad special token If we do that end up with a index error becaus go beyond the embed limit Now we want our hidden size So thi is the size of the vector that our embed layer within Roberta will creat So each token so we have or token And each on of those will be sign a vector of size Thi is the typic number So the origin came from the BERT base model Then we set up the architectur of the intern of the model So we want the number of attent head which go to set to And also the number of hidden layer which I So the default for thi is for Roberta But go to go with six for the sake of keep train time a littl shorter Now we also need to add type vocab size which is just on So the differ token type that we have We just have on need to worri about that OK so our configur object readi And we can import and initi a Roberta model with that So we went from transform Thi is kind of similar to what we usual do Import Roberta And do thi for mass LL So MLM right So train us MLM So we want Roberta for mass LL And we initi our model us that Roberta for mass LL object And we just pass in our config And thi will right there initi our Roberta model So a plain Roberta model randomli initi weight and so on And now we can move on to set up everyth for train So we have our model Now we need to prepar a few thing befor we train it First thing is we need to decid which devic go to be train on So whether CPU or a CUDA-enabl GPU And to figur out if we have that we write Well we can write torch CUDA is avail So write thi And for me it is So the typic wai that you would decid whether us CUDA or CPU or the typic line of code that will decid it for you is you write devic and you do torchCUDA or torch devic sorri And then you write CUDA insid here If avail otherwis we ar go to us torch devic CPU Now CPU take Yeah just take a realli long time So if you ar us CPU you have to leav it overnight for sure Mayb even longer Even if just like a littl bit of data it take so long So but hopefulli you have a GPU If not just go to have to be patient all Or if you could mayb try and us Googl Colab but you have to us a premium version becaus otherwis just go to shut off after like an hour or two I know I realli us it So I know how long it will train for befor just decid that done And the GPU is also not that good anywai So yeah Howev you can do it And then after that we want to move our model to our devic So whether GPU or CPU we move over there go to get a realli big output now So just our model So thi is like the structur of our model So we can see a few interest thing got Roberta for MLM We have the Roberta model And then insid that we have our embed And then we have our Did I sai I think it wa six Six encod should be Yeah So it goe from to our six And then we have the output here And then our final bit which is the languag model head the MLM head So cool Now we need our optim So from transform import AdamW which is Adam with weight and decai And what go to do is just go to activ the train mode of our model go to give us load of output again So just yeah You know what Mayb I can just just remov that There we go Easier And then our optim is go to be AdamW We need to pass in our model paramet And we need a learn rate So from I mean I usual us Roberta But look onlin thi look like a reason learn rate I think you can go from sort of here to I think from what I rememb down to like here the sort of typic rang But obvious go to depend on how much data you have And we want to do that How much data you have and load of differ thing right So what go to go with And that should be pretti much it So our setup Now just go to creat our train loop Now for the train loop we want to import TQDM So we can see how far through we ar go to train for two epoch And go to initi our loop object us TQDM So TQDM We have our data loader What is the name of that data loader not sure Data loader Cool Data loader And we set leav equal true But I need that Sorri I need that in the same style So for batch in loop And then here we run through each of the step that go to perform for everi singl train loop So the first thing we do is initi the gradient in our optim So zero grad So the reason we do thi is after the first loop our optim is go to be assign a set of gradient which go to us to optim our model And on the next loop we want those residu gradient to still be there in our optim We want to essenti reset it for the next loop So what do here Then we want our tensor So we have input ID And that is go to be batch input ID And we also want to move that over to our GPU or CPU if on that And thi is pretti much the same for our three So mask label And thi is just attent mask So extract our tensor And we just need to feed them into our model now So go to get output from the model We just do model Input ID attent mask which is go to be equal to mask And our label equal to label So everyth ha been fed into our model We have our output Now we need to extract a few thing from the output So we need the loss So we write loss equal outputsloss And from that we want to calcul all of the differ paramet in our model We need to calcul the loss for each on of those paramet So we do thi lossbackward to backpropag through all of those differ valu and get that loss After done that we us our optim take a step and optim all those paramet base on that loss Then everyth we need to train the model And just a few thing So for the progress bar I just want a littl bit of inform there just so I know go on And I just write loop set descript And I just want to print out the epoch So write that And then I want to set the postfix as well So loopset postfix And here I just want to see the loss So just do loss item like that So that should be everyth Yeah run that see what happen Hopefulli it should work No it work OK Let me see Oh no a CUDA error So probabl just need to refresh everyth I hate CUDA error One moment OK so final figur it out Took so long So a few tip anywai When you do get a CUDA error switch your devic to CPU and then re-run everyth And you should get a more understand error So if we come down here chang it to CPU We see that we get an index error Scroll down index out of rang itself So the reason for thi is so you get thi error if you add the extra two token onto the end of here But we ad them So I wa pretti confus about that And then it took me a realli long time to realiz that thi argument is wrong And there should be an S on the end So that wa the error So yeah super super cool That wa liter it It took me so long to figur that out But now we have it good We just need to run everyth again So just go to run through everyth Remov thi cell here where I chang it to CPU becaus I need it now And just re-execut all that OK so back And finish train our model now Now it ha taken a long time a few dai later And I made a few chang dure train as well So thi definit the cleanest train process becaus I wa kind of updat paramet as it wa go along So initi well first train for like three and a bit epoch And train on the full data set as well So if I come up here I think do I print out how much data it wa Mayb in anoth file So if we come down here so yeah a lot more data here So we have no Let me think million OK so million sampl in that final run And initi when we start train we start with a learn rate of to the minu Now I look into thi a littl bit And it just wa not realli move And show you in a minut So for the second epoch I move it down to to the minu Or move it up sorri to to the minu And that move start move thing a lot quicker So that wa good And then in total like I said it wa three and a bit epoch Other than that I realli chang anyth The onli thing I did wa I train for on epoch at a time becaus I want to see how the result were look after each epoch And that wa quit interest So let me show you that OK so thi is after the first epoch So here what do is got thi fill which is a pipelin fill object And enter chow and then put in now mass and then va And I want to sai chow come here va And in the middl I have to predict come Now thi is after the first epoch And we can see not yeah just put like random charact So question mark here three dot here chow and chow again here Kind of weird So yeah not the best right Then we move on to the second epoch And get well still rubbish At least got word So like here we have a word chow kiva or chiva Kek oh chow kiva I know if the wai I alwai the C-H in Italian I alwai get mess up If ani Italian watch sorri Chow kiva At least get word But none of these so it make ani sens OK so no still not good Now if we come across again so thi is thi on Yeah thi on Now we get it So the rest of them ar nonsens OK so the four here ignor them Howev at the top we get thi score of And we get chow kiva So what we want So good It mean work Thi wa after the third and a bit epoch Let me show you the loss function as well So thi I know thi is realli messi So here we have our I know why thi so short actual Why is that on so short Strang mayb I Yeah for the last on it look like I finish train for the full epoch So I thought I did Mayb someth happen not sure But fine what it is fine So the first set of train I did wa here And you see in the middl my comput went to sleep for a bit overnight becaus it wa just so loud So I turn it off for a bit And then continu go down Now thi first epoch is when we were at or to the minu And then here I wa test the to the minu And you can see straight awai it goe down wai quicker So I wa like OK go to go with that clearli a lot better And then continu over here the next epoch and then the final on here which seem to chang much anywai But there wa still a pretti clear differ So the loss over time And yeah I mean seen the result from that So now we have that move on to actual test the model So go to bring Lara in go to just open the file OK so thi is the test go to do So us the file mask got thi pipelin Sorri fill mask got thi pipelin And what go to do is just get Lara to come in and some Italian sentenc and just add thi random mask token in and see if the result ar bearabl or not So see So I will see you in a minut Thi is Lara She can speak Italian So go to go through thi and test it a few time and hopefulli sai good see Hopefulli Ciao OK so all you need to do is we have a sentenc here And you just write some Italian And then for on of the word in there we want to replac it with thi text here And then go to mask that word And then the model is go to try and predict what is there And hopefulli it will predict see So just write some Italian phrase Not too difficult yet And see So I have to write ciao va No no no no You write Just write a sentenc And do that Buongiorno gent No buongiorno Mayb a few word there Can I put comma or dot Yeah you can put comma Buongiorno come va OK And then so which word should we cover Come OK And then OK so just cover it with the mask And see what it sai So not thi I seem to read on these as well OK so give it a moment Chi va Yeah but the second on come va Come va Almost there Doe chi va mean anyth Like who Yeah like is there someon But I understand becaus Italian but I think that we usual sai that I think go to take that as fine go to take that as good So do it again mayb Yeah Try anoth on Oh wait actual what about these on no definit not right No No No OK But just like after buongiorno I expect Chi va Chi va Yeah OK So you can just put anoth on like where we put Phil again You can write anoth sentenc So here Yeah So we can write Yeah Yeah and then what do you want to replac Incontriamo mayb Or dove as well OK Yeah So which on You decid fine Incontriamo is interest OK try Yeah yeah Oh look at that Yeah good Dove ci vediamo oggi pomeriggio Dove ci incontriamo oggi pomeriggio Dove ci siamo oggi pomeriggio No No Dove ci troviamo oggi pomeriggio Dove ci ritroviamo Yeah quit good Yeah OK Should we try with dove Like us the same phrase Yeah yeah Try You can do it OK You can control Z right Yeah OK run it Dove the second on Come ci incontriamo quando ci incontriamo is good Si cool Yeah cosa se try anoth on Yeah Yeah Cosa pripar che cena stacera OK remov pripar Yeah Control Z Yeah Go Run it Yeah Cosa fare per cena stacera good Cosa serv per cena stacera yeah Cosa aspetta per cena stacera ni No Cosa succed per cena stacera ni Cosa veder per cena stacera ni So I find what we said befor which wa cosa pripar per cena stacera Yeah But doe it make sens Yeah it make sens Cosa fare per cena stacera cosa serv per cena stacera si Try someth hard like grammat difficult Mm Cosa succederebb Is it hard No think I when like that you know I know like come to my mind Cosa se avess scelto un altro giorno Okai Yeah And then what should we replac Avessimo Avessimo Avess How do you sai Avessimo Si Avessimo Avessimo What doe that mean If we had Avessi avessi avessero Not not the same on No but good becaus avessimo for third person plural like we had Avessi is third person singular So if he had or she had Yeah Chosen someth differ What doe thi actual mean cosa So what would have happen if we had chosen anoth dai Ah So the first on se avess scelto it will be the third person Uh-huh Se avess scelto it will be the first person So if I had chosen anoth dai Uh-huh Se avessero second person plural So it will be if thei had chosen anoth dai Uh-huh Se av Thi on no Se av Yeah Thi is good se veniss scelto as well Uh-huh No mayb no But the first three ar veri good Yeah I have an idea So now if we chang to se So if we put se loro so if we specifi the person mayb take the correct on So if we put se loro And then we expect it to sai avessemo Avesero Avesero Avesero So run it Avesero You see cool veri good And then the other on se loro anno is right Ave anno I mean sai Well the verb is incorrect but Yeah in the wrong place but sai the right Like the mean is correct but the grammar not correct Okai Right okai Yeah cool welcom Not happi it actual work becaus I sure if I could just Well it work with ciao come back but that wa all I test it with so I wa a littl bit worri that it might not do anyth els But thank you welcom Bye Okai so I think a pretti good result So I mean pretti much everyth we need for build our model our transform model Although I do want to so go to do on more video after thi where go to upload our model to the Hug Face model hub And then what be abl to do is actual download it directli from Hug Face which I think will be super cool to do that and figur out how we actual put all that togeth So yeah I think good result pretti happi with that And thank you for watch 
Choosing Indexes for Similarity Search (Faiss in Python)
Hi welcom to the video go to take you through a few differ index in FIAS todai So FIAS for similar search And go to learn how we can decid which index to us base on our data Now these index ar reason complex but go to just have a high level look at each on of them At some point in the futur go into more depth for sure But for now thi is what go to do So go to cover the index that you see on the screen at the moment So we have the flat index which ar just the plain and simpl noth special go on there And then go to have a look at LSH or local sensit hash HNSW which is hierarch navig small world And then final go to have a look at an IVF index as well So first thing go to show you is how to get some data for follow through thi So go to be us the SIFT dataset which is on million vector that we can us for test similar Now a littl bit of code so just go to show it to you So we have here just download the code be a notebook for thi in the descript as well So you can just us that and copi thing across But download it from here and thi will give us a tar file So we download that And then here all do is extract all the file from insid that tar file And then here read everyth into the notebook So insid that tar file get these FVEX file and we have to open them in a certain wai which is what do here So set up the function to read them sorri here And then here read in two file So we get a few differ file here So sorri thi should be SIFT So we get the base data which is go to be the data that go to search through And then we also have queri data here And then what do here is just select a singl queri a singl vector to queri with rather than all of them becaus we get quit a few in there And then here we can just see So thi is our queri vector that get Q And then we also have WB here which is go to be the data that index and search through And we can see some of it there as well So how we get data move on to some flat index So what you can see at the moment is a sort of a visual represent of a flat index Now up here thi is what do So calcul we have all these point So these ar all of the WB point that we saw befor And thi is our queri vector And we just calcul the distanc between all of those And then what we do is just take the top three So the top K in realiti but in thi case top three Now we also have IP So we have both distanc and IP distanc as well IP work in a differ wai So us a differ format to actual calcul the distanc or similar there So not exactli as you see it here But befor we write ani code just want to sai with flat index thei ar qualiti And typic what we want to do with and similar search index is balanc the search qualiti versu the search speed Higher search qualiti usual slower search speed And flat index ar just pure search qualiti becaus thei ar an exhaust search Thei check the distanc between your queri vector and everi other vector in the index which is fine if you have a particularli big data set or you care about time But if you do then you probabl want to us that becaus it can take an incred long time If you have a billion vector in your data set and you do queri a minut then as far as I know imposs to run that And if you were go to run that need some pretti insan hardwar So we us flat index and exhaust search in most case But I will show you how to do it So first just go to defin dimension of our data which is which we can see up here also go to sai how mani so how mani result do we want to return go to sai Okai We also need to import befor we do anyth And then we can initi our index So I said we have two So we have index flat or IP go to us IP becaus veri slightli faster It seem from me test it veri slightli faster but hardli ani differ in realiti So initi our index and then we want to add our data to it So we add WB and then we perform a search So let me creat a new cell and let me just run thi quickli Okai And what go to do is just time it so you can see how long thi take as well So go to do time and go to do index or sorri DI equal index search And in here we have our queri vector and how mani sampl like to return So go to go with K Okai So that wa reason quick and becaus we have a huge data set and just search for on queri So not realli too much of a problem there But what I do want to show you is so if we print out I that return all of the ID or the index of the most similar vector Now go to us that as a baselin for each of our other index So thi is like I said qualiti and we can us thi accuraci to test out other index as well So what go to do is take that and convert it into a list And if we just have a look at what we get we see that we get a list like that And just go to us that like I said to see how our other index ar perform So move on to the other index And like I said befor we want to try and go from thi which is the flat index which is search qualiti to someth more But it depend on our us case as well Sometim we might want more speed sometim higher qualiti So we will see a few of those through these index So we start with LSH So a veri high level LSH work by group vector in two differ bucket Now what we can see on the screen now is a typic hash function for like a Python dictionari And what these hash function do is thei try to minim collis So collis is where we would have the case of two item mayb sai these two be hash into the same bucket And with a dictionari you want that becaus you want everi bucket to be an independ valu Otherwis it increas the complex of extract your valu from a singl bucket if collid Now LSH is slightli differ becaus we actual do want to group thing So we can see it as a dictionari But rather than where befor we were avoid those collis you can see here put them into complet differ bucket everi time Rather than do that try to maxim collis So you can see here that push all three of these kei into thi singl bucket here And also push all of these kei into thi singl bucket So we get group of our valu Now when it come to perform our search we process our queri through the same hash function and that will push it to on of our bucket Now in the case of mayb appear in thi bucket here we us Ham Distanc to find the nearest bucket and then we can search or we restrict our scope to these valu So we just restrict our scope there which mean that we do not need to search through everyth We ar avoid search through those valu down there Now have a look at how we implement that So pretti straightforward All we do is index we do vise index LSH We have our dimension and then we also have thi other variabl which is call nbit So I will put that in a variabl up here Do nbit and what go to do is go to make it d multipli by So nbit we will have to scale with the dimension of our data which come into anoth problem which mention later on which is the curs of dimension But talk more about it in a moment So here we have nbit and then we add our data like we did befor and then we can search our data just like we did befor So time and we do d pi equal index search and we ar search us our queri our search queri and we want to return item So quicker speed see here And what we can also do is compar the result to our qualiti index or flat index and we do that us numpi in baselin i Okai so just go to look at it visual here so we can see we have quit a lot of match here So plenti of true coupl of fals true fals fals fals So these ar the top that have been return us our LSH algorithm and check if thei exist in the baselin result that we got from our flat index earlier and return that most of them ar present in that baselin So most of them do match so a reason good recal there So good and it wa faster So got millisecond here How much did we get up here We got millisecond So slightli less accur but what is that time faster so pretti good And we can mess around with n bit We can increas it to increas the accuraci of our index or we decreas it to increas the speed So again just try to find that balanc between them both Okai so thi is a graph just show you the recal with differ n bit valu So as we sort of saw befor we increas the n bit valu for good recal but at the same time we have that curs of dimension So if we ar multipli our dimension valu d by in order to get a good recal then if we have a dimension of not a veri high number so go to be reason fast but if we increas that to dimension for exampl that becom veri veri complex veri quickli So you have to be care with your dimension Lower dimension is veri good for LSH otherwis not so good You can see that here so at the bottom here us thi is on the same data set so an n bit valu of d multipli by with LSH super fast faster than our flat index which is what you would hope but if we increas the n bit valu quit a bit so mayb you want veri high perform then it get out of hand veri quickli and our search time just grow massiv So you kind of have to find that balanc but what we got befor wa pretti good We had a d multipli by I think and we got reason perform and it wa fast so good And that also appli to the index size as well so lower n bit size index size too bad With higher n bit pretti huge so also someth to think about Now move on to HNSW Now HNSW is what first part of it is NSW which is Navigo Small World Graph Now what make a graph small world it essenti mean that thi graph can be veri larg but the number of hop so the number of step you need to take between ani two vertex which is the point is veri low so in thi exampl here we have thi vertex over here and to get over to thi on on the opposit side we need to take hop and thi is obvious a veri small network so it realli count but you can see thi sort of behaviour in veri larg network so I think in there wa a studi from Facebook and at that point I rememb the exact number of peopl that thei had on the platform but I think in the billion and thei found that the averag number of hop that you need to take between ani two peopl on the platform is like so a veri good exampl of a Navigo Small World Graph Now hierarch NSW graph which is what we ar us built in the same wai like a NSW graph but then split across multipl layer which is what you can see here and when we ar perform our search the path it take will hop between differ layer in order to find our nearest neighbour Now pretti complic and thi is realli I think over simplifi it a lot but the gener gist of it not go to go ani further into it we will I think in a futur video and articl Now put that togeth in code So we have a few differ variabl here we have M which go to set to and M is the number of connect that each vertex ha so of cours that mean greater connect probabl go to find our nearest neighbour more accur EF search which is how what is the depth of our search everi time we perform a search so we can set thi to a higher valu if we want to search more the network or a low valu if we want to search less of the network obvious low valu can be quicker high valu can be more accur And then we have EF construct now thi similar to EF search is how much of the network will we search but not dure the actual search dure the construct of the network So thi is essenti how effici and accur ar we go to build the network in the first place So thi will increas the add time but the search time it make no differ on so good to us a high number I think for thi on So initialis our index and we have is FICE index HNSW flat so we can us differ vector here that we can I think PQ PQ there and essenti what go to do is make thi search faster but slightli less accur Now thi is alreadi realli fast with flat and all go to stick with but again like I said we will return to thi at some point in futur and cover it in a lot more detail for sure So dimension we need to pass in our M valu here as well Now we want to appli those two paramet so we have EF search which is obvious EF search and then we also have HNSW the obvious the EF construct So that should be everyth readi to go and all we want to do now is add our data So indexadd WB Okai now like I said we have that EF construct we us a reason high valu so you can see thi is alreadi take a lot longer than the previou index to actual add our vector into it but still not go to take that long And then onc it is done we ar go to do our search just like we did everi other time so we have DI equal search sorri indexsearch and we ar go to pass in our queri and also K Okai so second to add the vector there so a fair bit longer but then look at thi super fast like that millisecond So much faster than the last on I think the last on wa millisecond right Okai thi is a flat index LSH we have okai so realli quick which is cool But the perform so have a look Okai so we get quit a few here and onli a coupl of so okai not so great it wa realli fast but not veri accur but fortun we can fix that so increas our EF search go to increas it a fair bit go Thi is probabl I would imagin more than enough to get good perform so run thi and run thi Okai and now we see we get pretti good result Now the war time is higher so just a case of balanc it becaus thi is now higher than LSH but what we can do is increas EF construct time the valu of EF construct increas or decreas depend on what you want a lot of flexibl with thi and it can be realli fast thi is HNSW is essenti on of the best perform index that you can us if you look at the current state of the art a lot of them ar HNSW or base on HNSW in some wai or anoth So these ar good on good on to go with you just need to plai around them a littl bit So thi is a few of the perform I found us the same data set but mess around so we have the EF construct valu down here so start with over here up to EF search valu over here and our M valu over here And got pretti good recal over on the EF construct so EF construct is a realli good on to just increas becaus it increas your search time which is pretti cool I think And then here is the search time again for HNSW M and EF search obvious I includ EF construct there becaus it make a differ And thi is the on thing with HNSW the index size is absolut huge so just on thing to bear in mind the index size can take a lot of memori but otherwis realli realli cool index And then that leav us on to our final index which is the IVF index and thi is super popular and with good reason it is veri good So the invert file index is base on essenti cluster data point so you see here we have all of these differ data point the littl cross and then we have these three other point which ar go to be our cluster centroid So around each or base in each of our centroid we expand a catchment radiu around each of those and as you can see here where each of those circl collid it creat the edg of what ar go to be our almost like catchment cell Thi is call a Voronoi diagram or try a realli hard word Dirichlet tessel I know if correct but it sound I think it sound pretti cool so I thought throw that in there So we creat these cell in each on of those cell ani data point within those cell will be alloc to that given centroid and then when you search within a specif cell you pass your XQ valu in there and that will be compar the XQ valu will be compar to everi singl cluster centroid but not the other valu within that cluster or the other cluster onli the cluster centroid and then from that you find out which centroid is the closest to your queri vector and then what we do is we restrict our search scope to onli the data point within that cluster or that cell and then we calcul the nearest vector so at thi point we have all the vector onli within that cell and we compar all of those to our queri vector Now there is on problem with thi which is call the edg problem Now just show thi in two-dimension space obvious in realiti for exampl the data set us we have dimens so dimension the edg problem is kind of complic when you think about it in the hundr of dimens but what thi is is so with sai with our queri we find our queri vector right on the edg of on of the cell and if we sell n probe valu so I mention n probe here how mani cell we search if that is set to on it mean that go to restrict our search to onli that cell even though if you if you look at thi we have two or we have try to think so thi on for sure is closer to our queri vector than ani of the magenta data point and possibl also thi on and thi on but and mayb even thi on but not go to consid ani of those becaus restrict our search onli to thi cell so onli go to look at you know these data point and also these over here so the edg problem but we can get around that by not just search on cell but by search quit a few so in thi case our n probe valu is eight and that mean go to search eight of the nearest centroid or centroid cell and how IVF work go ahead and implement that in code so first thing we need to do is sell n list valu which is the number of centroid that we will have within our within our data and then thi time so thi is a littl bit differ we need to set the the final vector search that go to do so thi is kind of split into two differ oper right so search base on cluster and then actual compar the full vector within the select cluster so we need to defin how go to do that final that final search between our full vector and our queri vector so what we do is write vice so do index flat go to index five p you can us as well we set our dimens it so just initi a flat index there and then what go to do is feed that into our IVF index so our IVF index is vice index IVF and flat becaus us the flat index the flat vector there we need to pass our quantiz so the thi step here the other step to the search process the dimension and also our n list valu of how mani cell or cluster go to have in there and with thi becaus cluster data we need to do someth els so in fact let me show you so if we write index dot is train we get thi fals if we wrote off ani of our other index thi would have been true becaus thei need to be train becaus not do cluster or ani other form of train or optim there so what we need to do is we need to train our index befor we us it so we write index train and we just pass all of our vector into that but veri quick so not realli an issu and then we do index add pass our data and then what we do on thing so I want to show you we have our n probe valu search with on for now so we search on cell and to search we write di as we have everi other time search execut okai okai so I mean super fast millisecond I think mayb the fastest other than how bad perform or low qualiti hsw index so see how how perform so you write mp dot in on d baselin hi you see not too bad to be fair like almost so actual pretti good but what we can do if we want it to be even better is we increas the n probe valu so go up to four so increas the wartim quit a bit so from like three to which is now super slow actual but now get perfect result we can mayb decreas that to two so now faster that could have been a one-off sometim occasion you get a realli slow search and just happen sometim so thi is so we set n probe to super fast and super accur so that a veri good index as well so these ar the stat I got in term of recal and search time in millisecond for differ n probe valu and differ endless valu so again all just about balanc it again index size the onli thing that affect your index size here is obvious the size of your data and the endless valu but you can increas the endless valu load and the index size hardli increas so thi is like increas by kilobyt per like doubl of the endless valu so thi is veri like noth so it for thi video and we cover quit a lot so gonna leav it there but I think these all these index ar super us and quit interest and figur out just plai around with them like you see done load with these these graph just see what is faster what is slower what where the good qualiti is just plai around the paramet and see what you can get out of it is super us for actual understand these now what I do want to do go forward is actual explor each on of these index in more depth becaus onli cover them like veri veri veri high level at the moment so in futur video articl go to go into more depth and explor them a lot more so that we pretti interest I think so it for thi video thank you veri much for watch and I will see you in the next on 
Training BERT #4 - Train With Next Sentence Prediction (NSP)
Hi and welcom to the video Here go to have a look at how we can us NSP or Net Sentenc Predict to train a BERT model Now in a previou video I cover how NSP work but I realli cover how you actual train a model us it So what go to do here So go to jump straight into it and we have thi notebook Here is the data that go to be us I will load that in in a moment but first thing I want to do befor do that is import and initialis everyth we need So obvious when we ar download that data go to be us request for that so go to import request And for our actual train of the model go to be us both HuginFac Transform and PyTorch So I need to import Transform and go to import a BERT tokenis class and also a BERT for Net Sentenc Predict class So BERT for Net Sentenc Predict And as well as that we need to import Torch So onc import all those we can initialis our tokenis and model So tokenis equal BERT tokenis from Pre-train go to be us BERT base on case for thi exampl Obviousli you can us anoth BERT model if like So go to copi that and initialis our model as well Okai And we can run that And now extract thi data So thi warn here need to worri about that just sai if you ar us thi model for infer you becaus you need to train it a littl bit So we need to worri about that fine Becaus we ar go to be train Now what we do need to do is get thi data So new data equal requestsget and we just take thi link I will keep thi link in the descript for you so you can just copi it across if you want to follow along And we should see that we get a respons there good So all we need to do is extract the text from that and go to store it in anoth variabl here text variabl And if we go back to our text variabl and if we just have a quick look at what we have in there we see that we have all of these paragraph Thi is from the Medit by Marcu Aureliu You can get that book onlin why us it And the languag is a bit uniqu as well so why I want to us it becaus when us net sentenc predict train the BERT model to better comprehend the style of languag that we train it on So in here we have our paragraph and all separ by a new line charact So just go to add anoth littl bit of code here which is a split by new line charact And we have a look here we now have a list contain paragraph and our train data what we want to be us So when us NSP we want to creat a split of sentenc that ar random and sentenc that ar not random So go to be take sentenc and of the time go to be ad the genuin sentenc B for that sentenc eg the sentenc that follow it in the text And then the other of the time go to be just choos a random sentenc and pull that in and us that So to do that we first want a bag of sentenc to actual pull that text from So the reason we just us text directli is becaus if we for exampl look at thi we see that we have multipl sentenc in thi singl paragraph So I just split by period we get thi on two three four so we get four sentenc and thi empti on at the end as well which we need to remov So what go to do is loop through our text here so the text variabl split everi sentenc by a by the period charact and append all of those to a new list so a flat list contain just sentenc so no paragraph just sentenc And at the same time need to make sure we includ these empti on becaus we get those with almost I think actual everi paragraph in there So we need to make sure we includ those Now to creat thi bag we write someth like thi so we want to go through each sentenc So we want each sentenc from each paragraph so sentenc sorri sentenc for each paragraph in the text for the sentenc so for sentenc in so thi is where get sentenc from so paragraph dot split we split by the period And as well as that we also need to add that condit that we want ani sentenc that look like thi So we just add that in so if sentenc is not equal to that and that should be OK So check length OK so we get sentenc from that And actual want to save thi to a paramet becaus us it later So we now have the sentenc to sampl from and now we want to do is loop through each sentenc within text or each paragraph within text choos a sentenc from each paragraph if multipl paragraph onli multipl sentenc onli and then percent of the time select a random sentenc from a bag and append that to the end percent of the time append the actual genuin net sentenc onto it and then we creat label as to whether we have random it or not random it So to creat that random percent we import the random librari and we also want to initi our sentenc a list sentenc b list And we also need to initi a label list OK so be or Now what we want to do is loop through each paragraph in our text so for paragraph in text and then here we extract our sentenc like we did with the bag befor so we go sentenc equal and here we want to write sentenc for sentenc in paragraph dot split and rememb we have those random empti sentenc we want to includ those so we write if sentenc is not equal to that empti sentenc So we have now so now loop through each paragraph and split each paragraph into sentenc Now what we want to do is check if that paragraph eg our new sentenc variabl ha more than on sentenc So do number of sentenc equal the length of sentenc and then we sai if number of sentenc is greater than on oop OK excus it right now and then we appli our logic and append that to our actual train data Otherwis if just a singl sentenc we actual add it to the train data I mean ideal we would want to do someth like that but for thi us case I want to get make thing too complic So the reason do that is for exampl thi sentenc is just a singl sentenc in that paragraph and we guarante that each continu paragraph is talk about the same subject you might switch So for the sake of simplic just go to ignor the singl sentenc paragraph although we do have them in our bag so thei can be pull in as potenti sentenc bee when we randomis the select Now what we want to do is set the sentenc that we will start from So we write start equal random randint so thi is onli if we have more than on sentenc rememb So our random randint so thi is go to be the start sentenc in the case that we us sentenc A and B consecut so we randomis sentenc B we want to make sure that we have enough space at the end of our sentenc So here to take both sentenc A and sentenc B So sai for exampl we have go to us an exampl here so we have sai thi is our paragraph we have sentenc in here we want the start sentenc to sentenc A if we select then we have a sentenc B to select from So what we ar go to do here is sai you choos a random integ between and we want to be the maximum there So how do we do that got the number of sentenc here so thi valu will be in thi case so we would sai number of sentenc is minu becaus we the maximum valu we want to select is in thi case So go to be the number of sentenc minu Ok Now what we do is we do our random or not random sentenc B So if random random so thi will just select random float between valu and If that is greater than then sai make thi our random select Ok So for the random select what we do is sentenc B append and then here we would append a random sentenc from our bag up here So to do that we would just write bag and then in here we need to select a random integ like we did up here Ok So go to us that same function So random randint and that need to be between and the length of our bag minu So we us bag size why we have it So bag size minu Ok Now select a random sentenc B from that bag for us And as well as that we also want to set a label So our label in thi case would be a So we have the which mean it is the next sentenc We have a which mean it is not the next sentenc So we set Now our sentenc A it our sentenc A get select the same thing no matter whether we have the random sentenc B or the not random sentenc B So we can actual write our sentenc A append up here And thi is just go to be sentenc And in the index we have start which is our valu from here Ok So we have the random option Now do our not random option So in here we write sentenc B append And thi need to append sentenc start plu So the follow sentenc after our sentenc A And our label here would be which mean it is the next sentenc So quit a lot of code run that and see what we get Ok Now what I want to do is have a look at the first few label See if we have a mix of differ on in there Ok We just have So go to rerun thi becaus I want to show you the differ between and here Ok So we have these So let me print out what we have So for i in rang So just do thi so we can print and see what we actual have in our train data So I want to print the label at that index And then I want to print the sentenc A At that index And follow that with a new line charact and a few dash so we can distinguish between the start and end of sentenc A and B And then we will do print sentenc B And then just go to add in a new line there to distinguish it from the next set of answer So see here that we have We have our sentenc A And our sentenc B is a continu of that first sentenc Becaus we have that label We know that So we have sentenc A here And again thi on here is a continu of thi sentenc A And then down here we have a So thi is where select a random sentenc B And if we read thi I know not the easiest thing to read Yeah the differ reason clear differ in the context there Okai Now thi alwai work In some case we might select even the same sentenc for sentenc A and B But for what do here I think thi is a complet reason wai of go about it Becaus we want to over complic thing If we want to realli be veri strict on it we could add in some extra logic which confirm that we ar not get a sentenc B from around the same area as sentenc A for exampl But for now thi is I think fine Okai So now prepar our data What we need to do now is token it So to token our data just go to us a token which alreadi initi And in here we can actual just pass our sentenc A and sentenc B like thi And our token will deal with how to fit both of those togeth for us So pretti us go to be us PyTorch So we want to return tensor Pt And as well as that we need to truncat or pad each on of those sequenc to a maximum length of And we truncat us thi And we also set pad equal to max length Okai So that should be okai have a look at what we have We see that we have input ID token type ID and attent mass have a look at what thei look like So you see here we have all these differ vector and that is a singl pair of sentenc A and sentenc B And we have quit a few of those Now our token type ID what we would expect is sentenc A would have a token type ID of and sentenc B would have token type ID of We see those on in there So expand that out a littl bit So go with token type ID go with number Okai So now we see okai the reason is becaus in the middl here So what see here is sentenc A follow by sentenc B And then these remain token ar our pad token So we can also see that if we switch across to input ID we see that we have all these pad token And as well anoth item that the token doe for us automat is add a separ token in the middl of our sentenc A and B So sentenc A is thi sentenc B is thi Okai So we have our input tensor We also need to build our label tensor And to do that we just we add it to thi input variabl So we have input label And we set that equal to torch long tensor And thi is a littl bit differ So let me just expand that out So sai we just add label in here So sorri label And we just get thi on big tensor which is not realli in the correct format that we need We need each on of these to match to our input ID token type ID and attent mask So what I mean by that is if we just have a look at thi input ID you see that we get like a list within a list We need that but for our label as well in a differ format at the moment as you can see So we could try transpos that But you see that actual do anyth becaus just a singl invent So just switch everyth around So remov that transpos and add a list insid here You see now get somewher not quit there yet So now we have a list within a list And now what we do is we transpos it and now we get what we need So we have thi almost vector of each of these and each on of these here So thi vector match up to thi valu here And thi on match up to thi on And what we want So copi that and put it here So now we have all the sensor we need for train our model And what we now need to do is set up the input pipelin for train So when train go to need to us a PyTorch data loader object And to creat that data loader object we need to creat a PyTorch data set object from our data So to do that we write thi So go to be us a data set class here So go to call it MeditationsDataSet And in here we write torch util data data set So that make sure that we ar us the correct format for the class for a data set Now we need to defin a few method insid here So our initi method and for our initi method we need to be abl to pass our data So pass it through thi encod variabl And all we need to do in here is assign our encod variabl to be an intern attribut of that data set or that class So write self encod equal encod So that allow us to creat our data set class And then our data loader need two differ method from thi class as well It need a get item method and a length method So do the length method first easier So our length we need to pass anyth to thi just the same as when you would write thi and you put someth insid it So list We get that length exactli what do here So thi creat a enabl you to do thi same method on your class And insid here all we need to do is return the length So what length should we return Well if we just do length of input we get becaus we onli have item in there So we want that We actual want the number of sampl that we have within our input So what we can do instead is we write input input id shape So we have these item So just show you here See thi is our encod size So the max length we set here And thi is the number of sentenc or sentenc pair that we have So we take that and we return it But obvious we have input We now have thi self encod So swap it like that And then we want to pass thi get item method And what thi doe is given a certain index it will return your dictionari your input id token type id attent mask and label dictionari which creat down here for that specif index So we need to allow it to take an index argument there What we do is return let me show you down here what that would look like So we want to creat a dictionari just like we have up here but we just want that for that specif index So what we write is kei And then we write our valu index So mayb it make more sens for me to write so tensor So our kei is our input id attent mask and so on Our tensor is obvious the tensor insid there but we have the full tensor contain all item So then we pull out the index for that tensor But we need to make sure do that for each of our item So becaus we have multipl tensor here we We have the input id label and so on So we do for kei and tensor in and in here we would write sai we do input item So let me just take that out so you can see So that give us our dictionari item And if we do a for loop so for kei tensor in we want to print the kei So you see that loop through each on of those And we also get the tensor out for each on of those as well But specifi a certain tensor with each on So sai we want here We get the tensor and noth more But we want to specifi an index so we copi that And what go to return here Except here we chang it to self encod So our class And with that we can initi our data set object So data set equal medit data set And then all we need to do is pass our data which is just input here Like that OK So our data set readi Now we can initi our data loader And we do that like thi So we do loader torch util data dot data loader We pass our data set object We also want to specifi the batch size So go to us batch of And then we also want to shuffl our data set as well So we write shuffl equal true And our data loader So now now we just need to set up a few model train paramet So the first thing we want to do is move our model to a GPU if we have a GPU So to figur that out what we do is write let me do thi torch devic CUDA So thi is we sai we want to us a CUDA enabl GPU if torch dot CUDA is avail So thi will check our environ and check if we have a CUDA enabl GPU If it avail we want to us a torch devic CPU So run that and see for me I have a CUDA enabl GPU so it come up with thi So we saw that in devic And then what we can do is move our model and also move our tensor later on to that devic for train So we just write model to devic And get a lot of output from that Just ignor that we need to worri about it And we can also activ our model train mode like that OK So move our model over to GPU activ train mode Now what we need to do is initi our optim So go to be us Adam with weight decai for our optim So to us that we need to import it from transform So from transform import AdamW And we initi the optim like thi So we AdamW we pass our model paramet And we also want to pass the learn rate which is go to be to the minu to the minu OK a pretti common on for train transform And that look pretti good to me So now we can begin our train loop So first I want to import someth call TQDM Now thi is pure for aesthet We need it for train Thi is so that we see a littl progress bar dure train otherwis we see anyth So I just want to includ that so we can actual see what is go on So from TQDM import TQDM So thi is option we need to includ it up to you But I would recommend it train for go with epoch Again we want to train transform too much becaus thei will easili over fit And to be honest probabl over fit on thi dataset becaus veri small But fine We just want to us thi as an exampl So go to train for epoch And becaus us TQDM we want to set up our train loop like thi So we wrap it within a TQDM instanc And all we do here is pass our data loader So we creat that up here our PyTorch data loader And we also want to write leav equal true So thi is so that we can see the progress bar And then we loop through each batch that will be gener by that loop gener So for batch in loop So now in our train loop What we want to do here Veri first thing is set our gradient to So obvious in the veri first loop that fine It matter But everi loop after that our optim will have a set of gradient that have been calcul from the previou loop And we need to reset those So we write optim and after that we can load in our batch or our tensor from our batch here So we want input ID equal a batch And we access it like a dictionari So we have input ID So input ID And on other thing that we need to do is our model is on our GPU So we need to move the data that train on to our GPU as well So we just write that OK And copi thi So we have on more So we have all these that we creat up here So input ID set type ID attent mask and label We want all of those So token type ID attent mask and label OK So initi our gradient We have pull in our tensor and now we can process them through our model So we do model input ID We have token type ID We also have the attent mask And we also have our label OK So that will creat two tensor for us in the output It will creat a logit tensor which is our predict And it will creat a loss tensor which is the differ between our predict and our label So extract that loss So we do outputsloss And then we also after extract that loss we need to calcul that thi is the overal loss We need to calcul the loss for everi paramet within our model so we can optim on that So we just write loss backward Yeah backward And then we do optim step And thi will us our optim and take a step to optim base on the loss calcul here And that is all we actual need for our train loop We do also have the TQDM up here as well So I just want to us that And what go to do is just go to set the descript of our loop at thi current step equal to the epoch So thi is just pure aesthet We need thi for train but just so we can see what is go on And we also want to loop set postfix And here go to add in our loss which is just go to be loss equal lossitem like that Now that should be okai give it a go See what happen Okai So that look pretti good So you can see that our model is train Loss is reduc Now there that much train data so not go to see anyth crazi here but we can see that it is move in the right direct So pretti good So everyth for thi video a pretti long video Record for minut probabl be a littl bit short for you But yeah long So everyth for thi video I hope been us And I will see you in the 
Build NLP Pipelines with HuggingFace Datasets
Hi welcom to thi video go to have a look at Hug Face data set librari go to have a look at some of what I think ar the most us data set And go to look at how we can us the librari to build what I think ar veri good pipelin or data input pipelin for NLP So get start So the first thing we want to do is actual well instal data set So go clip instal data set and that will instal the librari for you After thi want to go ahead and import data set And then we can start have a look at which data set ar avail to us Now two wai that you can have a look at all of the data set The first on is us the data set viewer which you can find on Googl You just type in data set viewer and just an interact app which allow you to go through and have a look at the differ data set Now not go to alreadi spoken about that a lot befor and super easi to us So not go to go through it Instead just go to have a look at how we can have view everyth in Python which is the second option So first we can do thi So we just list all of our data set Now go to just write dslist here And from thi we will just get I think someth like data set now So quit a lot So if we go len of all dslist So yeah thousand data set which is obvious a lot And some of these ar massiv as well So if we for exampl if we were to look at the Oscar data set so in dslist we could go data set for data set in dslist If Oscar is in the data set So these ar just data set name Okai and we have Oscar I think PT is what is PT I imagin probabl Portugues And then we have all these other on as well But these ar just these ar user upload Oscar data set Thi is the actual Oscar data set been sold by Hug Face and huge It contain I think more than languag And some of them for exampl English obvious English is on of the biggest on that contain terabyt of data So a lot of data in there but just unstructur text What I want to have a look at is the squad data set Squad data set So go to be us just go to us the origin squad in thi video But you can see that we have a few differ on here So Italian Spanish Korean you have Thai Thai QA squad here and then also French as well at the bottom So you have plenti of choic Now obvious you kind of need to know what sort of data set look for I know look for a squad data set So gone look for squad There ar other on as well actual If I chang thi to lower see those also pop up Okai so we have like thi on here and thi on Thi on seem to work fine Now to load on of those data set obvious go to be us squad We write data set equal data set dot load data set And then in here we just write our data set name so squad Now two wai to download your data So if we do thi thi is the default method We ar go to download and cach the whole data set in memori Which for squad is fine I think squad not a huge data set so not realli a problem But when you think okai if we want the English OSCA data set massiv terabyt So in those case you probabl want to download it all onto your machin So what you can do instead is you set stream equal to true And when stream is equal to true you do need to make some chang to your code which show you And there ar also some thing particularli filter which we will cover later on which we do with stream But we will just go ahead and for now go to us stream switch over to not stream later on And thi creat like a iterat data set object And it mean that whenev we ar call a specif record within that data set it is onli go to download or store that singl record or multipl record in our memori at onc So not download the data set just process it as we get which is I think veri import And it is I think veri us Now you can see here we have two actual subset within our data If we want to select a specif subset all we have to do is rewrit data set again So let me actual copi thi So we copi that And if we just want a subset we write split And in thi case it would be train or valid And if I just call execut that So not go to store that in our data set variabl here becaus I want to us just train We have thi singl iter data set object So just pull in thi singl part of it or singl subset And we can also view So here we can see we have train and valid If you want to see it in a more clear wai you can us dictionari syntax So sorri data set kei You can us dictionari syntax for most of thi So we have train and valid Now also so the moment we have our data set we realli know anyth about it So we have thi train subset And sai I want to understand what is in there So what I can do to start is I write a data set train And I can write for exampl the data set size So how big is it Right data set size data set not data size size know what I wa do there Let me see that we get so like so about megabyt So reason big but not anyth huge noth crazi We can also so we have that We can also get if I copi thi you can also get a descript Let me see what the data set is So SQUAD I even mention it alreadi but SQUAD is the Stanford Question Answere Data Set So us it gener for train Q&A model or test Q&A model And you can paus and read that if you want to And then anoth thing that is pretti import is what ar the featur that we have insid here Now we can also just print out on of the sampl but us to know I think And thi also give you data type so kind of us So we have ID titl context question and answer All of them ar string Answer is actual so within answer we have it sai Sequenc we can view it as a dictionari But we have a text a attribut and also an answer star attribut So pretti us to know I think And to view on of our sampl so we have all the featur here but sai we just want to see what it actual look like We can write data set and we go train And when we have stream set to fals we can write thi But becaus we have stream set to true we do thi So instead what we have to do is we actual just iter through the data set So we just go for sampl in data set And we just want to print a singl sampl And then I want to print anymor so go to write break after that So we just print on of those sampl And then we see okai we have the ID we have titl So each of these sampl is be pull from a differ Wikipedia pull from a differ Wikipedia page In thi case the titl is a titl page So thi on is from the Univers of Notr Dame Wikipedia page We have answer So further down go to ask a question and these answer here So we have the text which is the text answer And then we have the posit so the charact posit where the answer start within the context which is what you can see here Now we have a question here which ask And then the model the Q&A model is go to extract the answer from our context there Okai So not go to be train model in thi video or anyth like that just experi with the data set librari We need to worri so much about that So the first thing I want to do is have a look at how we can modifi some of the featur in our data So with SQUAD when we ar train a model on of the first thing we would do is we take our answer start and the text and we will us that to get the answer end posit as well So go ahead and do that So I first I want to just have a look okai for sampl in the data set train just go to print out a few of the answer featur So we have sampl answer or answer sorri And I just want to print that So print it And I want to sai okai I want to enumer thi So I can count how mani time go through it So here just view the data so we can actual see what we have in there So I want to sai if i is greater than four just break just stop print answer for us So and then we have a few of these So we have text and we have answer start We want to add answer end And the wai that we do that is pretti straightforward We just need to take the answer start and we add the length of our text to that to get the answer end Noth noth complic there So what go to do here is modifi the answer featur And the best wai or I think the least the most common wai of modifi featur or ad new featur as well is to us the map method So we go data set So go to output a new data set So we write data set train equal data set train And go to us the map method And with map we us lambda So we write lambda x So in here build a lambda function And what we need to do So thi is on of the thing that chang depend on whether us stream or not So with stream equal true in here we need to specifi everi singl featur So what I mean by that is let me do it for stream fault initi So when stream is fals we would just write answer And we would write the modif to that featur So in thi case we ar take the current answer So it would be x answer And we would be merg that with a new dictionari item which is go to be answer end So answer start So answer end is equal to And here what we have to do is we go x answer So thi is a littl bit messi now But just how it is So within answer and we want to take the answer start posit So answer start And we want to add Let me start a new line here And we want to add the length of answer text OK so all do there is take answer start and ad answer text Or the length of answer text to that to get our answer end Now thi is all we would have to write if we were us stream equal fals But not With stream equal true we need to add everi other featur in there as well not sure why thi is the case But it is So we need to just add those in as well So all thei ar is a direct map from the old version to the new data set So we need to realli do anyth there We just need to add ID We want to map that to ID and do that for the other featur as well So we have also have context Which is exit context We have answer alreadi done of cours A question which is go to be exit question So ID context question answer Is there anyth els miss ID oh titl of cours Titl just titl Yeah so also add titl in there as well Okai and with that we should be readi to go So map that And what find is when us stream keyword equal true The actual process is or the transform that we just built is lazili load So we actual just done anyth there All said is pass thi instruct to transform the data set in thi wai But it actual transform anyth yet It onli perform thi transform when we call the data set So if we did thi again Thi would call the data set and it would forc the code to run thi instruct or thi transform So run that And you see we actual do get an error here And why is that So let me come down We have so what am I do And start plu delet those answer wrong with that Ah okai so if we look up here We have these item here that ar within the list So we actual need to access that first item But good becaus we saw that when we first execut thi code noth happen And it onli actual came across that error when we call a data set Becaus when thi transform is actual perform And now what we have to do is becaus alreadi ad thi instruct to our data set you know transform or build process We actual need to reiniti our data set So we will come back up here So where ar you here So date no not that on thi on So we need to load that again to reiniti the all of the instruct that ad in there And then we can go ahead rerun thi And now it should work hopefulli see There we go So now if we have a look at thi And thi is someth I probabl should have done but I complet forgot to So I should have ad thi as mayb a list rather than just the number But fine becaus you know if you come across and you need to do thi You mai want to add that in But not do anyth other than plai around with the data set librari So not realli a problem But you can see that we have ad answer and into there now which is what we want to do And also importantli is if I let me copi thi bring down here notic that we do still have all of our data set So if I go here I realli need to remov fine just break straight awai fine So sampl sorri here So you see the whole thing And we see that we still have the ID We have the text We have the context We have everyth in there Now just go to show you you know why thi break Or what happen if I remov these Okai so let me rerun that and thi as well So yeah so thi should look the same Do we have yet fine But then if I run thi So befor thi had all the all the featur but now we onli have the singl featur that we specifi in thi formula So the answer So why you need to when shuffl is set to true why you need to add everi singl featur in there Otherwis just go to remov them when you perform the map oper But onli the case when shuffl is actual set to true Shuffl Why am I sai shuffl Stream is set to true So let me bring thi down here And let me also copi our initi load code So here Becaus go to need to reload our data set now anywai Becaus we just remov all the featur from it Okai and what go to do now is just set stream to fals And go to run thi same code where we still have our id or anyth like that in there And see what happen As well also notic get a load bar here And go to take a littl bit of time to process thi Although actual with thi probabl go to be super fast So probabl ignor that But it will you see okai take a littl bit of time So now go through a whole data set We call the date set But we have us thi map function When stream is set to fals the data set lazili load And so the oper is go to be a bit more complic So the oper the map oper is perform as soon as you call it So a slightli differ behavior And the other behavior which is differ is the fact that we onli need to specifi the answer featur here So we onli when we have stream set to fals We need to includ everi featur within the map oper We onli need to includ the featur that we ar modifi or creat Which you know weird I know why a behavior differ when stream is true or fals But it is there So if I now take thi again Come down here and run that We see now that we have all of our featur again Right so befor when stream wa true If I run thi code it would have onli includ our answer The id titl context question thei all would have been remov But now with stream equal to fals still there So weird a weird So a weird featur or a weird behavior But how it is and we obvious just need to deal with it Now the next thing I want to show you is how we can also add batch to our map process So typic with well pretti much everi or ani as far as I can think of ani NLP task go to want to token our text So go to go ahead and do that for Q&A So we would import transform or from transform import a BERT token sai and I would initi that So thi is you know what we typic do We do token equal BERT token from pre-train And sai BERT base un-gas Okai initi that And then what I want to do is go to token my context or question and context In the format that SQUAD would usual expect when do Q&A or build a Q&A model And I want to do that us the map function So you can do thi in both stream and non-stream by the wai So we just write date set It wa train so same as befor Date set it wa train or date set train dot map We ar us a lambda function so lambda x And in here we just want to sai token So not do the usual when you write thi you would includ a dictionari here But the token the output from the token is alreadi in dictionari format So we need to I need to do it in thi case But basic what we have here is is still a dictionari And what I want to do is so with Q&A in your token you pass two text input You pass your question and you would also pass your question And you would also then pass your context And as usual we would we set our max length So usual I would set pad equal to the max length And also do truncat as well Okai so veri typic token process Noth noth differ go on here Thi is what we normal do when we token our text go into a transform model And then we want to sai okai batch equal true So thi allow us to do everyth or perform thi oper in batch And then we can also specifi our batch size So batch size equal sai So now when we run thi where is it where is it go here Now when we run thi the map function here is go to token our question and context in batch of So go ahead and do that Okai and then you can you can see that process there So I mean all we realli need to do with that So I think probabl it for the map method And well fast forward and continu with I think a few of the method I think ar quit us as well Okai so just finish up now So we can go ahead and have a look at what actual produc So come to here and sai dataset train So what do we have now We have answer like we did befor but now we also have attent mask We have input id and we also have token type id Which ar the three tensor that we usual output from the token when we do that So we now have those in there as well We can also have a look anoth thing as well we can now rather than loop through our dataset becaus not us a not us stream which is true us stream equal fals We can now do thi and we can see okai we have attent mask and not go to show me everyth becaus quit larg So just delet that but you can see that we have the attent mask in there So what I want to do is sai I want to be quit pedant and I like the fact that there is the fact that we have on featur call titl Mayb I want to sai okai it should be topic becaus a topic of the context and the question If I want to be realli pedant and modifi that I could sai dataset train renam column And to be honest you can us it for thi of cours but probabl not go to probabl go to us it more for when you need to renam a column to make sure it align to whatev the expect input ar for a transform model for exampl So where you would us it but just us thi exampl So go to renam the column titl to topic and print out and dataset train again So down here we have titl and the moment go to have topic Okai so now we have topic So just renam column like I said come us not in thi case but gener thi is usual us Now what I mai want to do as well is remov certain record from thi dataset So so far been print out the here we have thi which is now topic We have Univers of Notr Dame Mayb for whatev reason we want to includ those topic so we can sai veri similar to befor we write dataset train equal dataset train again Thi time go to filter so go to filter out record that we want And again veri similar to the syntax we us for the map function which is the lambda And in here we just need to specifi the condit for the sampl that we do want to includ or we do want to keep And in thi case we want to sai okai wherev the topic is not equal to Univers of Notr Dame Okai so run thi and have a look at what we produc So dataset train so somehow like we have number of row here which is just over And we should get a lower number now Now thi will also go through so thi rememb we have shuffl Set to shuffl what I keep call it shuffl We have stream set to fals thi time So go to run through the whole dataset and perform thi filter oper Now whilst wait for that Now just fast forward again to when thi finish in a moment Okai so now we have thi finish and we can now run thi finish and we have befor we had row now we have And we should see so let me take the dataset train topic and I want to see sai the first five of those Okai now all Beyonc rather than befor where it wa the Univers of Notr Dame So we have those and what we mai want to do now is sai for exampl perform infer with Q&A with a transform model We realli need all of the featur that we have here So we would onli need the attent mask the input id and also the token type id So what we can do now is we can remov some of those column So do dataset train as alwai dataset train again And we want to remov those column so remov column And just remov so all of them other than the on that we want So do answer context id question and topic Okai and then have a look at what we have left Okai and then it so we have those final featur and these ar the on that we would input into a transform model for train Now I mean noth els I realli want to cover I think that is pretti much all you need to know on Iconfac dataset to get start and start build pretti I think good input pipelin and us some of the dataset that ar avail So leav it there Thank you veri much for watch and I will see you again in the next on 
How-to do Sentiment Analysis with Flair in Python
Hi welcom to thi video on sentiment analysi us the Flare librari So Flare is an incred simpl easi to us librari which contain a load of pre-built model for NLP that we can simpli import and us to make predict So it actual allow us to us some of the most power model out there as well So in thi tutori go to be us the Distilbert model which is base on a BERT but a lot smaller but almost as power as BERT itself So go to go ahead and begin First if you alreadi you need to pip instal Flare And alongsid Flare you ar also go to need PyTorch If you got PyTorch instal alreadi need to head over to the PyTorch websit And thei give you instruct on exactli what you need to instal So we come down to here and we can see OK for me I have Window I want to instal us Conda us Python and then CUDA So thi is if you have a CUDA enabl GPU on your machin If you know what that mean you probabl So in that case just click none But for me I have So all we need to do is copi the command underneath here And then we would run thi in our Anaconda prompt I alreadi have these instal so go to go ahead and actual begin code So go to need to us Panda and also Flare So now we have import Flare we can actual import a sentiment model straight awai So all we need to do is pass our sentiment model to a variabl Which we will call sentiment model And we just need to write FlaremodelstextClassifi and load And then in here we pass the model name that we would like to load And in our case it will be the English sentiment model which is en-senti En-sentiment Like so OK so now we ar download the model And in a moment that will have download and we can begin us it Now obvious we need data I have download some data here Which is a sentiment data set base on the IMDB Moveri review So you can find the same data set over here OK so sentiment analysi on Moveri review data set So from Rotten Tomato You can just scroll down and we have the train data and test data here just go to us the test data and build a test data set just go to us the test data but we can us either just go to be make predict base on the phrase here So we need to read in our data So go to read it in as if it were a CSV file And we will just pass a tab as our separ becaus we ar actual work with a tab separ file OK so here actual a CSV not CSV OK so the first thing notic is that we actual have duplic of the same phrase That is actual just how thi data set is It just contain the full phrase initi So thi first entri here is the full phrase And then all of these follow ar actual part of that phrase So what we can do so chang it so we can actual see the full phrase first OK so we realli see that much more anywai but we can see that the full phrase ha fine So to remov thi we just want to drop all of the duplic whilst keep the first instanc of the sentenc ID So you see each on of these thei all have the same sentenc ID actual onli the first on that we need So we just drop duplic on thi column keep the first entri OK so keep the first entri drop duplic from sentenc ID and just do thi oper in place OK so now we can see each sampl is now a uniqu entri OK so now our data is readi So we need to actual first convert our text into a token as list us Flare So Flare doe thi on sentenc at a time So if we for exampl pass Hello World into the Flare token we will be abl to see what actual do OK so here we can see that it split each on of these into token So got Hello is a token World is a token and then we have also split the exclam mark at the end there And you can see that Flare is tell us that there ar a total of three token So we can see that Flare is tell us that there ar three token So we can see that Flare is tell us that there ar three token And you can see that Flare is tell us that there ar a total of three token there So each on of our sampl here will need to be process by thi Flaredatasent method befor we pass it into the actual model Once we do have thi so call thi sampl as well We will pass it to our model for predict Which is realli easi All we need to do is call the predict method On the sampl And now thi output anyth Instead it actual just modifi the sentenc object that we have produc So it modifi sampl And we can see now that our sampl we solv the sentenc and we solv the number of token But we also have these addit label which ar the predict We have the label which is posit which mean a happi or a posit sentiment And then what we have here is actual the probabl or the confid in that predict great but realist we want to be extract these label So actual abl to extract these by access the label method So you have label here and thi produc the posit and the confid To access each on of these we access the posit and the confid We access index zero follow by dot valu Okai so thi will give us the posit And then we can also do the same to get the confid call the score Like that So what we can do now is just creat a simpl for loop that will go through each sampl in our test data and assign a probabl for each on So we will initi creat a sentiment and confid list And then we will just as we ar loop through the data we will append our sentiment valu So the posit or neg and the confid to each on of these list So here we ar first token our sentenc Then we ar make a predict us that token sentenc which we ar call sampl And as we did befor we have now got thi label sentenc and we just need to extract the two label that we have here Okai so we can see here that on of our sentenc wa just blank so we will add in some logic to avoid ani error there Okai so look at thi also whenev a space as well So we just need to trim thi which we can do easili us the strip method Okai so it took a littl bit of time but we now have our predict So what we want to do is actual add what we have here in the sentiment and confid list to our data frame So to do that we just add df sentiment to creat a new sentiment column And we made that equal to the sentiment list that we have creat And then we also do the same for confid as well Then we can see our data frame Okai so initi look at thi it look pretti good So intermitt pleas but mostli routin effort Incredibl neg but basic sai occasion okai but gener noth special So obvious a neg sentiment which is match up to neg sentiment here Here sai okai the onli thing worth watch in birthdai girl And it serv as anoth exampl of the sad declin of British comedi in the post full monti world Fair enough also neg So thi on is our first posit Once you get into it relev The movi becom a headi experi Yeah I mean sound pretti posit to me So quit good even here where not sai anyth particularli like a neg or posit word just sai that the movi is or the movi deliv on the perform of strike skill and depth Which must be pretti hard for a machin to understand and actual get right But look at all these do realli well And I think realli cool that we can actual do thi with so littl effort And onli actual written a few line of code in realiti And produc realli good accur result which is realli impress to me So it for thi video 
How to Build Custom Q&A Transformer Models in Python
Hi and welcom to the video Todai go to go through how we can fine-tun a Q&A Transform model So for those of you that know Q&A just mean question answer and on of the biggest topic in NLP at the moment a lot of model out there where you ask a question and it will give you an answer And on of the biggest thing that you need to know how to do when you ar work with transform whether Q&A or ani of the other transform base solut is how to actual fine-tun those So what go to be do in thi video go to go through how we can fine-tun a Q&A Transform model in Python So I think realli interest and I think you will enjoi it a lot So just go ahead and we can get start Okai so first thing we need to do is actual download our data So go to be us the SQuAD dataset which is the Stanford question answer dataset which is essenti on of the better known Q&A dataset out there that we can us to fine-tun our model So first creat a folder go to us OS and OSMaker just call it SQuAD Obviousli call thi and organ it as you want Thi is what I will be do Now the URL that we ar go to be download thi from is thi Okai and there ar actual two file here that go to be download but both will be come from the same URL So becaus make a request through URL go to import request We can also us the Wget librari as well or if on Linux you can just us Wget directli in the termin up to you what go to be us request Okai and to request our data go to be do thi So just a get request us a FString and we have the URL that alreadi defin And then the train data that be us is thi file here Okai request Okai we can see that successfulli put that data in there Okai so like I said befor actual two of these file that we want to extract So what go to do is just put thi into a for loop which will go through both of them Just copi and past thi across Renam thi file And the other file is the same but instead of train we have dev Okai so here make our request And then next thing we want to do after make our request is actual save thi file to our drive Okai and we want to put that insid thi squad folder here So to do that we us open And again go to us a string here I want to put insid the squad folder here And then here we ar just go to put our file name which is file Now write thi in binari becaus JSON So we put wb for our flag here And then within thi namespac we ar go to run through the file and download it in chunk So we do for chunk And then we iter through the respons Like thi just us a chunk size of four And then we just want to write to the file like that So that will download both file Just add the colon there So that will download both file We should be abl to see them here now So in here we have data We have essenti a lot of differ topic So the first on is Beyonc And then in here we will see if we just come to here we get a context But alongsid thi context we also have QAS which is question and answer And each on of these contain a question and answer pair So we have thi question when did Beyonc start becom popular So thi answer is actual within thi context And what we want our model to do is extract the answer from that context by tell us the start and end token of the answer within that context So we go zero and it is in the late And we have answer start So that mean that a charact we get I So if we go through here we can find it here OK so thi is the extract And what we will be aim for our model to actual extract But there will be a start point and also the end point as well which is not includ in here But we will add that manual quit soon So our data and then also be test on the dev data as well which is exactli the same OK so we move on to the data prep So now we have our file here go to want to read them in So go to us the JSON librari for that And like we saw befor quit a complex structur in these JSON a lot of differ layer So we need to us a few for loop to fill through each of these and extract what we want which is the context question and answer all correspond to each other So in the end go to have a list of string which is go to be all of these And in the case of the answer we also have the start posit So it will be a list of dictionari where on valu is a text and on valu is the start posit So to do that go to defin a function call Rebsquad defin our path here as well And the first thing we need to do is actual open the JSON file So we do with open path And again we ar us a binari file So go to have B as a flag But instead of write we ar read So us R here So RB just go to do JSON load F here So now we have our dictionari within thi squad dict here So mayb whilst just build thi function up probabl more us to do it here So we can see what actual do So copi that across And then fill thi out afterward And then fill thi out afterward Of cours we do actual need to includ the path So take thi And now we can see insid here Mayb we can load just a few rather than all of them Or we can investig it like thi Okai so we have the version and data which we can actual see over here Version and data So we want to access the data And within data we have a list of all these differ item which is what I wa try to do befor So we go into data and just take a few of those Okai and then we get our differ section First on just take zero which is Beyonc And then we have all of these So go to want to loop through each on of these becaus we have thi on the next and go to keep need to just run through all of these So do that We want to do for group in squad dict And rememb we need to includ the data here just see how sai group titl So group titl so we can see a few of those Okai go to go through each on of those So the second part of that ar these paragraph And within the paragraph we have each on of our question So first go with paragraph and do a chop in here Sorri a list There we go And the first thing we need to extract is the easiest on which is our context Howev that is also within a list So now if we access the context we get thi So essenti go to need to jump through or loop through each on of these here Now go to access the paragraph and loop through each on of those And then here go to access the context So write that So we alreadi have on group here So just stick with that And go to run through the passag in the paragraph So alreadi here go through the for loop on thi index And now go to go through the for loop And now go to go through a loop on thi index keep that So that mean that we will be abl to print the passag context And there we go So here we have all of our context So on of our three item that we need to extract Okai so great put that all togeth So go to take thi put it here And then we have our context Okai great But obvious for each context we have a few differ question and answer So we need to get those as well Now that requir us to go through anoth for loop So go thi passag We need to go into the QAS kei and loop through thi list of question and answer So we have thi and then we have our list So anoth layer in our for loop will be for question answer in passag QAS And then take a look at what we have there Okai great So we have plausibl answer question and answer So what we want in here is the question and answer So question is our first on Perfect So we have the question now And then after we have extract the question we can move on to our answer As we see here the answer come as anoth list Now each on of these list all just have on actual answer in there which is complet fine So we can access that in two wai We can either loop through or we can access the zero valu of that arrai Either wai it matter So all we need to do here is loop through those answer or if we want just go with QA answer So in most case thi should be complet fine As we can see here most of these question and then thei have the answer dictionari The question and then thei have the answer dictionari which is fine Howev some of these ar slightli differ So if we scroll right down to the end here see okai we have thi which is talk about physic And then rather than have our answer arrai we have these plausibl answer which is obvious slightli differ And thi is the case for a coupl of those So from what seen to state like the best wai to deal with thi is simpli to have a check If there is a plausibl answer kei within the dictionari we will includ that as a check Within the dictionari we will includ that as the answer rather than the actual answer dictionari So to do that all we need to do is check if QA kei contain plausibl answer If it doe we us that Otherwis we us answer Okai Then we us thi on Otherwis we will us answer So just add all of that into our for loop here So we have our context and then we want to loop through the question answer And thi is where we get our question Then onc here we need to do someth slightli differ which is thi plausibl answer Okai And then we us thi access variabl in order to defin what go to loop through next So here we go for answer Answer sorri in QA access becaus thi will switch to implaus answer or answer And then within thi for loop thi is where we can begin ad thi context question and answer to a list of question context and answer that we still need to defin up here So each on of these is just go to be an empti list And then all we do is copi thi across and we just append everyth that extract in thi loop And the context And then we just add the context And then we just add the context And the context question and answer And that should work So now take a look at a few of our context Okai and we can see we have thi and becaus we have multipl question and answer for each context the context just repeat over and over again But then we should see someth slightli differ when we go with answer And question Okai so great We have our data in a reason format now But we want to do thi for both the train set and the valid set So what go to do is just go to put thi into a function like we were go to do befor Just read squad So here go to read in our data and then we run through it and transform it into our three list All we need to do now is actual return those three list And answer So now what go to do is just go back to our train set And go to do thi for both the train set and the valid set And answer So now what we can do is execut thi function for both our train and valid set So go to train context question and answer Okai so that is on of them And we can just copi that And we just want thi to be our valid set Like so Okai so great We now have the train context and the valid context which we can see right here So here hope that there is a slight differ in what we see between both Okai great what we would expect Okai so now we have our data almost in the right format We just need to add the end posit So we alreadi have the start posit if we take a look in our train answer Okai we have the answer start but we also need the answer end And not includ within the data So what we need to do here is actual defin a function that will go through each on of our answer and context and figur out where that end charact actual is And of cours we could just sai okai the length of the text We add that onto the answer start and we have our answer end Howev that unfortun work becaus some of the answer start ar actual incorrect and usual off by on or two charact So we actual need to go through and on fix that and two add our end indic So to do that just go to defin a new function go to be add end index And here we will have our answer and the context And then go to just feed these in So first thing we do is loop through each answer and context pair And then we extract someth which is call the gold text which is essenti the answer that we ar look for call the golden text or gold text So simpli our answer and within that the text So we ar pull thi out here So we should alreadi know the start index So what we do here is simpli pull that out as well And then the end index ideal will be the start plu the length of the gold text Howev not alwai the case becaus like I said befor thei can be off by on or two charact So we need to add in some logic just to deal with that So in our first case assum that the charact ar not off So if context start to end Start to end equal the gold text Thi mean everyth is good and we need to worri about it So we can modifi the origin dictionari and we can add answer end into there And we made that equal to our end index Howev if not the case that mean we have a problem on of those dodgi question answer pair And so thi time what we can do is add a els statement So just go to go through when the posit is off by on or two charact becaus it is not off by ani more than that in the squad data set Loop through each of those and sai OK if the context And then in here we need to add the start index and thi again So just copi and past that across Be easier But thi time check to see if it is off by on or two charact So just minu N And alwai minu and it shift alwai shift to the left rather than shift to the right So thi is fine So in thi case the answer is off by end token And so we need to updat our answer start valu and also add our answer end valu So start index minu N and we also have the end So great We can take that and we can appli it to our train and valid set So all we do here is call the function And we just see train answer and train context And of cours we can just copi thi and do the same for our valid set OK perfect So now if we have a quick look we should be abl to see that we have a few of these end point as well OK so I think that look pretti good And that mean we can move on to actual encod our text To token or encod our text thi is where we bring in a BERT token So we need to import the Transform librari for thi And from Transform we ar go to import the Distilbert So Distilbert is a smaller version of BERT which is just go to run a bit quicker but it will take a veri long time And go to import the FAST version of thi token becaus thi allow us to more easili adjust our charact and then start locat to token and start locat later on So first we need to actual initi our token which is super easi All do is load it from a pre-train model And then all we do to creat our encod is to load the token So do the train set first call it token And in here we includ our train context And the train question So what thi will do is actual merg these two string togeth So what we will have is our context and then there will be a separ token follow by the question And thi will be fed into Distilbert dure train I just want to add pad there as well And then copi thi and do the same for our relat set Okai and thi will convert our data into encod object So what we can do here is So what we can do here is print out differ part that we have within our encod So in here you have the input ID so access that and find in here we have a big list of all of our sampl so check that we have and open on of those okai and we have these token ID and thi is what Bert will be read Now if we want to have a look at what thi actual is in sort of human readabl languag we can us the token to just decod it for us Okai thi is what feed in so we have a coupl of these special token thi just mean the sort of sequenc and in here we have a process form of our origin context Now you find that the context actual end here and like I said befor we have thi separ token and then after that we have our actual question and thi is what is be fed into Bert but obvious the token ID version So just good to be awar of what is actual be fed in and what actual us here but thi is a format that Bert is expect and then after that we have anoth separ token follow by all of our pad token becaus Bert is go to be expect token to be fed in for everi on sampl so we just need to fill that space essenti so all that is do So remov those and we can continu So the next thing we need to add to our encod is the start and end posit becaus at the moment we just have them in there So to do that we need to add a addit bit of logic We us thi charact to token method so if we just take out on of these take the first on Okai we have thi and what we can do is actual modifi thi to us the charact token method Remov the input ID becaus we just need to pass it the index of whichev encod we ar want to modifi or get the start and end posit of and in here all do is convert from the charact that we have found a posit for to the token that we want to find a posit for and what we need to add is train answer We have our posit again becaus the answer and encod the context and question that need to match up to the answer of cours that ask about and we do answer start So here just feed in the posit of the charact and thi is answer Okai so feed in the posit of the charact and expect to return the posit of the token which is posit So all we need to do now is do thi for both of those so for the start posit and end posit See here we should get a differ valu Okai but thi is on limit of thi Sometim thi is go to return noth as you can see not return anyth here and that is becaus sometim it is actual return the space and when it look at the space and the token see that and thei sai okai noth not concern about space and it return thi non valu that you can see here So thi is someth that we need to consid and build in some ad logic for So to do that again go to us a function to contain all thi and call it add token posit Here have our encod and our answer and then we just modifi thi code so we have the encod we have the answer and becaus collect all of the token posit we also need to initi a list to contain So we do start posit empti list and end posit And now we just want to loop through everi singl answer and encod that we have Like so And here we have our start posit so we need to append that to our start posit list And we just do the same for our end posit which is here Now here we can deal with thi problem that we had So if we find that the end posit the most recent on so the neg on index is non that mean it found and it mean there is a space So what we do is we chang it to instead us the minu on version And all thi need to do is updat the end posit here Okai great but in some case thi also happen with the start posit but that is for a differ reason The reason that will occasion happen with start posit is when the passag of data that ad in here so you saw befor we had the context that separ token and then the question Sometim the context passag is truncat in order to fit in the question So some of it will be cut off and in that case we do have a bit of a problem but we still need to just allow our code to run without ani problem So what we do is we just modifi the start posit again just like we did with the end posit Obviousli onli if a non and we just set it to be equal to the maximum length that ha been defin by the token as simpl as that Now the onli final thing we need to do which is becaus us the encod is actual updat those encod to includ thi data becaus as of yet we ad that back in So to do that we can us thi quit handi updat method and just add in our data as a dictionari So you have start posit start posit and we also have our end posit And then again we just need to appli thi to our train and valid set and just modifi that add the train encod here and train answer We do that again the valid set So now take a look at our encod and here we can see great now have those start posit and end posit We can even so a quick look what thei look like What done is actual not includ the index here so just take it for the veri first item everi singl time So just updat that So obvious that get us veri far And just updat that as well And now thi should look a littl bit better So lucki we check Okai so our data at the moment is in the right format We just need to us it to creat a PyTorch dataset object So to do that obvious we need to import PyTorch And we defin that dataset us a class just pass the torch util data dataset We need to initi that Like so And thi is come from the Houden Face Transform document take credit for thi And we essenti need to do thi so that we can load in our data us the PyTorch data loader later on Which make thing incred easi And then we just have on more function here Or method Okai And return And also thi as well That should be okai So we appli thi to our dataset to creat dataset object Now our encod And then the same again for the valid set Okai so that is our data Almost fulli prepar All we do now is load it into a data loader object But thi is everyth on the data side done Which is great becaus I know thi bit doe take some time And I know not the most interest part of it But just someth that we need to do And need to understand what do as well So now we get to the more interest bit So just add the import in here So we need our data loader go to import the Adam optim with weight decai Which is pretti commonli us for transform model when you ar fine-tun Becaus transform model ar gener veri larg model And thei can over fit veri easili So thi Adam optim with weight decai essenti just reduc the chanc of that happen Which is suppos to be veri us and quit import So obvious go to us that And then final bit is TQDM So TQDM is a progress bar that we ar go to be us So that we can actual see the progress of our train Otherwis just go to sit there for probabl quit a long time not know what is actual happen And trust me it take long befor you start question whether anyth is happen Becaus it take a long time to train these model So thei ar our import And be stupid again here from did that twice Okai so all good So now we just need to do a few littl bit for the setup So we need to tell Pytorch whether us CPU or GPU In my case it will be a GPU If us CPU thi is go to take you a veri long time to train And still go to take you a long time on GPU So just be awar of that But what go to do here is sai devic CUDA If CUDA is avail Otherwis we ar go to us the CPU And good luck if that is what do So onc defin the devic we want to move our model over to it So we justmodelto devic So thisto method is essenti a wai of transfer data between differ hardwar compon So your CPU or GPU quit us And then we want to activ our model for train So two thing we have here So we havetrain and eval So when in train mode a lot of differ layer and differ part of your model that will behav differ depend on whether you ar us the model for train or us it for infer which is predict So we just need to make sure our model is in the right mode for whatev do And later on switch it to eval to make some predict So almost everyth So we just need to initi the optim And here us the weight decai Adam optim We need to pass in our model paramet and also give it a learn rate And go to us thi valu here All these ar the recommend paramet for what we ar do here So the on thing that I have somehow miss is defin the actual initi the model So just add that in And all do here is load again a pre-train on So like we did befor when we were load the transform token Thi time for question answer So thi the Silbert of question answer is a the Silbert model with a question and answer head ad on to the end of it So essenti with transform you have all these differ head that you add on and thei will do differ thing depend on what head it ha on there So initi that from pre-train And us the same on we us up here which is the Silbert base uncas And sometim you will need to download that Fortun I need to as alreadi done that but thi can also take a littl bit of time Not too long though and you get a nice progress bar hopefulli as well Okai so now that is all set up we can initi our data loader So all do here is us the PyTorch data loader object and we just pass in our train data set The batch size so how mani we want to train on at onc in parallel befor updat the model weight which will be And we also would like to shuffl the data becaus we want to train the model on a singl batch and it just learn about Beyonc and then the next on learn about Chopin and it will keep switch between those but never within a singl batch have a good mix of differ thing to learn about So it is data set seem a bit of a weird name to me so just go to chang it And thei also spell There we go And that is everyth we can actual begin our train loop So gonna go for three part and what we want to start with here is a loop object So we do thi mainli becaus us TQDM as a progress bar otherwis we need to do thi There would be no point in do it and all thi is do is kind of like pre-initi our loop that we ar go to go through So go to obvious loop through everi batch within the train loader so we just add that in here and then thi other paramet which I know if we So leav it but essenti you can add a leav equal true in order to leav your progress bar in the same place with everi epoch Wherea at the moment with everi epoch what it will do is creat a new progress bar We ar go to creat a new progress bar but if you do that and you want it to just stai in the same place you add leav equal true into thi function here So after that we need to go through each batch within our loop and the first thing that we need to do is set all of our calcul gradient to zero So with everi iter that we go through here or everi batch at the end of it we ar go to calcul gradient which tell the model in which direct to chang the weight within the model and obvious when we go into the next iter we want those gradient to still be there So all do here is reiniti those gradient at the start of everi loop so we have a fresh set of gradient to work with everi time and here we just want to pull in our data So thi is everyth that is relev that go to be feed into the train process So everyth within our batch and then in here we have all of our differ item So we can actual see go here we want to add in all of these and we also want to move them across to the GPU in my case or whatev devic you ar work on I would do that for the attent mass start posit and end posit So these start and end posit ar essenti the label that ar target that we want our model to optim for and the input ID and attent mass ar the input So now we have those defin we just need to feed them into our model for train and we will output the result of that train batch to the output variabl Our model the ID we need the attent mask We also want our start posit and end posit Now from our train batch we want to extract the loss And then we want to calcul loss for everi paramet and thi is for our gradient updat and then we us the step method here to actual updat those gradient And then thi final littl bit here is pure for us to see thi is our progress bar So we call the loop we set the descript which is go to be our epoch And then it would probabl be quit us to also see the loss in there as well We will set that as a post fix so it will appear after the progress bar Okai and that should be everyth Okai so that look pretti good We have our model train and as I said thi will take a littl bit of time So I will let that run Okai so we have thi non type error here and thi is becaus within our end posit we normal expect integ but also get some non valu becaus the code that we us earlier where check if end posit is non essenti good enough So as a fix for that just go back and add like a while loop which will keep check if non and everi time it is non reduc the valu that we ar see by on So go back up here and thi is where the problem is come from So just go to chang thi to be a while loop and just initi a essenti a counter here And us thi as our go back valu and everi time the end posit is still non just add on to go back and thi should work Just need to rememb to rerun anyth we need to rerun Yeah Okai and that look like it solv the issu So great we can just leav that train for a littl while and I will see you when done Okai so the model is finish and go ahead and just save it So obvious need to do that whenev actual do thi on ani other project So just go to call it the Silbert custom and super easi to save we just do save pre-train and the model path Now as well as thi we might also want to save the token so we have everyth in on place So to do that we also just us token and save pre-train again Okai so if we go back into our folder here receiv model and we have thi Silbert custom and then in here we have all of the file we need to build our PyTorch model a littl bit differ if us TensorFlow but the actual save process is practic the same So now finish train we want to switch it out of the train mode so we us a model eval just get all thi inform about our model as well we actual need ani of that and just like befor we want to creat a data loader So for that gonna call it val loader and exactli the same code as befor in fact probabl better if we just copi and past some of thi At least the loop So what gonna do here is take the same loop and appli it as a valid run with our valid data Just past that there initi thi data loader thi time of cours with the valid set stick with the same batch size Now thi time we do want to keep a log of accuraci so we will keep that there and we also need to run most for epoch becaus not train thi time just run through all the batch within our loop of valid data So thi is now a valid loader and we just loop through each of those batch so we need to do anyth with the gradient here and becaus not do anyth to gradient we actual add thi in to stop PyTorch from calcul ani gradient Thi will obvious save us a bit of time when process all of thi And we put those in there The output we do so want thi but of cours we need to be put in the start and end posit so we can remov those and thi time we want to pull out the start predict and end predict So if we have a look at what our output look like befor you see we have thi model output and within here a few differ tensor which each have a access name So the on that we care about ar startLogit and that will give us the logit for our start posit which is essenti like a set of predict where the highest valu within that vector repres the token ID So we can do that for both see we get these tensor Now we onli want the largest valu in each on of these vector here becaus that will give us the input ID So to get that we us the argmax function and if we just us it by itself that will give us the maximum index within the whole thing But we want that we want on for everi singl vector or everi row and to do that we just set dim equal to and there you go we get a full batch of output So these ar our start posit and then we also want to do the same for our end posit So we just chang start to end So pretti easi Now obvious we want to be do thi within our loop becaus thi is onli do on batch and we need to do thi for everi singl batch So just go to assign them to a variabl and there we have our predict and all we need to do now is check for an exact match So what I mean by exact match is we want to see whether the start posit here which we can renam to the true valu whether these ar equal to the predict valu down here and to calcul that so let me just run thi so we have on batch That take too long to process and we can just write the code out So to check thi we just us the doubl equal syntax here and thi will just check for match between two arrai So we have the start predict and the start true valu so check for those Okai so if we just have a look at what we have here we get thi arrai of true or fals So these on look particularli good but fine we just want to calcul the accuraci here So take the sum and we also want to divid that by the length Okai so that will give us our accuraci within the tensor and we just take it out us the item method but we also just need to includ bracket around thi becaus at the moment try to take item of the length valu Okai and that give us our veri poor accuraci on thi final batch So we can take that and within here we want to append that to our accuraci list and then we also want to do that for the end the predict as well And just let that run through and then we can calcul our accuraci from the end of that And then we can have a quick look at our accuraci here We can see fortun not as bad as it first seem So get a lot of gener pretti good So of cours if we want to get the overal accuraci all we do is sum that and divid by the length And we get for an exact match accuraci So what I mean by exact match is sai if we take a look at a few of these that do not match so we have a match on the fourth batch although that be particularli us becaus we see that batch right now So just take the last batch becaus we have these valu here Now if we look at what start true is we get these valu Then if we look at start pred we get thi So none of these match but a coupl of them do get pretti close So these final four all of these count as on the exact match But in realiti if you look at what we predict for everi singl on of them predict just on token befor so get quit close but not an exact match so it score zero So when you consid that with our accuraci here that mean that thi model is actual probabl do pretti well not perfect of cours but do pretti well So overal I mean everyth for thi video gone all the wai through thi If you do want to code for thi gonna make sure I keep a link to it in the descript so check that out if you just want to sort of copi thi across But for now everyth so thank 
How to Build Q&A Models in Python (Transformers)
Hi and welcom to thi video on question answer with Bert So firstli go to have a look at the transform librari and go to look at how we can find a Q&A model And then go to look at the Q&A pipelin So go to look at actual load a model in python us the transform librari go to look at token how we load a token and what exactli token is actual do And then go to take a look at the pipelin class which is essenti a wrapper made avail by the Hug Face Transform librari And it basic just make our job in term of build a Q&A pipelin incred easi So go to cover all those go to be quit straightforward and quit simpl So just get straight into it Okai so when do question answer essenti ask the model a question and pass a context which is what you can see here for the model to us to answer that question So you can see down here we have these three question So what organ is the IPCC a part of And then the model will read through thi and us it languag model to figur out which organ the IPCC is part of which is not inher clear from read thi We can see got IPCC here and is a scientif intergovernment bodi under the auspic of the Unite Nation So clearli the IPCC is a part of the Unite Nation but not clear not definit sai that in thi but onc actual built thi model it will quit easili be abl to answer each on of these question without ani issu So the first thing we want to do is go over to the Hug Face websit And on the Hug Face websit we just want to go over to the Model page So here Okai and on thi Model page the thing that we want to be look at is thi question and answer task So here we have all these task becaus when work with transform thei can work with a lot of differ thing Text summar text classif gener load of differ thing But what we want to do is question answer So we click on here and thi filter all of the model that ar avail to us just pure for question and answer So thi is the sort of power of us the Hug Face Transform librari It alreadi ha all these pre-train model that we can just download and start us Now when you want to go and appli these to specif us case you probabl want to fine tune it which mean you want to train it a littl bit more than what it is alreadi train But for actual get us to how all of thi work all you need to do is download thi model and start ask question and understand how everyth is actual function So obvious a lot of model here got model for question answer and new on be ad all the time A few of the on that I would recommend us ar the DeepSet model So here ar the DeepSet model eight of them for question answer The on that we will be us is thi BERT Base Case Squad Another on that I would definit recommend try out is thi Electra Base Squad But we will be stick with BERT Base Now call DeepSet here becaus from the DeepSet AI compani and thi model is be pull directli from their GitHub repositori So DeepSet is actual the GitHub organ and then thi is the repositori BERT Base Case Squad BERT is obvious the model BERT from Googl AI Base is the base version of BERT So you can see here we have BERT larg just a larg model us the base model Case just refer to the fact that thi model will differenti between uppercas and lowercas charact or word The altern to thi would be uncas here where no differenti between uppercas and lowercas And then Squad refer to the question answer data set that thi model ha been train on which is the Squad version So go to take thi model So you see DeepSet BERT Base Case Squad and we ar go to load it into here And all we need to do to do that is from transform So thi is the Hug & Face Transform librari We want to import BERT for question and answer So thi is a specif class and us thi class we can initi a few differ model not just thi specif model So you can see here we have thi BERT base case We can also initi thi BERT larg uncas Roberta And if a Distil BERT as well we can also load those in And what thi doe is it load that BERT base case And what thi doe is it load that specif model with it question and answer layer ad on there as well So thi model ha been train with the extra layer specif for question answer And we need to us BERT for question answer to load that Otherwis if you ar not us it with a specif us case and just want to get the model itself you can just us the auto model class like that But we want it for question answer so we load thi on Another thing to note is that we ar us the PyTorch implement of BERT here So Transform work by have both TensorFlow and PyTorch as altern framework work behind the scene In thi case us PyTorch If you want to switch over to TensorFlow all you do is add TF in front of that class So that is our model And to actual load that in all we do is copi thi and we us the from pre-train method And then thi is where the model name from over here come into plai So got deep set BERT base case squad And we just enter that in there And we just enter that in there Okai and with that actual just load the model all we had to do Of cours there ar a few other step Thi is just a model But there ar a few step befor we actual get the data to the model So we need to actual process the data So we have thi context here and thi is just a string BERT understand string BERT understand an arrai of integ where each integ repres a token ID And that token ID is veri specif to BERT And each on is uniqu and repres a specif word or piec of syntax punctuat or so on So we need to convert thi string into that specif BERT readi format And to do that we need to us a token So again go to go from transform go to import the auto token class Here we can us for exampl the BERT token But for thi we need anyth specif quit gener It will just load all of those map from the string or the word into the token no real issu there So we import our auto token And to initi it we just see thi practic the same syntax as what we us for the first on The same syntax as what we us befor We us thi fromPretrain method And then again us the same model OK And then with thi we can actual token our data So all we need to do is write token and code And then just pass in on of these question So pass in the first on So question And the first question there And two variabl that we will need to add in here ar the truncat which we will set to true And the pad which we also set to true So when we ar set up these model and the data go into them Bert in particular will expect token with everi input Now here when we look at thi we can see probabl on So each on of these word is most like to be truncat One so each on of these word is most like to be a token And then thi question mark at the end will also be a token So we have around token in there Now becaus we have pad thi will add a set of pad token onto the end of it to bring that total number of token up to Now altern sai if we had token in there we would be reli on the truncat to cut the final token to make it a total of And why we need those two argument in there So see what we get from thi You can see here that we have our token input So Bert will be abl to read and understand thi And essenti what we have so thi is the equival to what Thi is equival to organ and so on and so on Now what you might not see here is why we have thi So for Bert actual refer to a special token which look like thi And thi just signifi the start of ani sequenc So if we were to just take thi we can see that OK we get the same again We get thi which is the start sequenc Then we get the start sequenc token again becaus all put into here And the Bert the token is read that and convert into the And then we also get thi final special token as well And we can also see that here So thi is anoth special token which signifi the end of a sequenc or it signifi a separ point So if we write thi out you see here that the separ is And what I mean by it signifi a separ point or a separ So when we feed thi context and thi question into our Bert model Bert will expect it to be within the format someth like thi So we have the start sequenc token Then we will have our context token So thi will just be a list of integ which ar the token ID And then what we will see is a separ token here follow by our question Which again after thi is follow by a separ token And again after thi we get a set of pad token which look like thi And that will just take us up to the token amount And how the data go into Bert will look like We have that start sequenc we have the context we have a separ we have a question we have a separ and then we have pad alwai go to look like that when go into a Bert Q&A model So if we just remov that and thi here And what we want to do now is actual set up thi token and our model into a pipelin into a Q&A pipelin So again we get thi pipelin from the Transform librari So we come down here do from Transform import pipelin And now what we want to do is just initi a pipelin object So to do that we just write pipelin And then in here what we need to add is a model type So obvious you can see up here we have all of these differ task So summar text gener and so on The Transform librari need to understand or thi pipelin object need to understand which on of those pipelin or function we ar intend to us So to tell it that we want to do question answer we just write question answer And that basic set the wrapper of the pipelin to handl question answer format So see our input and for our input we will be pass a context and a question So see that it will convert into the right structur that we need for question answer which is the CLS context separ question separ and pad It will convert into that feed it into our token And the output of that token our token ID will be fed into BERT BERT will return us a span start and span end which is essenti two number which signifi the start posit and end posit of our answer within the context And thi pipelin will take those two number and appli them to our context to get the text which is our answer from that So essenti just a littl wrapper and it add a few function so that we have to worri about convert all of these thing So now we just need to pass in our model And the token as well And as simpl as that our pipelin set up So if we want to us that now all we need to do is write NLP And then here we pass a dictionari And thi dictionari like I said befor need to contain our question and context So the question And for thi we will just pass the first of our question up here again So thi question at the index zero And then we also pass our context which is insid the context variabl up here Okai And thi will output a dictionari contain the well we can see the score of the answer So that is the confid that thi is actual an answer Like I said befor the start index and end index and what those start index and end index map to which is Unite Nation So our question wa what is the answer And we got Unite Nation which is correct So let me just show you what I mean with thi start and end So if we do here we get the first letter of our answer becaus we ar go through here and it is pull out thi specif charact If we then add the first letter of our answer if we then add thi and go all the wai up to our end which is at we get the full set becaus what do here is pull out all the charact from you or charact all the wai up to charact which is actual thi comma here But obvious with Python list index we get the charact befor And that give us Unite Nation which is our answer So ask anoth question We have what UN organ establish the IPCC And we get thi WMO and Unite Nation Environ Program UNEP So if we go in here we can see it wa first establish in by two Unite Nation organ the World Meteorolog Organiz WMO and the Unite Nation Environ Program UNEP So here we have two organ and it is onli actual pull out on of those So I think the reason for that is all that is read is WMO and Unite Nation Environ Program So it is pull out those two organ in the end just not the full name of the first on So still a pretti good result And go down to thi final question So what doe the UN want to stabil And here get the answer of greenhous ga concentr in the atmospher So if we go down here we can see the ultim object of the UNFCCC is to stabil greenhous ga concentr in the atmospher at a level that would prevent danger anthropogen interfer with the climat system So again we ar get the answer stabil greenhous ga concentr So our model ha gone through each on of those question and successfulli answer them And all done is written a few line of code And thi is without us fine tune them at all Now when you do go and appli these to your own problem sometim you need to do ani fine tune and the model as is will be more than enough But a lot of the time you will need to fine tune it And in that case the answer is ye You fine tune it and in that case there ar a few extra step But for thi introduct everyth I want to cover there In term of fine tune I have cover that in anoth video So I will put a link to that in the descript But everyth for thi video So thank you veri much for watch I hope you enjoi and I will see you again next time Thank 
How-to Structure a Q&A ML App
Okai so I had an idea I thought mayb what we could do is actual put togeth a few video and what do in the next few video is build out a full Q&A project and thi is someth that actual been want to do for quit a while now and I think it will actual be realli cool I wa think through thi morn and I thought okai todai gonna start it So what you can see on the screen right now is the GitHub repo for thi project which is complet empti almost a read me a gitignor and thi on architectur draw that I sell liter you can see four minut ago So gonna show you that and go to explain what actual go to go through and I think gonna cover a lot of differ thing So I think thi is I think that is on of the reason why I think thi is such a cool project So at the moment thi is like the basic architectur of what we would need to build a Q&A model and the end goal is to have is to have a front end which look kind of like thi So have like a search bar here and have some visual up here gonna be a littl bit better than the six man gonna show you what I alreadi have and mayb we can us that or mayb do someth differ I know And what gonna be abl to do is ask a question here and go to be abl to answer the question base on a Soic philosophi book So I realli read ani of these read like littl bit but pretti interest and I think quit uniqu So as far as I know definit not anyth like thi out there at the moment where you ask a question and you get the answer back from some ancient Soic philosophi book And onli realli two book that thought of so far which is Medit by Marcu Aureliu and Letter from a Stoic by Seneca And the good thing with both of these is that we can find both of them onlin for free so we can us Python request to get these So just kind of put a littl list here what I think gonna need So the first on is actual extract and download thi data so us request for that And then onc we have actual got that data we need to pre-process it And when pre-process it I think that will just be a case of us regex more than anyth els but not sure yet so see So after we pre-process it and when we get into thi stuff over here so thi whole sort of stack that you can see without the API So thi is a typic call a reader or retriev reader and what we do is we us thi up here thi is our databas a document store elasticsearch document store and what gonna do is feed all of these Sorri get a littl bit messi with the color so let me chang it So what gonna do is take these and gonna feed them into our document store And onc we have that what we want to do is build thi retriev reader stack and it will allow us to queri the retriev down here and what the retriev will do is send that queri to elasticsearch here which is what you can see happen there and return from that get so mani differ context So all of the text from medit and letter from Stoic will split them up by mayb paragraph and store them in here and what these context will be ar the most relev paragraph And onc done that thi retriev will pass on the context to our reader model down here and what the reader model will do is sai given a long sentenc like thi or paragraph it will sai okai the actual answer that you want is actual onli these three word here and it will return those three word and what we want to do is return those three word in our answer back to our API but alongsid the answer also go to includ the the full context here as well So we get a few thing back and I think that like go to be more machin learn side of it but obvious we need to support the machin learn side and I mean the veri first part of that that you can obvious see here is the API so let me so let me write down so we have the ML part and for that go to us in someth call Haysec and onc we get out of that part we move on so we us differ color here move on to our API the API just us probabl fast API to set that up then onc we set that up we go on to our front end part and the front end I not a front-end develop I just mainli us Python but I do know Angular a littl bit so what gonna do is build all of thi part us Angular so thi will be our angular front end and essenti everyth be cover but no quit a lot in here in particular as well what miss is alongsid you know we have our reader model down here but what I want to try and do is rather than just take the reader model from Hug Face transform as we normal would I want to actual try train it and for that we need to us someth call MLM which is mass languag model so we would need to train a BERT model us MLM or fine-tun the BERT model I should sai on the data from our book up here and then also want to train it so that it perform Q&A and for that we need to us the squad data set probabl squad anywai so you know quit a lot that I think we would have to do to build thi and I think be pretti interest so that is what go to be cover in sort of the next few video and the on or the final littl thing okai so over here we have the Marcu Aureliu Superman and I thought mayb someth like thi would be cool I know thi is someth I drew ag ago and thi is thi sorri so thi is like thi is Marcu Aureliu and I think someth like thi mayb thi or someth like it would be pretti cool to just have in the middl of the web page and underneath we have a search bar and keep it pretti simpl so I think everyth realli for the plan and I mean the first thing go to do in the in the next video is actual sell request and download that data and mayb pre-process it as well or thei thei might be two video so everyth for thi video I hope that as excit about thi as I am becaus realli look forward to actual build all of thi I think be super cool and I mean ideal at the end on gonna look cool and two gonna learn like a huge amount of stuff if you even put all thi togeth so mani differ thing that you need to know in order to make everyth work so it should be realli cool and look forward to get start with it so I will see you in the next video where 
Streamlit for ML #5.1 - Custom React Components in Streamlit Setup
Todai go to have a look at how we can build custom compon in Streamlit us React and extern librari like Materi UI So there ar alreadi plenti of compon that you can us directli from Streamlit and also from the Streamlit commun Obviousli you mai find yourself need to us someth that alreadi exist So go through that process Fortun not too difficult It doe requir a littl bit of front end code But Streamlit doe make it pretti straightforward So have a quick look at what thi card might look like Thi compon is a card So all we have for our custom compon is thi littl card in the middl here The rest of thi is just Streamlit And then thi bit here is base on a Materi UI compon So if I go back we just have a titl subtitl text or bodi of text and a link here which is a Materi UI button Click on it and for now just taken to Googl But we can essenti like an object in Python we can pass whatev valu we like to thi compon So here ar the actual card from Materi UI that base thi on And with these card we can includ much more than what I have shown you Thi is just the absolut basic Like we can includ these littl profil imag drop down option and so on Even littl expand pictur So a lot we can do with it And as far as I know there anyth like thi avail directli within Streamlit So go ahead and set up our environ for develop thi compon So if you ar on Mac and you have Homebrew instal you can just go brew instal node And thi will instal Nodej and the node packag manag I alreadi have it instal so not go to rerun it If you ar not on Mac or you have Homebrew you can download it from the Nodej websit So go over here Thi here Nodejsorg And then download OK And you have Window Mac and so on So onc you have that instal you also need to instal Streamlit go to assum you alreadi have it instal But if not you just pip instal Streamlit Now I have Streamlit in anoth environ so go to activ that go to activ Streamlit And the Streamlit side of thing I will run from thi termin window Now to build our compon for Streamlit we need to follow a set of structur And fortun Streamlit provid us with a few templat that we can start from to make our live a littl bit easier So let me show you where you can find those So just type in Streamlit GitHub or mayb we can go with Streamlit Compon templat probabl easier Click on here OK And we have the Streamlit Compon Templat repo We just want to git clone thi so we can code over here Copi thi And then switch back to our termin Navig to whatev folder go to store the templat folder within So go to go to Document Project and go to write git clone And then go to clone the compon templat repositori OK open that VS Code and have a look at what we have in here OK So On the right over here we have our directori structur We have a few differ templat So go to go thi templat here which us React And within thi templat directori thi is our actual project So we can almost ignor the rest of thi stuff here Just anyth within templat is what we care about We have set up py So go to us thi and manifest for creat a pip packag which will contain our compon So to actual us that compon we put in solappcompon and then we just import it into our streamlit app script Or apppi usual And then us it And within here we just have the default file name here or default directori name of my compon go to chang some of these insid So here we have initpi So where initi our streamlit server or app from go through that fine We have thi front end which is anyth in here is the React side of thing So if we go a littl further we have some style The index And then in here we have these TSX file So in here us TypeScript So if you know TypeScript realli good If not not too much differ from Python I mean fairli differ but not unbeliev differ So I think if you know Python probabl read thi at least and kind of follow what do But I also realli know TypeScript I can just get through put someth simpl togeth with thi So most of the work go to do is go to be my compon TSX But we ar go to modifi a lot of these file as well So first renam everyth becaus everyth at the moment is us default name And we obvious ar build a custom compon We want to give it our own name That make it a littl more identifi So start by go to my compon up here Renam that to stCardCompon So st is just Streamlit Card Compon Down here we have myCompon Renam that to cardCompon Basic anywher that we have myCompon we modifi to cardCompon We updat these import So insid cardCompon here if we just find and replac So find myCompon and replac that with cardCompon Replac all of those See if we have anyth for myCompon No OK Save that and have a look at the index I think we also have someth here Just myCompon here OK Replac those So we have everyth in there And in set of Pi we also want to updat the name here So thi will chang it to Streamlit Card Compon OK So thi defin it for our packag later on And thi actual should align with the directori or the folder that we have here So not Streamlit but st And in our manifest here we also need to updat thi to be stCardCompon So thi need to point to the front end We have the build directori yet but we will So that need to go stCardCompon OK So that should pretti much be everyth we need to renam Mayb other littl bit ar actual in init Here we also want to chang thi to stCardCompon So here stCardCompon Anyth els OK That look good So go ahead and actual initi the current or basic default other than the thing renam version of thi compon and see how that look So go to and here we have the compon templat Inside the templat See the templat And then we have stCardCompon So in here we also have front end directori and insid here we have all of our packag and everyth So like the node side of stuff So first thing we need to do is actual instal the node packag we need for our CARD compon So So we just do npm instal and thi is just go to instal everyth from thi packagejson file So run that It might take a moment So just give it a second OK So done And next thing we want to do is instal the node packag that ar requir specif for the CARD that go to build So these ar all the node packag just instal extrem to function like the core of the packag that we need But becaus we ar us thi materi UI CARD thing we need a few extra thing So we need to npm instal again Thi is kind of similar to like a pip instal Although specif to thi directori So when we pip instal we instal them to our Python environ In thi case almost like the environ is thi directori in thi project So npm instal We need MUI materi MUI icon materi Icon materi Emotion react And also emot style Now I think thi is probabl go to give us an error see OK So ye we get thi error We have a depend conflict with I think if I rememb correctli So we have thi compon templat and thi throw some depend conflict with the MUI materi stuff So annoi but we can just get around it by ad thi legaci pia-dep Now I been plai around with thi and I notic ani issu pop up from us thi legaci pia-dep But obvious you just need to be awar just throw it in there all the time But in thi case it seem to work fine OK So with that we should be abl to run everyth So go ahead and do that First thing we want to do is so go to run two thing here go to run within thi front end directori We write npm start Thi is go to initi or execut the server that host our react compon And then we also need to open anoth termin window We also need to navig to our templat directori So cd document project It is compon templat Templat OK cd templat And in here we have a sd card compon OK We have thi initpi Initpi is like a templat app for just run and test our compon So we just write streamlit run initpi So previou video probabl seen me write streamlit run apppi Thi is kind of act as our apppi whilst in the develop stage But onc we switch to a releas version thi initpi we will modifi a littl bit And it will not be for do thi streamlit run It will do someth slightli differ It will extract everyth from a compil build distribut So now we have these host two server We have thi localhost If we go over there Open that go to see noth So thi is just host our React compon But our React compon by itself actual show anyth So we actual need to go over and open the streamlit localhost So that is in Now we have thi templat compon So we can click here and it updat thi basic Mayb even chang thi Enter We get thi So cool But obvious we want to build a custom compon And go ahead and do that in the next video For now I think leav it there set up the environ start run the default compon And I think good enough for now And in the next video be abl to realli focu on actual build the compon itself Which mean go to be toi with code over in the card compon TSX quit a bit And clean thi up And yeah Creat our card So I hope been us Thank you veri much for watch 
Making The Most of Data: Augmented SBERT
In thi video go to have a look at how we can make the most of limit data us languag data augment strategi and train approach More specif go to focu on someth call augment expert So you mai or mai not be awar that the past decad ha been sort of a renaiss or explos in the field of machin learn and data scienc and a lot of that especi the earli progress with thing like perceptron and neural network a lot of that wa research and discov back in the and and but we see that realli appli in industri or anywher realli until the past decad and there ar two main reason for thi So the first is that we have enough comput power back in the to train the model that we need to train and we also have the data to actual train those model Now comput power is not realli a problem anymor We sort of look at thi graph it depend on what model train of cours if you ar open AI and train or or whatev yeah mayb comput power is pretti relev but for most of us we can get access to cloud machineri person machin and we can wait a few hour or a coupl of dai and fine tune or pre-train a transform model that is good perform for what we need Now that obvious alwai the case until veri recent back in you see on thi graph here we have the IBM and you can see under the Y ax we have float point oper per second and a logarithm scale So linear scale just basic look like a straight line until a few year ago and then shoot up pretti impress how much progress is made in term of comput power Now like I said not realli an issu for us anymor We have the comput in most case to do what we need to do and and data is not as much of a problem anymor but talk about that in a moment So data again we have a veri big increas in data not quit as big as the comput power and thi graph here go quit as far back onli where I believ it wa at zettabyt and now or so in So a fairli big increas not quit as much as comput power over time but still pretti massiv Now the thing with data is ye a lot of data out there but is there that much data out there for what we need to train model to do and in a lot of case ye there is But it realli depend on what do If you ar focus on a more nich domain So what I have here on the left over here ar a coupl of nich domain not that much data out there on sentenc pair for climat evid and claim for exampl So where you have a piec of evid and a claim and whether the claim support evid or not there is a veri small data set call climat fever data set but not big For agricultur I assum within that industri not that much data although I have never work in that industri So I am not fulli awar I just assum probabl not that much And then also nich financ which I do at least have a bit more experi with and I imagin thi is probabl someth that a lot of you will find us as well Becaus financ is a big industri a lot of financ data out there but a lot of nich project and problem in financ where you find much less data So ye we have a lot more data nowadai but we have enough for a lot of the data that we need On the right here we have a coupl of exampl of low resourc data set So we have Adave from the Maldiv and also the Navajo languag as well So with these we kind of need to find a differ approach Now we can investig depend on your us case unsupervis learn TSEA which we have cover in a previou video articl and that doe work when try to build a model that recogn gener similar It work veri well as well But for exampl with the climat claim data we ar not necessarili try to match sentenc A and B base on their semant similar But try to match sentenc A which is a claim to sentenc B which is a claim As to whether that evid support the claim or not So in that case unsupervis approach like TSEA realli work So what we have is veri littl data and there realli ani altern train approach that we can us So basic what we need to do is creat more data Now data orient is difficult particularli for languag So data orient is not specif to NLP us across ML and more establish in the field of comput vision And that make sens becaus comput vision sai you have an imag you can modifi that imag us a few differ approach And a person can still look at that imag and think OK that is the same imag just mayb rotat a littl bit chang the color grade the bright or someth along those line We just modifi it slightli But still in essenc the same imag Now for languag a bit difficult becaus languag is veri abstract and nuanc So if you start randomli chang certain word the chanc ar go to produc someth that make ani sens And we when augment our data we want to just throw rubbish into our model We want someth that make sens So there ar some data augment techniqu And look at a coupl of the simpler on now So there is a librari call NLPorg which I think is veri good for thi sort of thing essenti a librari that allow us to do data augment for NLP And what you can see here is two method us vector and similar And what do is take thi origin sentenc So the quick brown fox jump over the lazi dog And just insert some word us So try to find what word think could go in here which word ar the most similar to the surround word And we have thi al-Ziari which I know I think it seem like a name to me But I am not sure That I think realli fit there So not great not perfect Lazi superintend dog That doe kind of make sens I feel like a lazi superintend dog is mayb a stereotyp or sure been in The Simpson or someth befor So okai fair enough I can see how that can fit in there Which again a bit weird not great Substitut for me seem to work better So rather than the quick brown fox we have the easi brown fox And rather than jump over the lazi dog jump around the lazi dog Which chang the mean slightli Easi is a bit weird there to be fair But we still have a sentenc that kind of make sens So good I think Now we have to us word to vet We can also us contextu word embed like with Bert And for me I think the result look better So for insert we get even the quick brown fox usual jump over the lazi dog So ad some word there It make sens I think good for substitut And onli do on word here And chang that to a littl quick brown fox instead of just quick brown fox So I think that make sens And thi is a good wai of augment your data and bring more data from less But for us becaus we ar us sentenc pair we can basic just take all of the data from sai we have A and B over here Imagin thi is a data frame And we have all of these sentenc and we have all these sentenc Now if we take on sentenc A alreadi match up to on sentenc B And what we can do is sai OK I want to randomli sampl some other sentenc and match them up to our sentenc A So we have three more pair now OK so if we did thi if we took three sentenc three sentenc and we made new pair from all of them not realli random sampl just take all the possibl pair we end up with nine new or nine pair in total which is much better if you extend that a littl further So from just a thousand pair we can end up with on million pair So you can see quit quickli you can take a small data set and veri quickli creat a big data set with it Now thi is just on part of the problem though becaus our smaller data set will have similar score or natur languag infer label but the new data set that just creat the augment data set have ani of those just randomli sampl new sentenc pair So no score or label there and we need those to actual train and model So what we can do is take a slightli differ approach or add anoth step into here Now that other set is us someth call a cross encod So in semant similar we can us two differ type of model We can us a cross encod which is over here or we can us a bi-encod or what I would usual call a sentenc transport Now a cross encod is the sort of old wai of do it and it work by simpli put sentenc A and sentenc B into a BERT model togeth at onc So we have sentenc A separ a token sentenc B feed that into a BERT model and from that BERT model we will get all of our embed output embed over here and thei all get fed into a linear layer which convert all of those into a similar score up here Now that similar score is typic go to be more accur than a similar score that you get from a bi-encod or a sentenc transform But the problem here is from our sentenc transform we ar output sentenc vector and if we have two sentenc vector we can perform a cosin similar or a Buclidean distanc calcul to get the similar of those two vector And the cosin similar calcul or oper is much quicker than a full BERT infer set which is what we need with a cross encod So I think it is someth like a BERT model so I think it is someth like for mayb mayb cluster vector us a cross encod an expert cross encod would take you someth like hour wherea with a bi-encod go to take you about five second So much much quicker And why we us bi-encod or sentenc transform Now the reason talk about cross encod is becaus we get thi more accur similar score which we can us as a label And anoth veri kei thing here is that we need less data to train a cross encod With a bi-encod if we I think the SBERT model itself wa train on someth like on million sentenc pair and some new model ar train a billion or more Wherea a cross encod we can train a reason cross encod on someth like or mayb even less sentenc pair So we need much less data and that work quit well what been talk about with data orient We can take a small data set we can augment it to creat more sentenc pair and then what we do is train on that origin data set which we call the gold data set We train our cross encod us that and then we us that fine-tun cross encod to label the augment data set without label and that creat a augment label data set that we call the silver data set So that sort of strategi of creat a silver data set which we would then us to fine-tun our bi-encod model is what we refer to as the in-domain augment SBERT train strategi And thi sort of what you can see thi flow diagram is basic everi set that we need to do to creat an in-domain or SBERT train process So alreadi describ most of thi so we get our gold data set the origin data set go to be quit small sai on to five thousand sentenc pair that ar label From that go to us someth like random sampl which just call random sampl go to us that to creat a larger data set sai we creat someth like a hundr thousand sentenc pair but these ar not label We have ani similar score or natur languag infer label for these So what we do is we take that gold data set and we take it down here and we fine-tun a cross encod us that gold data becaus we need less data to train a reason good cross encod So we take that and we fine-tun cross encod and then we us that cross encod alongsid our unlabel data set to creat a new silver data set Now the cross encod is go to predict the similar score or NLI label for everi pair in that data set So with that we have our silver data We also have the gold data which is up here and we actual take both those togeth and we fine-tun the by encod or the sentenc transform on both the gold data and the silver data Now on thing I would sai here is us to separ some of your gold data at the veri start so even train your cross encod on those good to separ them as your evalu or test set and evalu both the cross encod perform and also your by encod perform on that separ set So includ that in your train data for ani of your model Keep that separ and then you can us that to figur out is thi work or is it not work So that is in the main org ferment expert and sort of see thi is the same as what you saw befor just anoth thi is the train approach So we have the gold train cross encod We have our unlabel pair which have come from random sampl our gold data We process those for a cross encod to creat the silver data set and then the silver and the gold come over here to fine-tun a by encod So it for the theori and the concept and now what I want to do is actual go through the code and and work through an exampl of how we can actual do thi Okai so we have download the both the train and the valid set for our scsb data and have a look at what some of that data look like So scsb zero So we have sentenc pair sentenc on sentenc two just a simpl sentenc and we have a label which is our similar score Now that similar score vari from between zero to five where zero is no similar no relat between the two sentenc pair and five is thei mean that same thing Now see here these two mean the same thing as we Now we can see here that these two mean the same thing as we would expect So we first want to modifi that score a littl bit becaus we ar go to be train us cosin similar loss and we would expect our label to not go up to a valu of five but we would expect it to go up to a valu of on So all do here is chang that score so that we ar divid everyth by five normal everyth So we do that and no problem and now what we can do is load our train data into a data loader So to do that we first need to load our train data into a data loader So to do that we first form everyth into a input exampl and then load that into into our PyTorch data loader So run that and then at the same time dure train I also want to output a evalu sourc So how the cross encod do on the evalu data So to do that I import So here import from sentenc transform cross encod evalu import the cross encod CE correl evalu I again am us input exampl with work sentenc transform librari and I am import both text and label And here I am put all that develop or put all that valid of that data into that evalu Okai now I can run that and then we can move on to initi a cross encod and train it and also evalu it So to do that go to import from sentenc transform So from sentenc transform and just make sure work in Python go to import from cross encod a cross encod Okai and to initi that cross encod model call it C All I need to do is write cross encod veri similar to when we write sentenc transform initi and model We specifi the model from the face transform that we like to initi a cross encod from So that base on case and also a number of label that like to us So in thi case we ar just target a similar as well between and So we just want a singl label there If we were do for exampl NLI label where we have entail contradict and neutral label or some other label we would chang thi to for exampl But in thi case We can initi our cross encod and then from there we move on to actual train So we call model or Cfit and we want to specifi the data loader So thi is slightli differ to the fit function we usual us with sentenc transform So we want train data loader We specifi our loader that we initi just up here the data loader We need to do thi but if you ar go to evalu your model dure train you also want to add in evalu as well So thi is from the C correl evalu Make sure here us a cross encod evalu class We would like to run for sai on epoch and we should defin thi becaus I would also like to while train I would also like to includ some warm up set as well go to includ a lot of warm up set actual Although mention it talk about it in a moment So I would sai number of epoch is equal to on and for the warm up I would like to take integ So the length of loader So the number of batch that we have in our data set go to multipli thi by So go to do a warm up or do warm up set for percent of our total data set size or batch or percent of our total number of batch And we also need to multipli that by number of epoch Sai train two epoch we multipli that in thi case just on So not necessari but there So actual perform warm up for percent of the train step and I found thi work better than someth like percent percent percent Howev that be said I think you could also achiev a similar result by just decreas the learn rate of your model So by default So if I write in the epoch here defin the warm up set So by default thi will us optim param with a learn rate of to the minu OK So if you sai want to decreas that a littl bit you could go sai go to the minu to minu And thi would probabl have a similar effect to have such a signific number of warm up set And then in thi case you could decreas thi to or percent But for me the wai test thi end up go with percent warm up set and that work quit well So the final step here is where do we want to save our model So go to sai I want to save it into BERT base cross encod or sai BERT STSB cross encod And we can run that and that will run everyth for us just make sure actual Yep there we go So see run but not go to run it becaus alreadi done it So let me paus that and I will move on to the next step OK So we now have our gold data set which we have pull from HuginFac data set and just fine tune a cross encod So cross both of those off of here Thi and thi And now so befor we actual go on to predict label with the cross encod we need to actual creat that unlabel data set So do that through random sampl us the gold data set you alreadi have And then we can move on to the next step OK So just add a littl bit of separ in here So now go to go ahead and creat the augment data So as I said go to be us random sampl for that And I find that the easiest wai to do that is to actual go ahead and us a Panda data frame rather than us the HuginFac data set object that we current have So go to go ahead and initi that So we have our gold data That will be pdedata frame And in here go to have sentenc on and sentenc two So sentenc on That is go to be equal to stsb sentenc on OK And as well as that we also have sentenc two which is go to be stsb sentenc two Now we mai also want to includ our label in there Although I sai thi is realli necessari Or add it in So our label is just label And if I have a look here So we have go to overwrit anyth call gold So OK go to have a look at that as well So we can see a few exampl of what actual work with just go ahead and actual rerun these as well OK So there we have our gold data And now what we can do becaus reformat that into a kind of data frame We can us the sampl method to randomli sampl differ sentenc So to do that what I will want to do is creat a new data frame So thi is go to be our unlabel silver data set not go to be a silver data set Becaus we have the label or score yet But thi is go to be where we will put them And in here we again will have sentenc on And also sentenc two But at the moment empti noth in there yet So what we need to do is actual iter through all of the row in here So befor that just go to do from or import TQDMauto from TQDMauto import TQDM And just a progress bar So we can see where we ar I realli like to wait and have no idea how long thi is take to process And for sentenc on in TQDM So we have the progress bar And I want to take a list of a set So take all the uniqu valu in the gold data frame for sentenc on Okai so that will just loop through everi singl uniqu sentenc on item in there And go to us that and go to randomli sampl five sentenc from the other column sentenc two to be pair with that sentenc on And here sampl the sentenc two phrase that go to sampl ar go to come from the gold data of cours And we onli want to sampl from row where sentenc on is not equal to the current sentenc on becaus otherwis we ar possibl go to introduc duplic And go to remov duplic anywai but just remov them from the sampl in the first place So go to take that so all of the gold data set that where sentenc on is not equal to sentenc on And what go to do is just sampl five of those row like that Now from that just go to extract sentenc two So the five sentenc two phrase that we have there And go to convert them into a list And now for sentenc two in the sampl list that we just creat go to take my pair go to append new pair So pair ar append and I want sentenc on to be sentenc on And also sentenc two is go to be equal to sentenc two Now thi will take a littl while So what go to do is actual mayb not includ the full data set here So let me possibl just go mayb the first Yeah go to the first See how long that take And I will also want to just have a look at what we get from that So ye much quicker So we have sentenc on Let me remov that from there And just sai that top So becaus we ar take five of sentenc on everi time and random sampl it we can see that we have a few of those And anoth thing that we might want to do is remov ani duplic Now there probabl ani duplic here but we can check So pair equal pairsdrop duplic And then check the length of pair again And also print Let me run thi again and print Okai so there were not ani duplic anywai but a good idea to add that in just in case And now what I want to do is actual take the cross encod In fact actual go back to our littl flowchart So we have now creat our larger unlabel data set So good And now we go on to predict the label of our cross encod So down here what go to do is take the cross encod code here And what done is train thi alreadi and upload it to the Hugin base model So what you can do and what I can do is thi So go to write Jame Callum and it is call BERT STSB cross encod Okai so our cross encod And now what I want to do is us that cross encod to creat our label So that will creat our silver data set Now to do that go to call it silver For now I mean thi realli the silver data set but fine And what go to do is creat a list and go to zip both of the column from our pair So pair sentenc on pair sentenc two Pair sentenc on and pair sentenc two Okai so that will give us all of our pair again You can look at those Okai so just like thi And what we want to do now is actual creat our score So just take the cross encod What did we load it as CEpredict and we just pass in that silver data So do that run it It might take a moment Okai so definit take a moment So let me paus it go to just do sai becaus I alreadi have the full data set so I can show you that somewher els And have a look at what you have in those score So three of them So we have an arrai and we have these score Okai so that thei ar our predict our similar predict for the first three Now becaus randomli sampl a lot of these ar neg So if we go silver sai neg I mean more not relev So yeah we can see not particularli relev And just on must first issu with thi And you can try and modifi that by after creat your score If you oversampl and got a lot of valu or a lot of record and then just go ahead and remov most of the low score sampl and keep all of your high score sampl that will help you deal with that imbal in your data So what go to do is go to add to the label column those score which will not actual cover all of them becaus we onli have in here So let me mayb multipli that So thi you do thi obvious just so thei fit Okai and have a look Okai so we now have sens on sens two and some label And what you do although not go to run thi is you would write pairsto csv necessarili need to do thi if run everyth in the same notebook But probabl a good idea So with csv go to sai the silver data is a tab separ file And obvious the separ for that type of file is a tab charact And I want to includ those Okai and that will creat the silver data file that we can train with Which I do alreadi have So if we come over here we can see that I have thi file and we have all of these differ sentenc pair and the score that our encod ha assign to that So go to close that and go to go back to the demo And what now go to do is actual well first go back to the flow chart that we had go to cross off predict label And go to go ahead and fine tune the bui encod on both gold and silver data So we have the gold data have a look at what we have Ye and the silver go to load that from file So pdread csv Silvertsv And separ is a tab charact And have a look What we have Make sure all load correctli Look good Now what go to do is put both those togeth So all data is equal to goldappend silver And we ignor the index So go to get an index error Sorri True And all datahead Okai we can see that we hopefulli now have all of the data in there So check the length Yeah so definit a bigger data set now than befor with just gold Okai so we now have a larger data set We can go ahead and us that to fine tune the the bui encod or sentenc transform So what go to do is take the code from up here So we have thi train data And I think alreadi run thi befor so I need to import the import exampl here But what I want to do here is for row in all data And what we actual want to do here is for i row in all data becaus thi is a data frame It iter through each row We have row sentenc on sentenc two and also a label So we load them into our train data And we can have a look at that train data See what it look like Okai we see that we get all these input exampl object If you want to see what on of those ha insid you can access the text like thi Should probabl do that on a in a new cell So let me pull thi down here And you can also access a label to see what we what we have in there Okai so that look good And we can now take that like we did befor and load it into a data loader So let me go up again and copi that Where ar you Take thi Bring it down here And we run thi Creat our data loader And we can move on to actual initi the sentenc transform or by encod and actual train it So onc you run from sentenc transform go to import model and also go to import sentenc transform Now to initi our sentenc transform if been follow along with the seri of video and articl You will know that we do someth look like thi So go to convert and go to import the sentenc transform And go to import the sentenc transform So go to convert and that is go to be modelstransform And here just load a model from copi past transform So that base in case And we also have our pool layer So model again and we have pool And in here we want to includ the dimension of the vector that the pool layer should expect Which is just go to be vertget word embed dimens And also it need to know what type of pool go to us Are we go to us CLS pool Are we go to us mean pool max pool or so on Now we ar go to us pool and go to us a mean So mode mean token Let me set that to true So there ar the two sai compon in our sentenc transform And we need to now put those togeth So go to call model equal sentenc transform And we write modul And then we just pass as a list vert and also pool Okai So we run that We can also have a look at what our model look like Okai And we have a sentenc transform object And insid there we have two layer or compon First on is our transform a vert model And the second on is our pool And we can see here the onli pool method that is set to true is the mode mean token Which mean go to take the mean across all the word embed output by vert and us that to creat our sentenc embed or vector So with that model now defin we can initi our loss function So we do want to write from sentenc transform dot loss import cosin similar loss So cosin similar loss And in here we need to pass the model so it understand which paramet to actual optim And initi that And then we sell our train function or the fit function And similar to befor the cross encod although slightli differ So let me take that a littl further up from here Then take that and just go to modifi it So warm up go to warm up for of the number of step that go to run through We chang thi to model not C anymor And like I said there ar some differ here So we have a train object differ And thi is just a list of all the train object we have We ar onli us on And we just pass loader and loss into that Evaluat We could us an evalu not go to For thi on go to evalu everyth afterward The epoch and warm step ar the same The onli thing differ is the output path which is go to be vert stsporg it So go ahead and run that It should run check that it doe Okai so got thi error here So lucki that we check And thi runtim error found dtype long but expect to float And if we come up here go to be in the data loader or in the data that initi So here put int for some reason not sure why that is So thi should be a float The label in your train data And that should be the same up here as well Okai so here as well the cross encod We would expect a float valu So just be awar that make sure a note in the video earlier on for that Okai and okai continu through that and try and rerun it Should be okai now Oh I need to actual rerun everyth els as well So rerun thi Okai label Okai better Thi is thi just leav thi for a moment Just to be sure that is actual run thi time But it doe look good So yeah fine So it look good When for some reason in the notebook actual see the number of iter But okai yeah paus it now and we can see that actual run actual see the number of iter But okai paus it now and we can see that ye it did run through two iter So it is run correctli now good So great What I want to do now is actual show you okai evalu of these model So back to our flow chart quickli Okai so fine tune by encod just done it So now finish with our in the main augment expert train strategi And yeah move on to the evalu Okai so my evalu script here is mayb not the easiest to read But basic all do is import the embed similar evalu from down here load the the glue data SDSP again and take the valid split which we train on We ar convert it into input exampl feed it into our embed similar evalu And load the model The model name I pass through some command line argument from up here And then it just print out the score So let me switch across to the command line We can see how that actual perform Okai so just switch across to my other desktop becaus thi is much faster So I can actual run thi quickli So python and So go to run that evalu script And what go to pass here is we have all three All three the cross encod the sentenc transform train us augment expert and also a sentenc transform train pure on the gold data set So first have a look at the STSB gold data set train model So run thi It might take a moment to download it Okai so download and then got a score of So it correl to the predict of the model correl to the actual score with a correl So thei do correl not bad not great either have a look at the cross encod So again cross encod Okai and we get score of So as we would expect train on just the gold data the cross encod doe outperform the the by encod or sentenc transform And the final on would be okai with the augment data how doe the sentenc transform perform So run that again Not a wai to download And we get a much better score of So yeah the correl there is much higher than thi for the augment data set than if we had just us a gold data set So it realli realli ha improv the perform a lot Now thi is mayb an atyp perform increas someth like or point increas in perform and good But if you look at the the origin paper from Nil Reimer and Co thei found a sort of expect perform increas of I believ seven or nine point So thi is definit pretti signific Thi is definit a bit more than that But I think it goe to show how good these model or thi train strategi can actual be So it for thi video I hope thi ha been us and I hope thi help a few of you kind of overcom the sometim lack of data that we find in I think a lot of our particular us case 
How-to use the Kaggle API in Python
Hi and welcom to thi video where we ar go to go through set up and us the Kaggl API So the first thing we want to do is actual pip instal kaggl Now I alreadi have it instal so not go to go ahead and instal it again But onc you do have it instal You can try and import the kaggl modul And you will get thi error here So thi error simpli tell you that you could not find the kagglejson and you need to add it to thi locat here Now the reason tell you thi is becaus we us kagglejson to authent our API access Obviousli kaggl is not go to let anyon access their API You need to have a account befor you start download their data So to get our kagglejson credenti we simpli go over to kagglecom Now if you have an account have to go ahead and creat on Once creat your account you simpli go over to thi littl icon over here in the top right Click account And scroll down until you see thi API section Now all you need to do is creat a new API token And thi creat the kagglejson credenti and allow me to save them to my comput So just go to save them in my document for now And then head back to the notebook And go to see that we need to save it here So go to copi and past that across And here we have the directori that we need to put our kagglejson go to take my kagglejson and simpli move it into here OK so to check that work we simpli rerun thi cell And there we can see that our kaggl API is now function Now we actual need thi import kaggl Instead we need to import the kaggl API class from the kaggl API extend modul Once import that we simpli initi our API And then authent it Now readi to start download dataset And the kaggl API give us sever option for do thi The two that most like to us ar for download the competit dataset or standalon dataset Now a competit dataset is relat to a current or past competit So for exampl there is a sentiment analysi on movi review competit We can actual find it over here And you can see here in the URL kagglecom is follow by thi C And thi C essenti mean that thi is a competit And we can also see playground predict competit Everyth is tell us that thi is a competit And in thi competit it come with some data Now thi is differ to a standalon dataset And these standalon dataset can simpli be upload by anyon So if we go to sentiment dataset here You look in the URL and you can see that thi dataset ha been upload by Casanova And there is a slightli differ structur to the dataset page as well And you can see here a dataset First tab take us to data And we can scroll down and see the data that we can get here So there ar two differ method for download each on of these We download competit dataset with the standalon dataset method And we download standalon dataset with the competit dataset method So start with the competit dataset And to download on of these all we need to do is us the competit download file method And then we need to pass the competit name follow by the dataset So head back over here And you can see the competit name is thi And the data that we would like is traintsvzip And that is download into our current directori You can see it here Okai so how we download the competit dataset We can also download the standalon dataset To do so we us the dataset download file method And then here we need to pass the usernam follow by the dataset name So if we head over here You can find both in the URL So thi on is Casanova slash Sentiment We also need to specifi the file name Which in thi case is thi text here And then we just execut that And now we can see that we have download both file here Now you will notic that both of these file ar actual zip So we can just quickli unzip them us Python All we need to do is import zip file And with zip file We specifi the path to the data Which in thi case is just the file name And we specifi that we ar simpli read it And then we simpli call the extract all method And we have our dataset here And we see everyth is in the right format So everyth for thi tutori on us the Kaggl API If you have ani question let me know in the comment below 
API Series #2 - Building an API with Flask in Python
In thi video we ar go to explor how we can put togeth a pretti simpl API us Flask Now what go to do is go through and build essenti what you can see here So all of thi code here is a singl API not go to go through all of it in thi on video but go to go through the essenti and go to step by step put an API togeth that is veri similar to thi Now what thi API is do is pull inform from these two CSV file and either present that inform to us that data allow us to modifi allow us to add new entri or allow us to delet it So we get to us get post put and delet method Now if you ar follow along with thi tutori probabl want thi data so you can get it us thi code over here So over here we have make sure a link to thi in the descript so you can get it All do is download the code or those two data file from here and then store it in a local directori I mean you can chang that directori to whatev you want of cours Inside those two file we have locat here which is just a list of I think these ar actual real coffe cafe but I put thi I made thi CSV quit a long time ago so not sure Then over here we just have some made up user data as well And the end result will be someth like thi So thi is a program call Insomnia leav a download link to that in the descript but it just allow us to send API request veri easili So go to send an API request to thi address here which is the API I just show you And a endpoint We will have two endpoint which ar essenti just separ of the API So user and go to return all the data And then we also have locat And send that as well And we return all the locat Now go to start from scratch And the first thing that need to do is import everyth we need So we will need to import Flask So from Flask import Flask also need to import Flask RESTful So thi is anoth Flask librari that give us a few veri us tool for build an API So right from Flask RESTful import resourc API and rec pass Now specif to what do here we will also want to import panda and import AST Now you will probabl have to pip instal these as well So the pip for those is pip instal Flask and Flask RESTful Noth weird there All right So the first thing we need to do is initi our Flask app and then initi our Flask API So we do that So thi is typic Flask here So we write Flask name and then the API is API equal API app Okai Super easi Now we alreadi touch on it but our app is go to have two differ endpoint So go to have the user endpoint and also go to have the locat Endpoint And go to go through build the user endpoint but be abl to find the code for both of those in the descript But the wai that we separ both of these is we us a class object So go to creat our us class first So we write class user And then to initi thi as an endpoint for our API we actual need to pass the resourc object into it That will inherit the resourc object and expos it to differ like HTTP request like get post and so on For now not go to add anyth in there but to make it easier we need to write API add resourc And then we need to specifi the web address of thi or the or the almost like a page of a web address that the resourc or endpoint will be So sai user which is our class and then the actual endpoint locat which is user Okai So thi is sai I want you to map the class user here to thi place in our API So our web address So if for exampl the user is a user then go to pass the API So our web address So if for exampl the API wa locat at sai APIcom thi resourc would be locat at APIcom slash user Okai And also creat anoth on although not go to fill it out in thi video that will be call locat So again exactli the same thing Yeah Resourc just pass for now and copi that and do the same for locat Okai Now sort of the structur or the veri high level structur of our API itself So what I like to do is whilst we ar build or write the code for our API I like to also test it as we go along I think it make thing a lot easier So to run our API we need to write thi If name is equal to main we write apprun Okai that easi And now what we can do is just press the execut button up here Okai Okai So we got thi error becaus I okai I know why I did that So thi is thi need to be the class not a string of the class Okai So now run that again It should work Okai So we get thi Thi is a develop server Do not us it in product environ fine becaus just test it And then we can see thi is run at thi address Okai So if we take thi copi it and go to take it over into Insomnia Okai And just clear everyth that I wa do befor And go into here Okai And if we send thi we return anyth yet So just get thi not found But if we go to user we should at least return thi intern server error So thi mean that okai we ar not get anyth back becaus an error on server side So in our API but there is someth there So befor when we just had thi so rememb we have those two endpoint not actual us the base URL So not actual anyth there We get thi not found becaus there just anyth there But if we type in the locat or user we will return thi intern server error Now get thi becaus we written ani code So it know what to do when we send that request So go back to our code and start write someth So first thing I want to do is defin a get request So write find get bu self And in here what I want to do is load the user CSV So to get that go to write user path So thi is the specif directori that I have store my user data Of cours it might be differ for you And what go to do is down here go to us panda to just read in that data Whenev I call thi get request go to read it in So thi is veri similar to if we for exampl had a databas on a server somewher When the API get a request it would read that data quickli And return it to us So thi is a veri simpl version of that So go to do data equal pdread CSV And we just go user path And hopefulli the right path I think it is We can see So I come up to here Ye we have data So I guess that should be fine Ye that should be fine User CSV Cool And then what we want to do we pass a we pass a panda a data frame object through an API So we need to convert thi into a dictionari So write datatoD Now what we want to do is return We want to return the data So go to write data which is equal to data our dictionari And go to return the code which mean the API request wa success your data Okai save that And I believ it should should updat automat Let me check So come over here send that again to user Okai no So I think we need we need to restart the API So come over here Here just go to control C and run it again Now you can you can turn debug mode on So I think that will automat reload everyth So to do that first So down here in app run just go to set debug equal true Save that go to press control C down here and execut again And now when we make chang it should reload automat So come over here send that request again Now we see user path is not defin So see why that is wrong So path So save that and that should reload automat try again Okai perfect So now we get a respons there We get all of our user data Okai so just return that data frame in addit to the data frame Okai cool So how get request move on to put request I think So ad more data Okai so thi time what go to need to do is so us post request I said put a second ago on and post And in here the first thing we need to do is we need to retriev inform from the user that thei want to upload So the first thing we need to think about here is we ar try to get data from the user So thei ar go to send us like a new entri So what did we have befor in our user We have user ID name citi locat Okai so we want to us it to be abl to add a new row So each on us essenti a row in a data frame So we have locat citi name and user ID So go to send thi to the user So go to send thi to the user So name and user ID Now we want the user to be abl to add locat or locat locat name and citi Now the user ID we realli want the user go to add that go to be gener automat So we ignor that we do want the user to be abl to pass those three thing Now to do that go to us the recpars so request pars that we us up here And to initi that we need to write parser equal request sorri what is it up here recpars recpassrequestPars here That initi our parser and what thi is go to do is when we add argument into the request thi is go to read them and what go to do is pars them out into variabl So we ar go to within thi request we want to allow those three so locat name and citi So write parser add argument thi on will be locat and just go to do on locat at a time So do locat ID and then of cours we us a put request later if thei want to add more locat You sai requir is true thei do need to add on of these and the type of thi is a integ so write int Okai we need to add anoth on just go to copi thi so copi here and we have name citi and the type is for both of those string which I think might be the default so we probabl even need to includ that type string there but it is there so leav it So now we have the three argument that I us you can us to post inform to our API and what we need to do to extract whatev the user ha sent us is we do thi go to extract them into thi dictionari here which is call arg and we write parser pars arg like that Okai so now we have those argument and first just make sure thi is work so I want to just go to return what we just tri to send that so return locat lock equal arg locat id name and the citi Okai so just make sure actual work and set save it and that should reload automat go to insomnia and we ar send a post request here so post we have user we enter thi question mark so thi allow us to start ad our paramet so locat id realli go to be equal to go to sai five and so we we ad the ampersand symbol there to add anoth paramet so locat id and what ar the other on user user or name Jame me and for the citi oh put okai send that cool and then we just return that request back to azur you notic here we put lock and not locat id so we can tell not just return what we sent return what we have process through our function and return back to ourselv Okai cool so now we know pars those argument correctli now us them So again what I want to do is read our data so we do data equal data panda dot read csv again and read the path Okai so path and do user here so actual do we want locat in there at all I know Yeah mayb so no sai the user can specifi that user id oh not return anymor so remov that so I want the user to I want to be abl to post thi becaus then if we have a duplic of a previou user id not we can sai we can check if that it alreadi exist in the data or not now which do now so right if arg user id in the data that we just open so in data is it user id I think sure I can speak it okai I think it is I want to return an error messag sai thi alreadi exist write messag and all we do is write arg user id alreadi exist and what return here is the code which indic that some sort of conflict and there is a conflict we alreadi thi user id alreadi exist so sai conflict you you creat a new on alreadi there if it alreadi exist great and we go ahead and we creat that new user id So all we do is write data equal data dot append and then just append the new data that retriev from our arg so go to be we need user id which is go to be arg user id let me copi thi make it a littl bit quicker for locat just empti for now of citi and we have name okai and then we need to I think we need to ignor ye ignor index true true sorri okai and then save that data so to csv it is is it us path and go to set index equal fals so not save the index the row number and then I just want to return want to return the data again we return a data frame we need to convert it into a dictionari so write to dict and a respons go to save that test it see if it if it work so go to see modifi thi to user id now send that intern server error so see date frame no attribut to date so I need to add an underscor save again and send that again okai so now we can see we have more item in there the onli thing is ad five for all of them oh did I in the code right what did I do yeah veri smart okai so chang that name and thi is citi and on other thing is that these ar all thi should also be a string on thing I notic and thi should also be a string go to leav that out fine so let me send that again five alreadi exist becaus so at least we know that our conflict code work so creat anoth on number six go to call thi no Jim and go again okai so now creat number six as well did we creat gosh we creat a few there we oop so yeah now now creat number six here which is Jim live in London hi user id is six okai so I mean format perfect but good for now creat our first post request okai so pretti annoi that we have to do thi again so go ahead and build that so we need to first defin a delet function method and all we want to do is delet the delet function and then we can go ahead and creat a new on so go ahead and creat a new on and go ahead and creat a new on and go ahead and creat a new on it tell us what kind of a page thi need to be to have so creat a new recognizereiben and just be like thi imag that bring up us a new item if the pictur of the data sourc that our end up have is we areI go to have to do a new line So in the begin we did make a section where we ar it alwai sai you know that the cyberxu heBeic name your type And then what we need to do is pass those as well So just like befor parser pass org Yeah there we go So now we have so at thi point the user ha given us a user ID that thei want to delet and we need to go ahead and delet that So to delet it first we need to sai okai doe it exist So we sai if org user ID is in our data so we need to load the data first So let me load it here Okai so if that is in data user ID then we can go ahead and delet it And to do that just go to us the typic like panda data frame logic So we just write data equal data data user ID And we just want to select the row that ar not equal to the user ID that we just pull in So the org user ID right So just go to return all of the row set from the on that specifi Okai so we delet that Now we just need to save that to file again To CSV it is I keep forget the variabl path and then index is fals Okai so open file delet the entri save it back to CSV If we have done all that we can return to user code So just return return all the data again We need to do thi everi time We could just return a statement sai hei delet everyth So just go to return so we can see what actual do in realiti Probabl pretti sure you want to actual do thi But I suppos it depend mayb you would who know So return to code becaus success Now in the case that the user ID doe not exist here we want to sai okai we want to check Well alreadi check if it exist And then from there just like okai it exist what ar you do So we just return we sai what did we us befor when we sent a messag messag just messag Okai cool Messag and the messag is go to be user ID doe not exist someth along those line So arg user ID doe not exist Let me format that a littl nicer And then here we need to add the code which is I suppos not found So Okai So I think it realli So save it Again it should reload automat Head over to Insomnia again so go to user just go to look at what we have So sorri get user What do we get We have all these Now we want to delet user ID four and five Okai these two ar messi We want those So we want to specifi a user ID We want four So we chang that to a delet request We want four send that see what happen Why did that not chang Data Why that chang User ID Is it becaus thi need to be a string I think Okai I think Yeah probabl Let me try again Send No still noth Okai so that took me far too long to actual figur out what wa wrong But there anyth wrong There wa the string issu So we did need to convert over to a string to compar them But also so what I wa do here wa read the kei valu here So the row number and think four wa user ID is not We have two five that we mess up and made those earlier So there is no four for us to remov now We need to worri about it So test it with five instead which ar the two messi on that we need to remov So send that And now you can see that actual remov those entri So pretti good Now the on thing I did also notic is if I try and send that again we do get the not found So pretti cool right got everyth that we would expect with it Now I think it for the code The onli other thing I want to show you wa thi other So thi is the actual full API script here So in here I also includ anoth method here or anoth endpoint locat And we run that and we can mess around with the cafe locat as well So everyth So I think noth els I want to cover in thi video So leav it there So thank you veri much for watch I hope been us and see you again in the next on 
Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)
Hi and welcom to the video Todai go to be cover anoth techniqu in similar search call Local Sensit Hash or LSH Now LSH is a huge popular techniqu us in effici similar search Now there ar a huge number of compani that us similar search I mean you have big name like Googl I mean Googl is built from similar search Then you have Netflix Amazon Spotifi All of them ar constantli recommend you differ product film music and thei do that by compar you to other custom So thei ar perform a similar search between you other custom and identifi the most similar on Now you have two approach You have exhaust which is compar all of the data point just go to call them vector from now on becaus what be us So compar all these vector and obvious slow Approxim search allow us to approxim those vector restrict our scope to a more relev rang of vector and so on So it caus a lot of differ techniqu not just on techniqu here The on go to be cover todai is low-pass sensit hash So at it core LSH is a hash algorithm which attempt to maxim hash collis So what we see on screen right now is a dictionari like a typic Python dictionari in the wai that it hash differ item So we have our kei which ar item that hash We process them through a hash function and that hash function attempt to minim hash collis eg to not put kei in the same bucket It want everi kei to go to a separ bucket and then these ar connect Then thei contain the valu but connect to the valu that we relat back to our kei So a Python dictionari our Python dictionari But not want to minim collis We ar want to maxim the collis So what we see here is a hash function that maxim those collis So thi is essenti what LSH is do So we ar attempt to for ani similar kei so these here and these here all similar enough for us to want to put them into the same bucket So put two of them into here and then the other three into thi bucket Now there ar quit a few differ wai of do thi and there ar a lot of differ LSH method In fact LSH is a veri gener term that appli to a lot of differ algorithm And the on that we will be cover is what I see as the tradit version So it is the origin version of LSH And what be cover in thi video is shingl min hash and that LSH function So get to understand why veri soon So here is the overview of the process that go to be walk through So we have shingl So we have at the veri start we have thi text So fly fish flew by the space station Now just a string And what we want to do is extract all of the uniqu pair of text So when we sai shingl k shingl And in thi case our k valu is two becaus take two charact at onc If we were to take k equal four for exampl then we would take like pace and then move on we take ac and a space and so on So the shingl And from that we creat a set So if we have duplic shingl we remov those So we just end up with on So in thi I know if we do have ani duplic but sai mayb down here we have i n again becaus we also have it up here We would end up with just a singl i n in the set You have two And then we want to encod those So that mean we take a vocabulari from all of our text and not just thi on sentenc but have more than on sentenc obvious that compar And us that to build a on vector from the vocab and our shingl set Then we process that through someth call a min hash function which produc thi dens vector or signatur So thi thing down here OK that is call a signatur And then we band that into thi final bit here Thi is our actual LSH process So we band that vector into multipl sub vector and then we hash them So where we find that we have ani two sub vector go to the same hash bucket then that mean that the full vector that thei both come from is consid or the two full vector that thei both come from ar consid a candid pair And we take those and we then calcul some other we calcul the similar between them OK so first step in our process like we discuss is the shingl oper So shingl is simpli where we take a window of length K charact and we simpli move that down through our text like you can see here And from that we creat the shingl set So in Python what we do to shingl these three sentenc we have here is creat a shingl function here And thi is go to take some text which is a string And go to sai go to defin the K valu of the number of charact we take within each window which is obvious an integ Now we initi the valu of the string Now we initi our shingl set here make a string initi And then what we do is for i in rang And then here we want to go from the or we want to go to the length of our text minu K So minu that window length plu on becaus we want to go right up to the end of that And then here all we do is shingl set dot append And then we write so we have the text and we want to go from i up until i plu K Okai our shingl list I suppos And then we want to return a set So thi will remov ani duplic that we have So shingl set Okai So our shingl function And we just want to process each on of our sentenc through that So go A equal shingl A Also we need to defin K which can be two just defin K here Okai And then have a look at what we have And we see that we have thi shuffl no order to our set here And we see that we have all of the pair of word in there So we have S for the sort of the space part here or station actual could be either And if we try and find okai so here we have the veri sort of fly or fly You have the L Y there as well Iron So our shingl set And with thi we have all of our shingl So the next step is to creat our vocabulari which is just all of our shingl our shingl set a union togeth So to creat that all we do is we go A union B dot union like that You can see again we have just a lot more text in there now or a lot more mani more shingl That is our vocab So now we have our shingl set and we have our vocab So we can tick both of those off Now what we need to do is creat our on encod over here And the onli other thing we need is a zero vector So two well I mean more than two wai to do thi but I think two wai of think about it Normal the more effici wai would be to creat a NumPi arrai full of zero and then just add the on in where we have match between our vocab and shingl set But not go to do that just go to keep thing like incred simpl in the code that write So go to do A on hot Or the on thing we should do is make thi a list becaus we want order in our vocab and not have it shuffl So what we do here is we sai on for x in A or sorri no on if x is in A els zero for x in vocab So what do here is what do here is loop through the vocab and everi singl shingl within there sai if that exist in our signatur make that point in our list a on otherwis make it a zero So simpli our on hot encod So if we do ABC and then we have a look at our A on hot we see that we have thi on hot encod or thi spars arrai Now min hash is the next step in our process and it allow us to convert our what ar current spars vector into dens vector which we call signatur Now what you see here is a run through of how we do thi for mayb on signatur We want to do that for multipl signatur though So we would actual run through thi process multipl time So what do here is creat a randomli permut arrai which count from on to the length of our vocab And then what we ar essenti do I know so in thi web basic shuffl it and then count through until we find the first align to on within our vector In realiti you just take all of your valu and you find a minimum on that align to on So if us NumPi which see later on just show you the code not go to actual write all of it though So in code that would look someth like thi So we would start with a list which is the rang from on to the length of our vocab And if we have a look at that we just sai account go to shuffl that So from random import shuffl And we just do it like thi So it modifi it in place So we need to do anyth there So view that Okai so now shuffl that shuffl it twice now but fine And just loop through five of those So four we can look for more Four i in rang from on to ten What go to sai is I just want to print i which align to the hash sampl index for that valu Okai if we print that we see so on the valu on where is it Here is at index Two is at and so on And essenti what do here is sai loop through these identifi thi index and thi index in our one-hot vector Doe it align to a on And see that here we find the first on at eight And that mean our signatur valu for thi point is for thi min hash vector and our one-hot spars vector here That signatur valu will be eight And we repeat that for multipl min hash vector Which is what you can see here So if we were to work through thi so we saw on here that doe not align to a on So we work up to two and we find that it doe align to a on So that is why we have thi here And then we go on to thi on here We find on and we work up to two We have thi here And then we go on to thi on here We find on doe not align Two still doe not align Three doe not align And four doe align So then we assign a four in our min hash function We go along and keep do that to creat our signatur Okai So go to us these function here just what we wrote befor but put into a cleaner format And what go to do is creat min hash vector Run that And then here we ar go to run each of our one-hot spars vector through our creat hash function which is here And go to convert them into our signatur as we describ befor And we see here that we have also what did I mention So here we have min hash vector which mean we have a length of for each signatur So what we see here ar our dens vector And these ar just compress version of our spars vector And we can check that that is true by defin a creat a jaccard similar function So we take and here we take x and y both will be set And we just return the length of the intersect between both of those So the intersect between those is divid by the union of both of those So that is how you calcul jaccard similar Thi should be a y Okai And then if we do jaccard on both of those so we have a sig b sig These will have to be convert into set Forgot So like that And then if we also take the jaccard for I think just a and b right So copi that Okai So we get thi is and thi is Now if we look up here I think a and b ar not suppos to be veri similar So fine And then b and c should be similar So if we swap thi for c and then c here we should both get higher valu And thei should be and thei should be roughli in the same ballpark I mean not perfect becaus us a veri low number here onli us valu and typic us a lot more But fine So you can see that both align right So despit convert these into the signatur vector it recogn that thei ar veri similar And convert these into signatur vector it still recogn that thei ar reason similar So good what we want Now the final step in our whole LHS process is the LHS function itself So thi is essenti what it doe So we have our signatur over here which we built us the step that we just went through which you can see here And from that signatur we take a certain number of equal length subvector So we defin that us thi here thi b So b is So that mean we split a signatur into three differ subvector which we see over here And ideal what we want to be do here is sai okai we process our subvector each through a either a differ hash function or it can be the same hash function just as long as we us that same hash function for the equival subvector in anoth signatur which see in a moment make sens And onc we have multipl signatur go togeth through those hash function you can see here that equival on both side hash on hash on here These can all just be a singl hash function as well which is what go to do not realli go to us a hash function And what we get here is three opportun to identifi these signatur as be potenti candid pair which is where we consid it for further similar comparison In thi case hash three both collid down here So we sai okai that mean that a and b ar candid pair just go to put cand pair So thi act of split our signatur up into multipl subvector just give us more opportun to identifi similar becaus if we were to us the full vector the full vector would have to be veri similar for them to be put into the same hash bucket With thi we onli part of it to be veri similar So increas the chanc of us find those similar signatur So go to implement a veri simpl version of thi go to keep thi veri simpl Here just split our signatur vector So we add our signatur and b which is the number of band And the first thing we do is just make sure that our signatur can be split into b band equal So where we take the remaind after the divis here it must be equal to zero And then we sai we need to calcul the row So the number of row within each band which obvious just the length of the signatur divid by b And then we initi a subvector arrai or list And then we loop through and append subvector Realli simpl simpl implement And appli that to b and c So we have said that we want band So we onli have item or number within our signatur vector So obvious we onli get band of two row at a time And we should find that at least on of those match So what we do is we loop through and we sai if b row equal c row break Okai And we find veri quickli that there is a candid pair there So that mean that b and c the full vector will be consid as a candid pair do the same for a And we should find okai so for both a and b and a and c not consid a candid pair becaus just no similar there So good exactli what we want to happen So we can now see that that is our implement of thi So the LSH tradit LSH approach Now a few other thing that we cover but we should just touch on quickli And you can find so an articl link in the descript that which cover thi I walk through all of thi and there will also be a notebook where get these result from in the first place So you can also look at that That includ the NumPi implement of what just done which is slightli more effici although not super effici becaus I want it to still be readabl So what we have here is visual show the similar the cosin similar of our signatur vector and whether thei were consid as candid pair or not So these up here these ar our candid pair Thi is just a random sampl I think the actual full dataset is realli big So run thi all of them is super ineffici becaus also run everyth els through So I can just have the visual here But if you run just that on it it doe work fine So at the top there we have our candid At the bottom we have our non-candid We have some like so we can see that high similar doe correl with them be classifi as candid pair which is good obvious what we want And there is thi formula that I did not write down which I should have done which is P equal on minu on minu S which is our similar down here to the power of R which is the number of row in each band And all of thi to the power of B which is number of band Now that correl to thi line here thi probabl Obviousli P capit P So where come from And if we run thi with differ similar valu thi is the pattern that we get And obvious that correl you can see with whether someth is classifi as a candid pair or not And what we can do is we can modifi B to push the number of candid pair classif either up or down So here we have differ B valu The side we have black which is Then we go which is what we us befor and five So sai we found that not identifi enough candid pair We could push that down a littl bit Mayb we do too much So we could chang B from to And if we do that we see thi So in green you have our old result and our old probabl line And then in blue and pink we have the new on again or blue and magenta So what we see here is push that down So chang B to And now return more result So over here we have these for exampl which we were not return befor And there ar also more valu in here as well And there ar less valu down here So the result of us modifi B So we can visual that like so So if we increas B we move it in thi direct which increas the number of candid pair which also increas the number of fals posit that go to return Thi line by the wai is our threshold So similar threshold is basic where we want the cutoff to be between thing be identifi as candid pair and not candid pair like our target almost Or if we want to reduc the number of candid pair becaus mayb get too mani fals posit we can push it thi wai which will result in less candid pair but also result in more fals neg So non candid pair where we should have candid pair So just a case of balanc both of those But everyth for thi video I hope been us 
How to Index Q&A Data With Haystack and Elasticsearch
Okai so in thi video what go to do is actual index our data so at the moment we just have all of our paragraph from Medit by Marcu Aureliu and to do thi we ar go to be us the Elasticsearch document store So of cours if us Elasticsearch we first need to actual download and instal it so just go to take you through those step now And all we need to do is head on over to thi websit up here and elasticssearchco and you can see the address just there Now go to follow the instruct for Window but of cours if on Linux or Mac just follow through veri similar either wai So here go to instal it on Window us the MSI instal So just scroll down here and we can see we can download the packag from thi link so download that and onc you download it just open it and see thi window pop up So onc you see thi window pop up we just go through with all of the default set So instal as a servic and continu through obvious if you do need to chang anyth chang it but for me noth here that I want to modifi Notic here we have the HTTP port and us be us that later We just continu through here default set and then we click instal and we just let that instal Okai so now that instal Elasticsearch we can go ahead and actual check that run So to do that go to import Python request and whenev we interact with Elasticsearch either go to be through Haystack or it will be through the request librari and just interact with the Elasticsearch API So to check the health of our cluster so essenti check that actual up and run What we need to do is send a GET request to localhost and if you rememb earlier we had it wa port Of cours if the port on your wa differ modifi it thi is just the default valu and after thi we need to reach out to the cluster endpoint and we ar check the health and then just format that as a JSON So what you should see here is we have our cluster which is Elasticsearch It mai have a differ name if you modifi it but by default Elasticsearch The statu is yellow which basic just mean we have on node up and run You can have multipl node in Elasticsearch and for your cluster health to be green it will expect your shard of index to have backup shard across differ node and obvious we do that if we onli have on node but complet fine for us becaus just in develop If in product ye probabl want it to have those backup shard If none of that made ani sens worri about it we realli need to know ani of that for what do here Now what we can also do is we can check if we have ani indic alreadi Now if I take a look at mine I will alreadi have some indic set up which just set up prior to record thi and to check that we go to localhost again and thi time we want to call the cat API which is what we would call whenev we want to see data in a tabl human readabl format rather than JSON and what check here ar the indic And just add text on there so we can actual see that and thi is quit messi so if we just print it instead look a bit cleaner Okai so you can see I have these two indic you I think have either of those no you have either of those so worri about that Now what we ar go to do is creat a new index which will be call Aureliu and that is where we will put our document Now to actual implement that we will be go through the Haystack librari which you can pip instal farm Haystack and what we want to do is from Haystack dot document store elast search import elast search document store So thi is our document store instanc and of cours thi is not awar of our elast search instanc we need to initi that so store it in a variabl call doc store and all we write is elast search document store Now we need to initi it with the paramet so it know where to connect to our elast search instanc So to do that we write host and thi is local host Now if you have a usernam and password set which you by default you will need to enter them in here I have ani set so no worri And then we also need to specifi our index and at the moment we have an Aureliu index and fine becaus thi will initi it for us So just call it Aureliu Now if we go down here we can see what it actual did so it sent a put request to here localhost Aureliu So how you creat a new index After that what we want to do is first import our data So we have the data here which I got from thi websit and process with thi script which you can find on GitHub keep a link in the descript so you can just go and copi that if you need to Now I realli done much pre-process pretti straightforward and all you need to do here is actual open that data So we do that with open and from here that data file is locat two folder up in a data folder call meditationstxt go to be read that and all we do is data equal fread and then if we just have a quick look at the first charact there we see that we have thi newlin charact and that signifi a new paragraph from the text So what we want to do here is split the data and then we can see that we have a newlin charact So what we want to do is split the data by newlin and then if we check the length of that see that we have separ paragraph in there So what we now want to do is we want to modifi thi data so that in the correct format for Haystack and Elasticsearch So that format look like thi so it expect a list of dictionari where each dictionari look like thi from the text and insid here we would have our paragraph So each on of these item here and then anoth option field call meta and meta contain a dictionari and in here we can put whatev we want So for us I think at the moment realli that much to put into here other than where it came from so the book or mayb the sourc is probabl a better word to us here and all of these ar come from Medit Now later on we will probabl add a few other book as well and then the sourc will be differ and when we return that item from our retriev and our reader at least be abl to see which book came from him It would also be pretti cool to mayb includ like a page number or someth but at the moment with thi there ar no page number includ so not do that at the moment So the format that we need and go to be a list of these So to do that just do some list comprehens So go to write thi and just copi thi I think yeah it should be fine copi thi and just indent that and in here we have our paragraph and sourc Medit for all of them and then we just write for paragraph in and data okai so yeah that should work and if we just and if we just check what we have here okai so what we want so we have text we have the paragraph and then in here we have thi meta with a sourc which is alwai Medit at the moment so that look pretti good and just doubl check the length again it should be okai perfect now what we need to do is index all of these document into our Elasticsearch instanc and to do that super easi all we do is call docstor becaus do thi through Haystack now and we do write document and we just pass in our datajson and that should work Okai cool so we can see here what done as sent a POST request to the Bulk API and sent two of them I assum becaus it can onli send so mani document at onc so pretti cool and now what I want to check is that we actual have document in our Elasticsearch instanc so to do that go to revert back to request so do requestsget again go to our localhost and here we need to specifi the index that we want to count the number of entri in and then all we do is add count onto the end there and thi will return a JSON object so we do thi so that we can see it and sure enough we have item in that document store So if we head on back to our origin plan so up here we had medit now got that and also set up the first part of our sack over here so Elastic now ha medit in there so we can cross that off now the next step is set up our retriev which cover in the 
Training BERT #5 - Training With BertForPretraining
Hi welcom to the video Here go to have a look at how we can pre-train BERT So what I mean by pre-train is fine-tun BERT us the same approach that ar us to actual pre-train BERT itself So we would us these when we want to teach BERT to better understand the style of languag in our specif us case So jump straight into it but what go to see is essenti two differ method appli togeth So when pre-train us someth call mass languag model or MLM and also net sentenc predict or NSP Now in a few previou video cover all of these So if you do want to go into a littl more depth then I would definit recommend have a look at those But in thi video just go to go straight into actual train a BERT model us both of those method us the pre-train class So we need first to import everyth that we need So go to import request becaus go to us request download data us which is from here You find a link in the descript for that And we also need to import our token and model class from transform So from transform go to import BERT token and also BERT for pre-train Now like I said befor thi BERT for pre-train class contain both an MLM head and an NSP head So onc we have that we also need to import torch as well So let me import torch Once we have that we can initi our token and model So we initi our token like thi So BERT token and from pre-train And go to be us the BERT base uncas model Obviousli you can us whichev BERT model like And for our model we have the BERT for pre-train class So our token model Now get our data need to worri about that warn just tell us that we need to train it basic if we want to us it for infer predict So we get our data go to pull it from here So let me copi that And just requestsget And past that in there And we should see a code good And so we just extract data us the text attribut So text equal that We also need to split it becaus a set of paragraph that ar split by a new line charact And we can see those in here Now we need to power data both for NSP and MLM So go with NSP first And to do that we need to creat a set of random sentenc So sentenc A and B And then we need to creat a set of random sentenc So we need to creat a set of random sentenc So sentenc A and B where the sentenc B is not relat to sentenc A We need roughli of those And then the other we want it to be sentenc A is follow by sentenc B So thei ar more coher So basic teach BERT to distinguish between coher between sentenc So like long term depend And we just want to be awar that within our text so we have thi on paragraph that ha multipl sentenc So we split by thi We have those So we need to creat essenti a list of all of the differ sentenc that we have that we can just pull from when creat our train data for NSP Now to do that go to us thi comprehens here And what we do is write sentenc So for each sentenc for each paragraph in the text So thi variabl For sentenc in parasplit So thi is where get our sentenc variabl from And we just want to be awar of if we have a look at thi on we see we get thi empti sentenc we get that for all of our paragraph So we want to not includ those So we sai if sentenc is not equal to that empti sentenc And also go to need to get the length of that bag for later as well And now what we do is creat our NSP train data So we want that split So go to us the random librari to creat that random We want to initi a list of sentenc a list of sentenc and also a list of label And then what we do is go to loop through each paragraph in our text So for paragraph in text We want to extract each sentenc from the paragraph So go to us it similar to what done here So write sentenc Thi is go to be a list of all the sentenc within each paragraph So sentenc for sentenc in parasplit by a period charact And we also want to make sure not includ those empti on So if sentenc is not equal to empti then onc there what we want to do is we want to get the number of sentenc within each sentenc or sentenc variabl So just get length And the reason we do that is becaus we want to check that a coupl of time in the next few line of code And first time we check that is now So we check that the number of sentenc is greater than on Now thi becaus concaten two sentenc to creat our train data we want to get just on sentenc We need it where we have for exampl in thi on we have multipl sentenc so that we can select like thi sentenc follow by thi sentenc We do that with these becaus no guarante that thi paragraph here is go to be talk about the same topic as thi paragraph here So we just avoid that And in here first thing we want to do is set out start sentenc So thi is where sentenc A is go to come from And go to randomli select sai for thi exampl we want to randomli select ani of the first on two three sentenc Okai want to select ani of these three but not thi on becaus if thi sentenc A we have a sentenc B which follow it to extract So we write random randint up to the length of num sentenc minu two Now we can now get our sentenc A which is append and we just write sentenc start And then for our sentenc B we want to select random on from bag up here of time we want to select the genuin next sentenc So sai if randomrandom so thi will select a random float between and greater than And sentenc B is go to be make thi our coher version So sentenc start plu on And that mean our label will have to be zero becaus that mean that these two sentenc ar coher Sentenc B doe follow sentenc A Otherwis we select a random sentenc for sentenc B So do append and here we would write bag and we need to select a random on So we do random same as we did earlier on for the start we do random randint from zero to the length of the bag size minu on So we also need to do the label which is go to be on in thi case We can execut that Now that will work I go a littl more into depth on thi in the previou NSP video So leav a link to that in the descript if you want to go through it And now what we can do is token our data So to do that we just write input and we us a token So thi is just normal you know hug face transform And we just write sentenc A and sentenc B So hug face transform will know what we want to do with that It will deal with format for us which is pretti us We want to return PyTorch tensor So return tensor equal pt And we need to set everyth to a max length of token So max length equal The truncat need to be set to true And we also need to set pad equal to max length Okai So that creat three differ tensor for us Impart ID token type ID and attent mask Now for the pre-train model we need two more tensor We need our next sentenc label tensor So to creat that we write input next sentenc label And that need to be a long tensor contain our label which we creat befor in the correct dimension So why us the list here and the transpos And we can have a look at what that creat as well So look at the first We get that Okai And now what we want to do is creat our mask data So we need the label for our mask first So when we do thi what do is go to clone the input ID tensor go to us that clone for the label tensor And then go to go back to our input ID and mask around of the token in that tensor So creat that label tensor go to be equal to input input ID detach and clone Okai So now got our mask data Okai So now see in here we have all of the tensor we need but we still need to mask around of these befor move on to train our model And to do that us creat a random arrai us the torch rend That need to be in the same shape as our input ID And that will just creat a big tensor between valu of zero to on And what we want to do is mask around of those So we write someth like thi Okai And that will give us our mask here but we also want to mask special token which we ar do here mask classif token and also mask pad token up here So we need to add a littl bit more logic to that So let me just add thi to a variabl So we add that logic which sai and input ID is not equal to on zero on which is our CLS token which is what we get down here See the impact See we get fault now And we also want to do the same for our separ token which is on zero two We see ani of those And our pad token we us zero So you see these ar all that will go fals now like so So our mask arrai And now what we want to do is loop through all of these extract the point at which thei ar not fals So where we have the mask and us those indic valu to mask our actual input ID up here To do that we go for i in rang input input ID dot shape zero Thi is like iter through each row And what we do here is we get select So these ar the indic where we have true valu and mask arrai And we do that us torch flatten mask arrai at the given index where thei ar non-zero And we want to creat a list from that Okai So we have that Oh and so I want to show you what the select look like quickli So just a select of indic to mask And we want to appli that to our input input ID So at the current index and we select those specif item and we set them equal to on zero three which is the mask token ID Okai So our mask And now what we need to do is we need to take all of our data here and load it into a PyTorch data loader And to do that we need to reform our data into a PyTorch data set object And we do that here So main thing to note is we pass our data into thi initi that assign them to thi self encod attribut And then here we sai okai given a certain index we want to extract the tensor in a dictionari format for that index And then here just pass the length of how mani tensor or how mani sampl we have in the full data set So run that We initi our data set us that class So right data set equal medit data set pass our data in there which is input And then with that we can creat our data loader like thi So torch util data data loader And we have data set Okai So readi Now we need to set up our train loop So first thing we need to do is check if we ar on GPU or not If we ar we us it and we do that like so So devic equal torch devic cuda if torch cuda is avail Else torch devic CPU So sai us the GPU if we have a cuda enabl GPU otherwis us CPU And then what we want to do is move our model over to that devic And we also want to activ the train mode of our model And then we need to initi our optim go to be us Adam with weight decai So from transform import Adam w And initi it like thi So optim equal Adam w We pass our model paramet to that And we also pass a learn rate So learn rate is go to be to the minu Okai And now we can creat our train loop So go to us TQDM to creat the progress bar And go to go through two epoch So for epoch in rang two we initi our loop by wrap it within TQDM And in here we have our data loader And we set leav equal to true so that we can see that progress bar And then we loop through each batch within that loop Up here so I actual set the batch My mistak So up here we want to set where we initi the data loader We want to set batch size equal to And also shuffl the data set as well Okai So for batch in loop here we want to initi the gradient on our optim And then we need to load in each of our tensor which there ar quit a few of them So we have inputskei We need to load in each on of these So input ID equal batch We access thi like a dictionari So input ID We also want to move each on of those tensor that us to our devic So we do that for each on of those And we have attent mask And next sentenc label and also label Label and also label Okai And now we can actual process that through our model So in here we just need to pass all of these tensor that we have So input ID And then we have token type ID Just copi thi Attention mask Next sentenc label And label Okai So quit a lot go into our model And now what we want to do is extract the loss from that Then we calcul loss for everi paramet in our model And then us that we can updat our gradient us our optim And then what we want to do is print the relev info to our progress bar that we set up us TQDM and loop So loop set descript And here I wa go to put the epoch info So the epoch current on And then I also want to set the post fix Which will contain the loss inform So lossitem Okai We can run that And you see that our model is now train So now train a model us both assign model and net sentenc predict And we need to take ani structur data to set up the model So we can just run that And we can see that our model is now train We need to take ani structur data just taken a book and pull all data and format it in the correct wai for us to actual train a better model which I think is realli 
Q&A Document Retrieval With DPR
Okai so in the previou video what we did wa set up our Elasticsearch document store to contain all of our paragraph from medit so we did that in thi script here and All togeth we onli have not that much data paragraph or document within our document store so What we now want to do is set up the next part of our Retriev reader stack which is the retriev and What the retriev will do is given a queri it will commun with our Elasticsearch document store and return a Certain number of context which ar the paragraph in our case that it think ar most relev to our queri So what we ar go to be do here and The first thing that we need to do is initi our document store again so just go to copi these and past them here and Thi would just initi it from what alreadi built so us the same index that alreadi exist so Just initi that and onc we have our document store Okai cool We have that now Now what we want to do is set up our DPR which is a dens passag retriev which essenti us dens vector and a type of effici similar search to emb these index as dens vector and then onc it come to actual search And find the most similar or the most relev Document later on it will us those dens vector and find the most similar on So explain that a littl bit better in a moment So first what we want to do is actual initi that So we do from Haystack dens retriev Import dens passag retriev Sorri the other wai around here so retriev dens And then put into a Variabl call retriev which us the dens passag retriev from up here And in here we need to pass a few paramet So the first thing is the document store So the document store is just what alreadi initi up so and Then we need to initi two differ model so the queri embed model And the passag embed model Now behind the scene Haystack is us the Hug Face Transform librari So what do is head over to the Model over there and see which embed model we can us for DPR Okai so here just search for DPR and find we have all of these model from Facebook AI Now with DPR the reason that so us for question answer is that we have What ar two differ model that encod the text that we pass into it so we have thi sort of setup dure train and What we see down here Are these two model we have thi EP BERT and EMP model we have thi EP BERT encod And we also have thi EQ BERT encod Now the EP BERT encod encod the passag or the context So essenti the paragraph that we have fed into our elast search model Thi is what be encod them into these vector here Now thi is dure train thi whole graph So all we will actual see when encod these vector is we will see the EP encod And thi will creat the EP vector And all go to do is feed in all of the document from elast search into thi Now onc all of these have been encod We then have a new set of dens vector And then have a new set of dens vector And all of those Will be fed back into our document store so back into elast Now when it come to perform similar search later on go to ask a question and that question will be process by the EQ encod So here we have our EQ encod And we have our question so that will go into here And that will encod our question And then send it over to elast and sai okai what ar the most similar vector to thi vector that we creat from a question And the reason that ask thi question is becaus go to be us a new set of vector to train our question And the reason that DPR is so good is That if you look at the train down here We ar creat these EP vector and these EQ vector that ar match so where we have a match question to a match context We ar train them To maxim the dot product And the align between those two vector so what happen is That a relev passag and a relev question Will come out to have a veri similar vector So on exampl that I like to us Is if our question Wa What is the capit of Franc The embed that I will creat from that will creat a context that look someth like the Capit Of Franc Is and you know someth here we know what it will put becaus it actual know what the capit Franc is just do linguist Transform to try and figur out what sort of context the answer would come from and then of cours when you feed thi context into elast The most similar Vector will be the on which contain the answer to our question Okai becaus the answer to our question which is someth like the capit Franc is Pari now We have Pari here But it will be abl to figur that out becaus it will be the most similar sequenc to the context that DPR ha produc Now back to Hug face here You can see we have these multipl DPR Model and what we want is a pair We want a question encod And a ctx which is context encod now be us thi singl nq base So what do is just copi thi And in here we just add in our model Okai so a question encod Now what we also need is the context encod Which is instead of question here We just add ctx Now we have two other paramet that we need to add in here Which is ux and ctx So go to us the same paramet Two other paramet that we need to add in here which is us gpu Which is if us a gpu obvious you set thi to true If not you go with fault it will Take a littl bit of time to process thi if you ar not us a gpu though Then we also add emb Titl equal true as well now what we should see is Thi will execut without error hopefulli Okai great And then what we need to do is updat the embed within elastix search So what done here is kind of set up the process and now what we need to do is updat The document that we have in elastix search to have dpr embed So to do that we go doc store updat embed And then in here we pass our retriev Okai now thi mai take well thi would be realli quick for me We have that mani document and even on cpu actual with the lack of document we have it should be pretti quick so What we see here we creat these embed And then we post them again to our index So that is pretti cool and now what we need to do Is just test that it actual work Now go with retriev and thi is how we Get context from our elastix search document store So right retriev And then we pass in a queri here so let me just find someth here Like what did you learn from your Great grandfath mayb or from variou Yeah go From grandfath go grandfath So What did you what did your grandfath Teach you I know if thi is go to work but see Okai so you see that we return quit a few Context here Now we set up the full thing So just go to pass in a queri Here now we set up the full thing So just return What it see as be relev context We ar not actual extract an answer out yet Becaus that will be the job of our reader model So what we have is from my great grandfath we have that on so okai Some other on here type in grandfath Okai so just return that on which is fine not perfect but What we would expect to do in realiti is return more So try anoth on as well And sai who taught you about freedom of will Who taught the freedom of will And we see here okai in the first on we get the correct answer that we want or the correct context And we go down and I saw There he is So here is the Context that we want to return So it return that as the fourth best context Which is fine becaus not We kind of expect that to sort those a littl bit better than our Retriev model Thi is pretti cool And I think Definit a good start So now what we have Retriev for medit Set up our document store And now we have also set up our retriev So we can see that And now we have also set up our retriev So we can also cross that off And next thing is our reader model So I think it for thi video in the next on of cours move on to that reader model And just see how that goe 
Hugging Face Datasets #2 - Dataset Builder Scripts
Todai go to continu with the Huckabas data set seri And go to have a look at how to us the builder script So with the builder script we can either we can do a few thing so we can includ data pre-process within the data load pipelin We can stream from a anoth sort of remot data sourc which is pretti us if you ar us a data set where the owner of that data set want the data to be stream from their server Which happen quit a lot or if you know mayb you have your data set split into multipl File or you have imag in your data set or someth along those line In those case you alwai need to us on of these data set build script So what first go to do veri quickli is show you how I creat a compress file for thi demo So go to creat Let me show you so gonna go over here So into thi Jame Callum HF data set repo on github builder script go to here and you you will see thi file here So thi data set tar gz file So thi is a zip or compress file and actual go to stream our data from Thi exact locat So if you We go on here I see we have thi download link download button just go to copi that link address and gonna us that to Stream our data into into the data data set build script so veri quickli How did I build that you can actual have a look at thi file here So all do is take thi the reddit topic data set that built alreadi veri similar to the date that we us in the last video just a littl bit bigger So not massiv thousand row All it convert panda convert to a dictionari or us the the record orient and then Save that as a JSON L or JSON line file then we compress it us thi so probabl if you have your own data set and you want to compress it and Kind of follow the same step of do here Thi is what you will need So you need to add your data set file to the Zip compress file here and you choos to half file that thi I believ is actual instal by default With Python so you have to pivot instal that so with all of that We can go ahead and actual look at how we build our data build script So we start with a templat first so come over to hug face here and go data set and And go squad okai so go here so squad is just a veri popular data set and I think among the tutori thei us it as a As like a templat for build your own script um And probabl where I got thi from but I just by default I go to thi data set and us thi my templat if build a new data set load script so Come over here So within the build script I have a few thing here Jason line just you can see what wa in there I can actual delet that I need that anymor So just remov that What I want to do is creat a Python file and go to name it the same as my data set So gonna call thi the reddit topic tar GZ And what gonna call thi thi data set Okai And Thi all okai gonna modifi a lot of thi but for now not go to touch too much we just want to Um onc first on the essenti thing that we need here So first thing we need thi like ad complex that necessari class call it so reddit Tar GZ I suppos fine alreadi topic tar GZ Builder config thi matter Thi doe matter we will mess around that later not now So what doe focu on what actual matter right now So the here we have thi download manag and gonna gonna look at download manag a bit more in the next video but for now download manag is essenti a hug face state set util that allow us to given a particular file either local or on the internet we can download it and Extract the content of it So thi is why I format the Date set file as a tar GZ file Becaus I want to us thi download and extract function or method So what we need to do is in your so gonna chang thi to URL gonna come up here where we defin URL Actualli move that and here go to replac that with the Locat that I copi earlier So See if that actual that work So I need to copi it again So if I go to here The repo again Good zero and build a script the Locat of the compress file go there and then where it sai download go to copi that link and gonna put in here Okai so with that Descript we can know demo chang the other thing later with that We or thi here will kind of almost work so thi is just on thing so download thi URL but With squad there were two URL Okai so If I actual go back a littl bit you see that there is these two year on for the train set on for the develop set and We onli have on so we actual Need to modifi thi a littl bit to deal with just on dai So not two So here we need to return split gener We actual just remov thi on the valid split becaus we just have a train split and the download file is actual Not go to be thi go to be so thi is basic go to show us a path to a particular locat Let me show you exactli what do Okai so go to do from Transfer no data set Sorri Import Import from data set util import download manag Might not be there Mayb here see Okai it wa that so dl manag and just go to Initial it Thi is kind of happen in the background of our of our builder script So we actual do thi in the build script It just kind of happen so We do that and then just Copi what we have elsewher So we have the url And it is thi okai that is the euro And just see what thi output so download manag and download and extract Your up Okai see what we get Um call thi out call thi out Okai so we see We get a file path from that now Okai Interest So what have a look at what is in that file path So os list out Okai so now we can see we actual have that json line file that we that we put insid our our compress tar file so Okai what what doe that mean for us it mean we can actual just load that From here base on what the download manag is give us Okai So thi is like a cach locat for our particular data set So return to the builder script uh we have download file I I realli like the name so just go to call it path um gonna sai path here as well remov train And if we just have a look at the path Um it is it is just the directori that contain our data set of json Okai or json line file So actual what we need to do Is we need to do like out plu Data set dot json out Okai thi thi will give us a full path to our file So that is what go to do here So where ar we um so path I mean a bit easi to read okai come here Zoom out a littl bit Okai so it will be path And then here we have data set dot json out okai And yeah how a split gener function here And what that will do Is you see that we have thi file path here That is go to get pass along to thi gener exampl function or method and Is thi method that is go to kind of output the data set method that is go to kind of output the row of the data set to us so What we need to do is actual Just us thi to kind of read our data set now It you know just kind of do that from from stretcher see happen kind of hard So return to that the notebook file and see how we can do that So here we have our Our call thi file path now becaus thi is what creat in the other file a file path And what go to do file path is well first we need to import json becaus a json minu file so gonna have to read that and Exactli We actual want to do thi so as fp I think we even need to put encod there but put it to be safe And What go to do is go to go through that so for line In fp a json line file So just line of data Each on those line repres a json object So we ar go to Oh we can we can just print it from now in in here so put a count on thi So print out a few item but not too mani so if count is break Okai see what we get Okai cool So we can see that we we get a few item here All right So just kind of go through those of red file and just loop through and and print them So we can do the same over in our other So we can do the same over in our other file in the builder script so come to here Copi thi in now All of thi we can see here Okai Some of thi we will need not all of it So go ahead and just remov what we need Okai thi is all we need Um thi yield so becaus thi uh gener exampl is creat a gener function right so come here remov Remov part of thi So the line or the we should call it a record Is equal to json dot load line Mayb call it object Okai And within the object we have a few A few differ kei valu pair right So what ar those We can have a look at the the make tar file File and we have we have all these here so we have subtitl self text Up vote ratio id and creat utc now We can actual just pass all of these directli onto the next so we can yield all of these So let me show you what I mean by that So we come here and you see that just yield and then yield thi dictionari type structur for squad All right so as we alreadi have that dictionari type structur becaus we us a json line file Thi is on of the reason I like us them So we can actual just do yield kei object like that now Okai what is um what is kei Kei is actual the index valu or id valu if you if you want but an index valu So go to renam it Index becaus that make more sens to me than kei And yeah here we go So we have We have set up here go to open the file locat or do that and read the line Um Load file object or json object And yeah we just yield them so what would that do when we ar load The function or when we ar load the data set over in hug face data set Thi is go to be the thing that gener all of those all those item so What we should do now is mayb we can Mayb we can test it and see what happen Um it work straight awai see But try so what go to do is just go to copi all of thi then go to come over to Hug face go to click on my or icon right over here click new data set go to call it reddit topic tar gc creat that And go to come to file go to add a file creat new file And thi is just go to be reddit topic Tar gc So the exact same file we creat befor just go to past all that code in there Okai so you see we have all thi code Uh just remov thi I think that import Uh not squad anymor So just call it reddit topic Tar gc demo data set One thing we do need is we need to import json So good alreadi there Um we need thi anymor but keep it in there for now befor we start remov everyth creat more error So commit that And then just try and see what happen Okai so go to creat a new file to test it So sai test test data set And what go to do is from data set import load data set And the data set just call data load data set And we can find the data set name over here So just go to copi click here copi that and there is just on split in thi data set So split equal train Okai see what happen Okai we download the build script so far so good download the data And then we get thi Okai What is thi os error come down here cannot find data file Okai so we had a look at thi so without thi Dot here Uh we can see that data file is there so we have our first error um which wa not on purpos But fine So the reason we have that is becaus here I we put a dot not sure why that so save that And actual just edit it in the in the web editor here as well So remov that commit chang And then try again Okai come up here clear everyth restart and go again Okai so now we get thi kei error So what doe that mean kei error context Context Okai I rememb put context anywher So have a look at the builder script And if we okai have a look Okai here we have thi so we modifi thi yet Now what is thi tell us Um basic tell the data set builder Which featur to expect in the data set So basic down here kind of feed in these these differ featur feed in these these record Each record is a kei valu pair So the kei ar the featur name and the valu ar obvious valu Which have a particular data type now here We have the the featur name So the kei But thei ar not align to our actual data set These ar us the squad data set kei valu pair So we need to come over to thi file and we can Or get those featur specif to our data set from there So take these go to copi them across here and all go to do is actual just Write those here Okai so we have a subtitl self text Upvote ratio Uh we have oh we have id and we also have anoth on so creat anoth Well actual make thi on more normal first the id Is thi and then we have on more which is creat utc Okai now we can try thi not go to work again but try Yeah rerun thi See what happen Okai So actual it doe work but not work in the wai that we might expect so If we have a look at data and zero Okai we have Subtitl self text and then we come down here a lot in thi self text Um but so just look at thi So the upvot ratio Which is a float point number is now a string The id fine We should expect that and the creat utc which is also a float point number is now a string as well So So a bit of an issu here Basic if we if we go back to our script When we ar feed the featur through thi featur specif see that sai everyth should be a string and convert everyth into a string We actual want everyth to be a string So what we need to do here is us a specif apach arrow Data type identifi for differ thing So for exampl flow that we have here Okai so go ahead and have a look at how What that might be so to find that just go to type like apach arrow data type Here so apach arrow data type and schema schema mayb Uh we come here and we can see we can see a load of these so we have we have integ valu unsign integ And then we have float So go to sai okai singl precis float point type is perfect Okai so just go to copi that float and go to put that for creat utc and also the oper ratio Okai go to save that go to chang a few thing that we we actual need so go to remov thi task templat becaus we Do question answer with thi data set Um Um no we at least not extract question answer or train with that for the home page put thi I suppos Okai supervis kei is none And what els do we have here So subscript Uh so a demo We know that okai Okai save thi and And try again Okai so go to copi thi over into home face come here Uh not here here edit And come here select all past and I am go to commit those chang Now have a look at what happen if we load the data set so Come back over here test data set Uh run thi see what happen Okai it load well it load correctli a good sign come down here And now we can see that these ar no longer string but actual flow point number Okai so That is everyth there ar mayb a few aesthet thing to chang here So the Like the citat chang that up here I can chang thi as well but not go to go through that in thi Uh in thi video but I think you want to watch me chang citat So yeah everyth For thi video in the next video What go to do is take a look at take thi a a littl bit further And ad more advanc data type like imag into our data set so Until then I hope thi ha been us 
Fast intro to multi-modal ML with OpenAI's CLIP
In thi video go to have a quick introduct to clip and how we can us it to almost move between the modal of both languag and imag Now befor we dive in just quickli understand what clip is So it consist of two big model In thi implement go to be us a vision transform that will emb imag And go to us a normal text transform that will emb text Dure pre-train OpenAI train the model on pair of imag and text and it train them to both output embed vector that ar as close as possibl to each other So the text transform wa train to output a singl embed dimension embed that wa as close as possibl to the vision imag embed for the imag text pair So what that mean is that clip is abl to take both imag and text and emb them both into a similar vector space And with that we can do a lot of thing You can do imag and text classif You can do imag and text search and a huge number of thing Anyth to do with imag and text a good chanc we can do it with clip So have a look at how we actual us clip OpenAI releas a GitHub repositori OpenAI clip here Thi contain clip but not go to us thi implement actual go to us thi implement of clip So thi is on Hug Face So go to be us Hug Face transform and thi is still from OpenAI still clip just an easi to us implement of it through the Hug Face transform librari which is a more standard librari for actual do anyth with NLP and also now comput vision and some other thing as well So to get start recommend you instal these librari To instal Torch you should probabl go through the PyTorchorg instruct rather than follow thi here So go to PyTorchorg and just instal PyTorch us the specif instal command thei us for your platform or your iOS from here And then pip instal transform and dataset You can still just us thi command recommend instal PyTorch from here instead Now after that go to need our dataset So thi is just a veri simpl dataset It contain I think just under imag and we onli care about the imag here So if we have a look we have ImageNet go the first item and just have a look at imag And we have thi Soni radio and we have other thing as well So if we go ImageNet anoth imag here of a dog OK just to point out that we have a lot of imag in here in the dataset that cover a rang of thing not a huge number of differ categori here but thei have dog thei have radio and a few other thing Now just go to go ahead and initi everyth So a few thing here From transform import the clip token So the token is go to handl the pre-process of our text into token ID tensor and other tensor We have the clip processor like the token but for imag So thi is actual just go to resiz our imag into the size that clip expect and also modifi the pixel valu as well And then we have clip model Clip model is clip itself OK so if you have CUDA or MPS if on Mac you just set that with thi OK and then readi to actual initi all of thi So the model ID is go to be what we saw befor So you come over here we have the token clip VIT base patch copi that And here we go OK And now we just need to look be told what to do alreadi OK so model clip model from pre-train model ID go to I normal set devic like that I know if you can I am go to do it like thi OK and token OK good job And processor Cool Almost there from pre-train OK and you got a littl bit confus So model ID OK that look good run that OK cool So now what go to do is take a look how we actual creat the text embed through clip So we start with a prompt go to go with a dog in the snow not mani pictur of dog in the snow in the dataset but there ar some And what we need to do is is token the prompt Yeah true OK not go to do it like that go to go with token prompt and the we need to return tensor us Pytorch So us go to be us Pytorch behind the scene here So make sure we do that And just have a look at what is actual in input OK so we get the thi input ID is tensor so recogn thi if you if you us a face transform befor These ar just the ID token ID that repres the word from thi OK And these thi is the attent mask Now for us it is go to all be on but if we had pad in here anyth beyond the length of our prompt would becom a zero Tell the model to not pai attent to to that part of the prompt And from there we can process thi through clip so we do model get text featur I think And we pass in those input OK And have a look at the shape of that OK so we have a five hundr and twelv dimension vector OK So the text embed side of thing Now we need to go ahead and do the imag embed side of thing OK So go to resiz the imag first with the processor not ad ani text in here so you can also process text through thi processor just keep it separ becaus it make more sens to me The imag should be imag actual Again we want to return tensor us PyTorch OK And then we can have a look at the go to go to show you the imag First we have a look at the shape and as well on thing So OK I can show you OK OK In here we actual have thi pixel valu so we actual need to extract that So go to put it here go to move those to the devic as well I think the devic I have set up right now is actual CPU so it make a differ for me but fine So have a look at the shape OK So you see that we have thi by imag with three color channel So thi is just the expect shape that will be consum by the vision transform of CLIP OK And to import my PLOTlib Pyplotplt And I want to show you thi imag So thi resiz imag So PLT show imag And I need to so I need to resiz it Let me show you what actual do here So imagesqueez Zero So go to remov that first dimens Now go to transpos it So we put the three color channel at the back Thi is for thi is for my PLOTlib to be abl to actual show us thi So go to take that go to put it here OK And you can see so the minimum maximum color valu ar all of the color valu Pixel valu ar modifi when we do thi process it through the processor So the color ar kind of mess up But you can see that thi is like a resiz You know what we saw befor OK So a Soni Just kind of backward now and flip We can sort it see that it is that Soni radio So with that we can go ahead and get the imag featur I think it just show me Model Get imag featur So an imag OK And then have a look at the shape Cool OK So similar to befor we have that dimension embed vector OK So cool And from here we can we can do a lot of thing What go to show you how to do is how to kind of search through thi or at least compar a small number of imag against our prompt so that we can actual see which on of those imag is the most similar to a dog in the snow OK So to do that go to want to emb more of these imag not go to emb load of them just go to emb imag Noth noth crazi So go to import NumPi as NP NP random seed So thi is just so you can replic what I am do And so thi will thi will randomli gener a set set set of random number OK So the reason do thi is becaus we want to take a sampl out of the data set We want to have the whole data set I want it to be at least somewhat random So to do that we want to go So sampl Indice ar go to be equal to NumPi random dot random from zero up to the length of imag net actual plu on And we need of those And then go to convert that into a list OK I can just have a quick look at what is in there OK So just all of these all these number here OK So yeah Cool And if we run it again becaus we have that random seed set the random set of number chang And what go to do is just creat a list of imag us that us those valu So I for I in sampl IDX OK Check OK So now imag from our data set And now we want to just go ahead and liter take everyth just done and put into a for loop to creat the embed for all of these imag OK So that will look someth like thi us TQDM here Thi is just a progress bar so we can see where we ar Batch size sai how mani imag to perform thi for in ani on go You can increas thi if on a if us a bigger GPU or or whatev els Image arrai set that to none for now We initi that in the first loop OK And then just in the same thing as befor So So from thi So select a batch of imag base on the on the batch size And then where we ar process and resiz the imag from that batch get the imag featur look exactli the same thing I think befor I actual includ pixel valu but the same thing just a default argument Convert into a NumPi arrai Did I show you thi befor I actual think so No mayb not But here the squeez is veri similar the same thing as what I show you up here So we squeez the first dimens out of that like we did here And then we ar move that batch of embed to the CPU If not alreadi on the CPU detach it from the gradient like the train graph of PyTorch The PyTorch model eg clip And then convert into a NumPi arrai OK And then go to add that batch of embed to a larger arrai of all imag embed OK And why the imag arrai come in OK So run that OK So we come up here I made a mistak in the code So here actual pull in the full row or record at ani on time We do that We want the imag itself OK So run that again OK And now if we check the type of imag Zero We should see a pill imag Yeah Cool Yeah Pill here Now we can run thi OK It take long And now we have on hundr five hundr twelv dimension imag embed from our data set and we can now us them to compar to our initi text embed and see which on of these match most close to that text embed OK So go to be us dot product similar So just on thing to be awar of with that And that is that it consid both the magnitud of the vector and also the angl So in thi case that will that can throw off our result So we should normal all of the imag embed so that we ar not look at the magnitud of vector And onli focus on the angular similar between our text embed and these imag embed So to do that we need to just show you quickli So look at the minimum maximum You know that kind of all over the place So to normal we need to do thi So do imag arrai divid by do numpi Linoug dot norm And here we have the imag arrai OK Axi equal on And let me show you what that is So we have all these number and these ar basic tell us for each on of these vector of what should we divid it by in order to bring each of them to within a within a set set Within a set magnitud pretti much So Take a look at the shape will be So yeah we do that So I think I need to Transpos thi OK And then so the imag arrai the shape is go to be transpos now so go to transpos it again Yeah Image arrai equal imag arrai transpos OK Cool And now if we have a look at the minimum and maximum So minimum and maximum we get these valu which ar more reason OK So now what we can do is us dot product similar to actual compar compar these So text embed go to take the text embed and similar to befor what we did is we need to move it to the CPU Detach it from the PyTorch graph and then convert to a numpi arrai OK Yeah And then for the score all we need to do is a numpi dot And we ar go to put the text embed follow by the imag arrai And actual I think I need to transpos thi again So Mayb we could have avoid transpos up here OK Yeah So the score that we get here we get a singl score for everi singl vector As we can see shape and thei ar the dot product similar score So what we can now do is sort base on thi score arrai and just return like the top So the top five imag and see what the top five most similar imag ar for our particular queri OK So go to return the top k So top k is go to be the five most similar or the five item with the highest score And then we want to take the index valu us nporg sort go to add the neg of the score there and just make sure we take becaus score ha thi here So actual just take the let me show you score Shape OK So take the valu there and then I want to take the top k from that OK So what left with is these five index valu which ar essenti index of the imag embed and therefor both the And therefor the imag that ar the most similar to our queri So we us matplotlib again to to visual those So we do for i in print the print the score first So score i And actual that would be i and then go to I am go to show and go to PLT show OK Cool So yeah I mean it So the first first item as we would expect is a dog in the snow So after that we get dog and we get like these snowi area The reason for that is that we just have ani more imag of dog in the snow Thi on I know what thi is like a toi that mayb a dog Mayb a bear not sure But I suppos technic like a dog in the snow So we have that So yeah obvious the model is perform pretti well and I think veri cool that we can do that so easili And yeah I mean CLIP is I think an amaz model that we can us to do a load of cool thing across both the text and imag domain which is super interest And definit like if you think just a coupl of year ago thi sort of thing wa imposs and seem like at least not to thi sort of degre of accuraci like it wa go to be happen anytim soon So thi is thi is realli cool Here obvious shown I show you how to do like a text to imag search You can do thi like the deep In realiti what do is kind of search through the vector So it matter you know which direct do that search The vector ar all the same So if you want to do a text to text search with with CLIP you could You want to do imag to imag search you could If you want to do imag to text or all of those thing all at onc you could It is not as search through vector So what is behind those vector realli matter so much OK So I think it for thi video I think CLIP is super interest and I hope that you do as well in the futur or veri soon actual go to be go into a lot more detail on CLIP So if you ar interest in that subscrib and click on the littl notif button and you will get a notif about that pretti soon 
All You Need to Know on Multilingual Sentence Vectors (1 Model, 50+ Languages)
Todai go to be have a look at multilingu sentenc transform go to look at how thei work how train and why so us go to be focus on on specif train method which I think is quit us becaus all it realli need is a reason small data set of parallel data which is simpli translat pair from a sourc languag like English to whichev other languag us So obvious if you ar want to train a sentenc transform in a languag that realli have that much data particularli sentenc similar data thi can be realli us for actual take a high perform for exampl English sentenc transform and transfer that knowledg or distil that knowledg into a sentenc transform for your own languag So I think thi will be pretti us for a lot of you And jump straight into it Befor we realli get into the whole multilingu sentenc transform part of the video I just want to sort of give an impress of what these multilingu sentenc transform ar actual do So on here we can see a singl English sentenc or brief phrase down at the bottom I love plant and the rest of these ar all in Italian So what we have here ar a vector represent of dens vector represent of these phrase And a monolingu sentenc transform which is most of the sentenc transform will onli cope with on languag So we would hope that phrase that have a similar mean end up within the same sort of vector space So like we have for amo lippiant here and I love plant these ar kind of in the same space A monolingu sentenc transform would do that for similar sentenc So in English we might have I love plant and I like plant which is actual what we have up here So thi here is Italian for I like plant And we would hope that in a similar area wherea irrelev or almost contradictori sentenc we would hope would be far off somewher els like our vector over here So how obvious a monolingu sentenc transform work And exactli the same for a multilingu sentenc transform The onli differ is that rather than have a singl languag it will comprehend multipl languag And what you can see in thi visual So in thi exampl I have I love plant and amo lippiant thei have the same mean thei have the same mean just in differ languag So that mean that thei should be as close togeth as possibl in thi vector space So here just visual three dimens In realiti be a lot more I think most transform model go with dimens But obvious we visual that So we have here So we want differ languag or similar sentenc from differ languag to end up in the same area And we also want to be abl to repres relationship between differ sentenc that ar similar And we can kind of see that relationship here So we have mi piacer e le piant and amo lippiant and I love plant ar all kind of in the same sort of area Mi piacer e le piant so I like plant is obvious separ somewhat but still within the same area And then in the bottom left down there we have un cane arancion which mean I have a orang dog So obvious you know realli noth to do with I love plant Although I suppos you could sai talk about yourself so mayb a littl bit similar but otherwis complet differ topic So kind of what we want to build Someth that take sentenc from differ languag and map them into a vector space which ha some sort of numer structur to repres the semant mean of those sentenc And it should be languag agnost So obvious we well mayb we can train on everi languag I know ani model that ar train in everi singl languag but we want it to be abl to comprehend differ languag and not be bias toward differ phrase in differ languag but to have a veri balanc comprehens of all of them Okai So how the vector should look And then okai So how what would a train data for thi look like And what ar the train approach So like I said befor two train approach that go to just briefli touch upon but go to focu on the latter of those So the first on that I want to mention is what the M-U-S-E MUSE or Multilingu Univers Sentenc Encoder Model wa train on which is a multitask translat bridg approach to train So what I mean by that is it us two or us a dual encod structur and those encod deal with two differ task So on on end you have the parallel data train So when we sai parallel data these ar sentenc pair in differ languag So like we had befor we had the Amalepiant and Isle of Plant which is just the Italian and English phrase for Isle of Plant So we would have our sourc languag and also the translat or the target languag probabl a better wai to put translat now So we have the sourc and translat our parallel data set And what do is optim to get those two vector or the two sentenc vector produc by either on of those sentenc as close as possibl And then there is also the sourc data So we basic have like sentenc similar or NLI data but we have it just for the sourc languag So we have sourc sentenc A and sourc sentenc B And we train on both of these Now thi is it work and good but obvious we train on a multi-task architectur here and train on a singl task in machin learn is alreadi hard enough Train on two and get them to balanc and train well is harder And the amount of data at least for Muse and I believ for if train us thi approach go to need to us a similar amount of data is pretti signific I think Muse is someth like a billion pair so it is pretti high And anoth thing is that we also need someth call hard neg in the train data in order for thi model to perform well So what I mean by hard neg is sai we have our you know we have our sourc sentenc A here and we have thi sourc B which is like a similar sentenc a high similar sentenc Thei mean basic the same thing also have to add a sourc C and thi sourc C will have to be similar in the word that us to sourc A but actual mean someth differ So harder for the model to differenti between them Again the model would have to figur out you know these two sentenc ar not similar even though thei seem similar at first but not So it make the task the train task harder for the model which of cours make the model better So that is train approach number on And mention the parallel data there the data set go to be us for the second train approach And that second train approach is call multi-lingu knowledg distil So that is a mouth and take me a while to write down sorri So multi-lingu knowledg distil So thi wa introduc in by you know who we mention befor the sentenc transform peopl Nil Reimer and Irenia Gurevich And the sort of advantag of us thi approach is that we onli need the parallel data set So we onli need those translat pair and the amount of train data you need is a lot smaller And us thi approach the sentenc transform peopl have actual train sentenc transform that can us all more than languag at onc And the perform is good not just that thei you know manag to get a few phrase correct The perform is actual quit good So I think you know pretti impress And you know the train time for these is super quick as see And like I said us just translat data parallel data which is reason easi to get for almost everi languag So I think pretti us Now well have a look at what that multi-lingu knowledg distil train process actual look like So what we have here So same exampl as befor got I like plant thi time and mi piaccia no le piant which is again the same thing in Italian Now we have both of those We have a teacher model and a student model Now when we sai knowledg distil that mean where you basic take on model and you distil the knowledg from that on model into anoth model here The model that alreadi know some of the stuff that we want that we want to distil knowledg from is call the teacher model Okai Now the teacher model in thi case is go to be a monolingu model So probabl go to be a sentenc transform veri good at English test onli And what we do is we take the student model which is go to be it have to be a sentenc transform just a pre-train transform model be us XLM Roberta later on and it need to be capabl of understand multipl languag Okai So in thi case we feed the English sentenc into both our teacher model and student model And then we optim the student model to reduc the differ between the two vector output by those two model And that make the student model almost mimic the monolingu aspect of the teacher model But then we take it a littl further and we process the Italian or the target languag through the student model And then we do the same thing So we try to reduc the differ between the Italian vector and the English vector And what do there is make the student model mimic the teacher for a differ languag Okai So through that process you can add more and more languag to a student model which mimic your teacher model which yeah I mean it seem at least realli simpl just to think of it like that in my opinion anywai but it work realli well So a veri cool techniqu in my opinion I do like it So just a more visual wai of go through that We have these differ circl thei repres differ languag task or differ languag but pretti similar or the same task in each on of those We have our monolingu teacher model and that can perform on on of these languag but fail on the other We take that monolingu model or our teacher model and then we also take a pre-train multilingu model So the import thing here is that it can handl new languag Like I said with Excel learn Roberta thi is our student We perform multilingu knowledg distil mean the student learn how the teacher perform well on the singl task by mimick it sentenc vector output The student then perform thi mimicri across multipl languag And then hopefulli the student model can now perform across all of the languag that we ar want to train on how the multilingu knowledg distil work have a look at that in code Okai So in our code here And the first thing go to do is actual get our data So in the paper that introduc the multilingu knowledg distil Reimer and Guravich us the focu partli on thi TED subtitl data So we know TED talk just low talk where peopl present on a particular topic usual pretti interest And those TED talk have subtitl in load of differ languag So thei scrape that subtitl data and us that as sentenc pair for the differ languag Okai So the parallel data Now what go to do is us Hug and Face Transform to download that So we just import dataset here Sorri I said Hug and Face Transform actual Hug and Face dataset here So import dataset and go to load that dataset and just have a look at what the structur of that dataset is So the TEDMortar and just get the train data here You see in here we have thi featur translat and talk name Now not realli veri clear But insid the translat data we have the languag tag So these ar languag code ISO languag code If you type that into Googl pop up If you know which on which ar which And below we also have in here not veri clear again So if I come here we have translat and each on of those correspond to the languag code up here Okai So if we came here we see EN English and we find it here Okai And then we also have talk name not realli import for us So we can get the index of our English text We need to extract that for our sourc languag So we extract that we get a number four So go into those languag pair find EN And then we us that index to get the correspond translat which is here And then us that to creat all of our pair Now here just creat load of pair Thi first on so thi is English to Arabic But if we have a load actual load of pair here So we have in total which is obvious quit a lot probabl not go to us all of those I mean you could do if you want it depend on what try to build But I think most of us ar probabl not go to be try to build some model that cross all these differ languag So what go to do is just initi a list of languag that we would like to train on So go to be feed all of thi into a sentenc transform class call parallel sentenc dataset And that requir that we on separ our pair us a tab charact And two keep all those pair separ in differ GZIP file So why us thi particular structur So data pre-process step here just run through them quickli becaus I want to focu more on the actual sentenc transform train part So run that we can well actual go to take a moment So let me skip forward Okai And then we want to see how mani pair well I just want to see we have to do thi I want to see how mani pair we have for each languag And you see here we have about for each of them The German on is slightli less Okai And then have a look at what those sourc and translat look like So here we have applaus and applaus Now I think Italian it seem so But here we can see okai the end of the talk end in applaus Obviousli the subtitl sai applaus or hopefulli it end in applaus And then we just have the tab charact and that separ the sourc languag English in thi case from the translat languag Now okai Now what we want to do is save that data So we sort all that in these dictionari Okai So initi dictionari here and access them here So we have enit es ar fr and de And now just go to save them So run thi That will save And what do is sorri OSLister So we can see what is in there Where is it data Just data Is that right Okai And then we have these five file Okai Now continu So now what we want to do is okai we have our train data readi or mostli readi befor we feed it into the Sentenc parallel sentenc dataset object later on So okai leav that for now and move on to the next step which is choos our teacher and student model So you know I alreadi mention befor we want our student model to be capabl of multilingu comprehens So what I mean by that or not just what I mean but on big compon of that is can the Transform token deal with differ languag In some case thei realli So let me show you what the Debert token doe with these four differ sentenc So just loop through each on So four text in sentenc And what go to do is just print go to print the output of Debert token And if I token that text now what doe it give me Okai So what we have here okai English of cours is fine But the token or the vocabulari of the token is I think roughli token Okai And most of those ar English base Okai Some like you can see here that it ha pick up some Chines charact becaus it doe you know other languag do feed into it a littl bit becaus just you know all the data is pull from the internet other bit do get in there but mostli English So why we see okai we have these unknown charact Now as soon as we have an unknown charact in our sentenc the token or no sorri the transform is read against the wood to understand you know what is in that posit What is that unknown token suppos to repres In the case of you know I think of it as like you know when a kid in school and thei had those you know had like a paragraph and you had to fill in the blank right So you had a paragraph and occasion in a coupl of sentenc be a coupl of blank line where you need to you know guess what the correct word should be If you onli have a coupl of those blank you know as a person you can probabl guess accur And the same for Bert Bert can probabl guess accur what the occasion unknown token is But you know if a kid in school you can guess what the actual unknown token is But if in school thei gave you a sheet and thei said okai fill out these blank And it wa actual just a paragraph of blank and you had to guess it correctli probabl I know I think your chanc ar pretti slim of get that correct So the same is true for Bert Bert for exampl in our Georgian exampl though So the token from Bert is not suitabl for non-Latin charact languag whatsoev And then it doe know some Greek charact here and mayb it know all of them So I suppos Greek feed into Latin languag a bit more than Georgian or Chines but it know what to do with them all singl charact token And the issu with singl charact token is that you realli encod that much inform into a singl charact Becaus that you know if you have charact in your alphabet that mean you have encod to repres your entir languag which is not go to happen So you know also not good So basic us a Bert token not a good idea What you can do is okai thi XLM token or token Now XLM is train for multilingu comprehens It us a sentenc piec transform which us byte level logic to split up the sentenc or the word So it can deal with token never seen befor which is pretti nice And the vocabulari size for thi is not I think It could be off a few k there but around that mark And been train on mani languag So obvious a much better option for our student model So have a look at how we initi that So thi XMR model is just come from transform Okai So I need to convert that model from just a transform model into an or initi it as a sentenc transform model us the sentenc transform librari Okai So from sentenc transform go to import model and also sentenc transform So XMR so thi is go to be our actual transform model go to write modelstransform Sentenc transform under hood us hug face transform as well So we would access thi as the normal model identifi that we would with normal hug face transform which is XMR Roberta base Okai As well as that we need a pool layer So we write modelspool And in here we need to pass the output embed dimens So thi get word embed dimens for our model And also what type of pool like to do We have max pool CLS token pool and what we want is a mean pool So is pool mode mean token equal true Okai So that two compon of our sentenc transform And then from there we can initi our student So student equal sentenc transform And initi that us the modul which is just a list of our two compon So XMR follow by pool And it So have a look at what we have there Okai We can just ignor thi top bit here We just want to focu on thi So you see we have our transform model follow by the pool here And we also see that us the mean token pool set to true rest of them ar fals Okai So our student model initi And now what we want to do is initi our teach model Now the teach model let me show you you just have to be a littl bit care with thi So sentenc transform So mayb like to us on of the top form on which a lot of them ar the old model So these ar monolingu model all MPNet base And okai initi thi and see what is insid it Okai So we have the transform the pool as we had befor but then we also have thi normal layer So the output from thi model ar normal And obvious if try to make anoth model mimic the normal layer output well not ideal becaus the model is go to be try to normal it own vector So you realli want to do that You want to choos a model You either want to remov the normal layer or just choos a model that have normal layer which I think is probabl the better option So what go to do So for the teacher go to us a sentenc transform go to us paraphras model becaus these us normal layer Distil Roberta base Okai have a look Okai So now you see we have the transform follow directli by the pool Now anoth thing that you probabl should just be awar of here is that we have thi max sequenc length here is which align with our paraphras model here But fine becaus go to limit the maximum sequenc length anywai to So fine But go to limit the the maximum sequenc length anywai to So you know you know not realli an issu but just you know look out for that if train your own model Thi on So none of those align But yeah just be awar of that that the sequenc length might not align there So okai so we have our sort of format our train data We have our two model the teacher and the student So now what we can do is prepar that data for load into our train process or fine tune process So as I said befor go to be us the parallel sentenc sorri from sentenc transform import parallel sentenc data set And first thing we need to do here is actual initi the object And that requir that we pass the two model that train with becaus thi kind of handl the interact between those two model as well So obvious we have our student model which is our student And we have the teacher model which is our teacher Alongsid thi we want batch size go to us but I think actual you can probabl us higher batch here Or you probabl should us higher batch is on that I see us a lot in these train code And you also us embed cach call true Okai So that initi the parallel sentenc data set object And now what we want to do is add our data to it So we need our train file So train file equal to OS list that we did befor I think in the data file in the data directori Yeah So all we want And what do is just for F in those train file go to load each on of those into the data set object Print F and data dot load data I need to make sure I includ the path there follow by the actual file name I need to pass your max sentenc which is the maximum number of sentenc that go to take from that load data batch So basic the maximum number of sentenc go to us from each languag there Now just go to set thi to which is higher than ani of the batch we have fine I think I mean if you want to try and balanc it out fine You can do that here And then the other option is where we set the maximum length of the sentenc that go to be process So that is max sentenc length And I said befor look the maximum we have here is or So just trim all of those down to Okai That will load our data And now we just need to initi a data loader So just us PyTorch here So run from torch utilsdata input data loader Loader is equal to data loader Pass out data We want to shuffl that data And we also want to set the batch size which is same as befor Okai So model is alreadi data is readi Now we initi our loss function So from sentenc transform again dot loss loss Yep Import MSE loss And then loss is equal to MSE loss And then here we have model equal student model Okai So onli optim our student model not the teacher model The teacher model is there to teach our student not the other wai around Okai So everyth we need readi for train So move on to the actual train function So we can train go to train for on epoch but you can do more I think in the actual zone in the other code that seen that do thi thei were train for like five epoch But you even just train on on epoch how you actual get a pretti good model So I think you need to train on too mani but obvious you know if you want better perform I would go with the five that seen in the other code So we need to pass our train objector here So we have the data loader and then loss function Now I want to sai okai how mani epoch Like I said befor go to get with on number of warm up step So befor you jump straight up to the learn rate that you you will select in a moment do we want to warm up first Ye we do go to warm up for of the train data which is just length of the loader and multipli by Okai And from there where do you want to save the model go to try go to save it in xml-ted Now optim paramet So we have a go to set a learn rate of to the minu epsilon of to the minu And also go to set correct bia equal to fals Okai There the optim paramet And then we can also save the best model Save the best model equal to true And then we run it Okai So run that go to take a long time So go to actual go to stop it becaus alreadi run it And have a look at the actual evalu that and have a look at the result Okai So I just have thi notebook where evalu the model So us thi STS sentenc textual similar benchmark dataset which is multilingu get the English data and also the Italian And you can see thei ar similar So the zero so each row in the English dataset correspond to the other languag dataset as well So in here sentenc on in the English mean the same thing as sentenc zero in the Italian Okai Same sentenc two also same similar score So first thing we do is normal that similar score And then we go down a littl bit So we reformat data us sentenc transform input exampl class And through thi creat three differ evalu set So we have the English to English Italian to Italian and then English to Italian And then what we do here is we initi a similar evalu for each of these dataset Again us sentenc transform just make life a lot easier We initi those and then we can just pass our model to each on of those evalu to get it perform So here on the English set and here Now I just train on on epoch If you want better perform you can train on on epoch and you should be abl to get more toward or mayb a littl bit higher So pretti straightforward and incred easi And then here just I want to compar that to the student befor we train it So I initi a new student and had a look and you can see the evalu is pretti low So for English Italian actual surprisingli Although alreadi a multilingu model So it doe make sens that you can understand Italian And then from English to Italian it rare drop down to So it for thi video I think been pretti us At least for me I can kind of see where you can build a sentenc transform in a lot of differ languag us thi which is I think realli cool And will probabl be us for a lot of peopl So I hope you enjoi the video 
Faiss - Introduction to Similarity Search
Hi welcom to thi video go to be cover Facebook AI Similar Search or FICE And go to be cover what FICE is and how we can actual begin us it and introduc a few of the kei index that we can us So just as a quick introduct to FICE as you can probabl tell from the name a similar search and a librari that we can us from Facebook AI that allow us to compar vector with a veri high effici So if seen ani of my video befor on build sentenc embed and compar sentenc embed in those video I just ad a gener Python loop to go through and compar each embed and veri slow Now if onli work with mayb vector probabl OK you can deal with that But in realiti probabl never go to be work with that smaller data set Facebook AI Similar Search can scale to ten hundr of thousand or up to million and even billion So thi is incred good for effici similar search But befor we get into it just sort of visual what thi index look like So if we imagin that we have all of the vector that we have creat and we put it into our similar search index Now thei could look like thi So thi is onli a three dimension space but in realiti there would be hundr of dimens here In our us case go to be us dimens of So you know a fair bit in there Now when we search we would introduc a new vector into here So sai here thi is our queri vector So x q Now if we were compar everi item here we would have to calcul the distanc between everi singl item So we would calcul between our queri vector and everi other vector that is alreadi in there in order to find the vector which ar closest to it Now we can optim thi We can improv we can decreas the number of dimens in each of our vector and do it in a intellig wai so thei take up less space and the calcul ar faster And we can also restrict our search So in thi case rather than compar everi singl item we might restrict our search to just thi area here And these ar a few of the optim at a veri high level that we can do with FICE So enough for the introduct to FICE actual jump straight into the code Okai so thi is our code In here thi is how we ar load in all of our sentenc embed So gone ahead and process some alreadi becaus thei do take a littl bit of time to actual build But build them from thi file here load thi into Python as well But I mean pretti straightforward to sai load of sentenc that have been separ by a newlin charact And then here we have all of those NumPi binari file Now NumPi binari file Like I said get them from GitHub which ar over here where pull them all in us thi cell here Now that save everyth to file And then we just read in each of those file and we append them all into a singl NumPi arrai here And that give us these thousand sampl Each embed is a vector with valu insid So how load in our data also load in that text file as well So we just want to do with open sentencestxt And then just read that in as a normal file And we just write go to put line equal fpread And like I said split that by newlin charact So we just write that Sorri sentenc And we see a few of those as well Okai Now to convert from those sentenc into those sentenc embed I need to import thi anywai for later on when build our queri vector just show you how I do that now What we do is from sentenc transform which is the librari us to creat those embed import sentenc transform And then our model us sentenc transform again And us the BERT and base NLI mean token model Okai So how we initi our model And then when encod our text see in a moment we just write model encod And then we write someth in here hello world Okai And that will encod that will give us a sentenc embed Okai So that is what we have insid here We just have the sentenc embed of all of our line here Now I think we have everyth we need to get start So build our first FICE index So the first on go to build is call the index flat And thi is a flat index which mean that all the vector ar just flat vector not modifi them in ani wai And the stand for the distanc metric that us to measur the similar of each vector or the proxim of each vector And is just Euclidean distanc So a pretti straightforward function Now to initi that we just write FICE So we import no so we need to import FICE And then we write index equal FICE dot index flat And then in here we need to pass the dimension of our vector or our sentenc embed Now what is our dimension So each on is valu long So if like a nicer wai of write out we put sentenc embed And we write shape on OK And our index requir that in order to be properli initi So do that That will be initi Let me run it again I think my notebook just restart It did restart weird OK on minut So go to initi the index And there is on thing that we need to be awar of So sometim with these index we will need to train them So if the index is go to do ani cluster we would need to train that cluster algorithm on our data And now in thi case we can check if an index need train or is train alreadi us the is train attribut And see with thi index becaus just a flat index not do anyth special see Becaus not do anyth special we need to train it And we can see that when we write is train it sai alreadi train Just mean that we actual need to train it So good Now how do we add our vector our sentenc embed All we need to do is write index add And then we just add embed like so So pretti straightforward So add sentenc embed And then from there we can check that been ad properli by look at the end total valu So thi is number of embed or vector that we have in our index And with that we can go ahead and start queri So first creat a queri So do xq which is our queri vector And we want to do the model and code that we did befor Now go to write someon sprint with a footbal OK go to be our queri vector And to search we do thi So we write di equal index search xq And then in here we need to add k as well So k let me defin it abov here So k is the number of item or vector similar vector that like to return So go to want to return So with here with thi we will return index id into thi i variabl here go to time it as well just so we see how long it take And print i You can see that we get these four item Now these align to our line So the text that we have up here that will align So what we can do is we can print all of those out So do i And then in here we want to write line i for i Sorri let me end that For i in i OK Ah sorri So thi is here OK So these ar the sentenc or the similar sentenc that got back And we see obvious it seem to be work pretti well All of them ar talk about footbal or be on a footbal field So that look pretti good right Only problem is that thi take a long time We have that mani vector in there And it took millisecond So a littl bit long And someth that we can actual improv OK So befor we move on to the next index I just want to have a look at the sort of speed that we would expect from thi when we ar thi is a veri small data set So what els could we expect So if we go over here alreadi written all thi code If like to go through thi notebook leav a link in the descript So come down here we have thi flat index And thi is the queri time So thi is for a randomli gener vector with a dimens size of And thi is a number of vector within that index So we go up to million here And thi is a queri time in millisecond You can see it increas quit quickli Now thi is in FICE but still an exhaust search not realli optim how we could do not us that approxim search capabl of FICE So if we switch back over to FICE we can begin us that approxim search by ad partit into our index Now the most popular of these us a techniqu veri similar to someth call Voronoi cell not sure how you pronounc it I think about right And I can show you what that look like So over here if we go here we have all of these So thi is call a Voronoi diagram And each of the sort of squar or the cell that you see ar call Voronoi cell So here we have Voronoi cell And that is just what you see here So thi thi all of these kind of squar ar each a cell Now as well as those we also have our centroid So just go to write thi down So centroid And these ar simpli the center of those cell Now when we introduc a new vector or our queri vector into thi what do is essenti so we have our queri vector and sai sai it appear here Now within these Voronoi cell we actual have a lot of other vector So we could have we could have million in each cell So a lot in there And if we compar that queri vector and thi thing here to everi singl on of those vector it would obvious take a long time go through everi singl on We want to do that So what thi approach allow us to do is instead of check against everi on of those vector we just check it against everi centroid And onc we figur out which centroid is the closest we limit our search scope to onli vector that ar within that centroid Voronoi cell So in thi case it would probabl be thi centroid here which is the closest And then we would just limit our search to onli be within these boundari Now what we might find is mayb the closest vector here is actual here wherea the closest vector here is right there So in realiti thi vector here thi on might actual be a better approxim or a better it might be more similar to our queri And why thi is approxim search not exhaust search becaus we might miss out on someth but that is kind of outweigh by the fact that thi is just a lot a lot faster So sort of pro and con whatev is go to work best for your us case Now if we want to implement that in code first thing that we want to do is defin how mani of those cell that we would like So go to go So us thi endless paramet And then from there we can set up our quantiz which is almost like anoth step in the process So with our index we ar still go to be measur the distanc So we still actual need that index in there So to do that we need to write FICE index flat And we pass out dimens again just like we did befor And like I said just a step in the process not our full index Our full index is go to look like thi So we write index And in here go to have our FICE And thi is a new index So thi is the on that is creat those partit So we write index IVF flat And in there we need to pass our quantiz the dimens and also the endless OK Now if you rememb what I said befor in some case need to train our index Now thi is an exampl of on of those time Becaus do the cluster and creat those foreign nois cell we do need to train it And we can see that becaus thi is fals Now to train it we need to just write index train And then in here we want to pass all of our sentenc embed So sentenc embed like so run that veri quick And then we can write train And we see true So now our index is essenti readi to receiv our data So we do thi exactli the same wai as we did befor We write index add And we pass our sentenc embed again And we can check that everyth is in there with index and total OK So now we see that we have our index readi And we can begin queri it So what go to do is us the exact same queri vector that we us befor Go to time it so that we can see how quick thi is compar to our previou queri And actual go to write the exact same thing we wrote befor So can I actual just copi it So take that Bring it here There we go So now have a look So total So bring it up here And we have Now thi is mayb a littl bit slow So see that the time do vari a littl bit quit randomli But mayb a littl bit slow But probabl pretti realist So that took millisecond Thi on Now have a look So these ar the index got compar them to what we had befor And I believ all the same So just shorten the time by a lot And get the exact same result So pretti good Now sometim we will find that we do get differ result And a lot of time fine But mayb if you find the result ar not that great when you add thi sort of index then that just mean that thi search is not exhaust enough Like we ar us approxim search But mayb we should approxim a littl bit less and be slightli more exhaust And we can do that by set the nprobe valu So nprobe explain in a minut So let me actual first just run thi And we can see it will probabl take slightli longer So yeah we get millisecond here Of cours we get the same result again becaus there were no accuraci issu here anywai But let me just explain what that is actual do So in thi case here what you can see is a IVF search where we ar us an nprobe valu of So just us just search on cell base on what the first nearest centroid to our queri vector Now if we increas thi up to us a smaller number in thi exampl So mayb we increas it to Our four nearest centroid So I would sai probabl these thi on thi on thi on and the on alreadi highlight All of those would now be in scope becaus our nprobe valu so the number of cell that we ar go to search is Now if we increas again to sai these two cell might also be includ Now of cours when we do that we ar search more So we might get a better perform better accuraci But in term of perform in time also not also go to increas and we want time to increas So a trade off between those two In our case we realli need to increas thi So realli need to worri about it So that is the index IVF And we have on more that I want to look at And that is the product quantiz index So thi is actual so we us IVF and then we also us product quantiz So probabl better if I try and draw thi out So when we us product quantiz imagin we have on vector here So thi is our vector Now the first step in product quantiz is to split thi into sub vector So we split thi into sever and then we take them out We pull these out and thei ar now their own sort of mini vector And thi is just on vector that visual here but we would obvious do thi with mani mani vector So there would be mani mani more So in our case on just under Now that mean that we have a lot of these sub vector And what we do with these is we run them through their own cluster algorithm So what we do is we end up get cluster and each of those cluster is go to have a centroid So thi on would also be run through on So each subset of vector slice is go to be run through it own cluster algorithm creat these centroid And these centroid ar smaller in size than the origin sub vector here And what we do is for each of these sub vector so each of these sub vector thei get pull into here So mayb thi on is here and it get assign to it nearest centroid And then we take that assign all the wai back over here and add it into our vector So thi is centroid three for exampl And when I sai assign it back probabl the wrong wai to think about it Mayb more like thi So it becom a new vector built from those centroid ID Okai So thi would be three Now what that doe is essenti reduc the size of our vector but pretti significantli depend on what dimens we us there So go back to the code implement that Now we need to defin two new variabl here So m which is go to be the number of centroid in the final vector So on thing that we do need to know with m is that m must be we must be abl to multipli m into d So what is our d valu I rememb Where ar we Let me check So Now we should be abl to divid that into eight I think Yeah So thi is good We can us eight for m but we us someth like five Becaus if we do five we see that d fit So five fit nice into d wherea eight doe So m or d must be a multipl of m Otherwis go to get an error And becaus of the wai that those vector ar broken down into the final centroid ID vector And we also need to specifi the number of bit within each of those centroid So thi valu we can us what we want go to us eight And then we can set up our index and also the quantiz So we us a quantiz as we did befor So the quantiz is go to be ficeindex flat d And also our index here is go to be so thi is a new on Thi is index IVFPQ So not a flat vector anymor which is the full vector a quantiz vector So where we have reduc the size of it through thi through the method I explain befor where we drew it out Now we need to pass a few argument into here First on is the quantiz So the quantiz d which is our own dimension n list m and bit So pass all those to our index Sorri we need to put fice there as well And there we go So we now have our index again You mai have guess that we might need to train thi on There we go So to train it we just write indextrain our sentenc embed Okai So it might take a littl bit longer thi time There we go And then we can add our vector Now after ad those see how quick thi is Should be a lot quicker or a fair bit quicker hard to get much quicker than the last on So go to us the same code as befor So go to take thi down here See So gotten a lot faster So gone from up here millisecond down to two Now there is on thing here These valu ar now differ So the accuraci ha decreas So if we where is the last on here So you can see that we ar get so we have the We still have that on And we have the But these two at the front ar now differ And thi is just on the trade-off of accuraci versu speed So if we come down here give that a go have a look at what we ar pull through So copi thi again just see So we have these I mean although the accuraci ha decreas technic becaus not get the same result as the exhaust search still pretti good result So I mean nonetheless I think that is pretti cool So have a look at compar thi to our previou two other method in term of as we did befor the graph So here is that final on So we have IVF PQ along the bottom Yeah a lot faster right And then we have IVF flat with a end period valu of much faster than but still not quit as fast as PQ And then we have flat at the top which obvious and just as well just be awar on the left here we have a log scale So the differ ar pretti signific when we get up to the million mark So I think it for thi video So I think obvious FIESS is pretti cool definit realli us And I think definit go to explor it more in the futur So for now it So thank you for watch and see 
API Series #3 - How to Deploy Flask APIs to the Cloud (GCP)
Hi in thi video go to be have a look at how we can deploi a plastic API us cloud platform or GCP So actual a veri simpl process and go to jump straight into it So we want to go to Googl and first thing go to do is actual go to the GCP consol So we just write GCP consol and we just click here And that should bring you through to thi page If you alreadi have an account if you you will need to obvious creat an account But onc you have your account you come over here you can either come up here to creat a project or you just click on thi button here creat project And just go to call mine flask API You will need to set up a bill account if you alreadi Mine is alreadi set so I need to do anyth there Now just come to creat Okai that will take a moment for it to actual creat a project Okai but onc it is done we will see we have thi project ID here go to need to keep that So just go to copi it now So go to be us it later And we need to initi the cloud build API So we come up here And we just want to click on thi first link here And what we do is enabl thi API for thi specif project So you need to make sure at the top here you have your your flask API project set So again that will take a moment to actual instal And then we can go ahead and creat a project So go to go ahead and creat a project That will take a moment to actual instal And the final thing that we need to set up is we need to instal the GCP SDK So all go to type into Googl is GCP SDK instal And you see okai we have thi first link here instal in cloud SDK Click on that And you just follow through These instruct ar pretti basic for whichev OS us So for me Mac Click here instal thi And then I have the G Cloud SDK on my termin So go to switch over to VS Code And take a look at what all thi actual look like in our code and actual creat our API and deploi it Okai so in VS Code now I have thi littl directori just an API We creat thi in the previou video seri and an articl So you can have a look at that if you want But the content of thi particularli import As a note import some data from some file here And we have these two endpoint user and locat realli it There noth els particularli import in here just a Flask API So what I want to do is open a termin window here Okai so assum my base environ here And alreadi instal the cloud SDK So that mean I can us these G Cloud call So go to write G Cloud app creat project And I need thi to be equal to the project ID that I copi befor So I have abstract abstract ring Okai and thi will just creat an app insid the project that we creat in the consol befor So go to enter And you also need to select your region that you ar go to be us For me just Europ West So put And that will creat the App Engine app in that chosen project which is an abstract ring which we call Flask API the actual thi is the ID of that project The actual name is Flask API So the app is now creat Pleas us G Cloud app deploi to deploi your first app Now befor we actual do that we need to instal the App Engine extens for Python Becaus on Python right now And by default the Python version of that is not instal or includ So we need to do G Cloud compon instal App Engine Sorri I realiz you probabl see So G Cloud compon instal App Engine Python Okai it And we see so in my case alreadi instal the App Engine Python in my G Cloud SDK So just sai I alreadi have it instal If you just instal it sai it will instal for you So everyth we need for deploi our app I think the last few thing we need to includ here So we have our actual script out API script But we do need a few other thing as well So befor we actual try and deploi our API we should just test that actual work So we can simul the same environ that the App Engine will be run our API within by us someth call Goonicorn or Gunicorn mayb So to instal that we just pip instal Goonicorn I alreadi have it instal so not go to do it again And onc you have instal that you just need to write the Goonicorn And then you select a host port So go to go with a And then you need to write the name of your Python API file So mine is main And then you come down here you can see that we have thi app run So where did I defin app Should be at the top here So app which is the Flask app So after thi colon here we need to includ that And that will initi and run our API Okai Now you can see here make thi a littl smaller You can see here that we have the actual locat So open that And if we just open it on thi first page we will see that it just sai not found Now thi actual a problem becaus we onli have the two endpoint user and locat So open that and see that we get thi JSON respons which is what expect Now the content of that is not import We just want to see that okai the API is actual run when we run it with Goonicorn So good So switch back over to VS Code Okai stop that from run And that mean that the core of the API will run in the cloud App Engine environ So good The onli thing is okai we have our Python environ here So us Python down here And up here I have a few modul import as well So Flask Flask RESTful Panda How is the App Engine go to know that us thi environ It We need to tell the App Engine what our environ look like and what environ it should set up So to do that we need to us two file So come over here and then creat a new file And go to call it appyaml And insid here we just want to set our runtim So the runtim is just go to be Python it Noth els is need there Now go to creat anoth new file And go to call it requirementstxt And in here go to call it run time And go to call it requirementstxt and in here I need to write which modul ar us which packag Our is pretti simpl So Flask Flask RESTful and we also have Panda Now save that And beyond that there anyth els So we can switch back over to the termin window And we can now do the gcloud deploi So write gcloud app deploi And you see also just within the API folder So thi is the folder that order directori that contain all of these file here Deploi Thi will take a moment go to tell it where I want to deploi Okai thi is actual quit us So got thi error pop up becaus I am current in anoth project which no longer exist So get thi error So I need to make sure that try to deploi to the right project So I just write gcloud app deploi I can specifi the appyaml And I just write project equal And then in here I need the project name from befor So just go to go and copi that again So come to here So thi abstract ring Abstract ring And I just write in here abstract ring Okai Now run that That should come up with thi here So we have come down here We have the script and then the target project So correct now go to the correct place And thi URL here is where we will be deploi the API to So go to click continu Ye And thi can take a littl bit of time for it to run So just wait a moment One thing actual pretti us is when you try when you first deploi gcloud will creat thi gcloud ignor file like a git ignor And in here it will just ignor the typic thing that you do actual want to ignor So you have PyCach here which ha been creat up here So we need to exclud that from the upload by default which is us I think So see how that is go Still upload Still upload Okai So good to go go to copi thi Open in the browser And we should see so what we saw befor with thi we should end up see the same So go first to that address Okai We get that not found that we saw befor So expect And just go to user There we go Okai So we see that our API is now deploi to Googl Cloud I can also see that over here So if I refresh we should see the request pop up over here Or thei will do eventu So thi will give you a summari of how mani request ar made to your API it for thi video I hope been us And I will see you in the next on 
The NEW Match-Case Statement in Python 3.10
Okai so go to go through the new match case statement in Python So the new match statement actual call it structur pattern match and what it doe is basic allow us to creat a switch case statement in Python So switch case statement ar pretti common in most languag but someth that Python just had and we can actual see in thi pep here pep that thei were go to add it back in or been consid at least but then it wa just reject becaus basic no on want it So sinc then it just been ad to Python but now with thi new pep pep it look like go to be ad in So what done is download Python alpha and just go to have a plai around and see how thi work So thi first quick exampl of how go to look and just a super easi simpl exampl just go to comment these out becaus caus an error and you can see here that we have thi HTTP code and what do here is match the subject which is thi head which is thi HTTP code and then we go into our case So in the case of that subject be equal to thi we do whatev is within thi block and we do the same if it is or if not we go down to and at the end of that we can also add a catch all case statement So in thi case we just sai okai case we put anyth here and we just sai okai code not found So thi is what will run if none of these ar execut So we can also remov thi and it will just carri on with the code So our first exampl you see here we get a teapot for code Obviousli a super simpl exampl and essenti how it work I also have thi graph here to try and explain more visual the actual flow of inform here So we have a subject which we set to match at the top So in thi case we ar us HTTP code and then we check for truthi or falsi whether that subject match the pattern that provid in our case If it is truthi we execut the block if it is falsi we continu to the next case So yeah pretti much how it work but have a look at a few other exampl which I think demonstr the actual us case and benefit of thi a bit better So the first on though that I want to quickli show you is thi on which is actual from thi pep here and I thought thi wa a pretti cool exampl So what I show you is just a realli simpl you know is thi equal to thi if not go on to the next on wherea thi I think better demonstr that we can check the structur of the data that feed in So here check the host port and mode of a connect So for exampl if connect through HTTP we might set our mode to HTTP and then we also have a host and a port and thi will all be within a tupl Howev in some case mayb show you here so we can have our host here and then we have the port and then here we mai sometim have the mode of the connect so mayb FTP and sometim we might not and if we see anyth here we just assum HTTP and essenti what thi code here is do So take that tupl as x and in thi case we can see us if elif and fine like noth wrong with thi but then if we look at how we write thi us match in case it doe look a lot cleaner So just on exampl and then I have an actual us case exampl been us the data set which is a veri popular data set for train question answer model in machin learn and I think actual a realli cool exampl of how the case match statement might actual be pretti benefici So just separ thi out and walk you through how we can appli that So first just go to import request and also import json becaus go to be pull a json file from the internet and go to be read that and the url for that is thi and the file that go to be read is the train data from the squad data set Okai so the data that look for is at thi address So if we want to just download that just download that we can with request and just get url plu the file and then after pull that us request just go to save it to file So go to file write binari as f and go to write it to file in chunk quit a big data set So go to us a content chunk size I mean we can kind of go for anyth here go for and then just write the chunk Okai so our data set download and we can open it over here and if we just look through a few of these we can see that quit a few layer to the data set So thi is someth go to have to consid when build out thi function for pars it both with the if els version and the match case version So what go to want to do here is loop through each on of these and you can see quit a few of them and we want to get the question and the text here which is the answer to that question and we just want to pull those out and noth els just put them out as a tupl and creat a big list Now the complex of thi come from so if I sorri open that again you can see here we have okai thi is fine and we have a question and we have a question and then we have answer which contain our answer So just rememb got answer here okai thi is the name of the kei that contain our answer If we go right down to the bottom the actual format is differ and thi is the case for quit a few of them So if we go into here okai see straight awai we have the question and we have answer but empti and instead we have thi plausibl answer and in here we have our answer So slightli differ format in that rather than us answer some of them us plausibl answer not sure why but some of them do and for some of those as well thei also includ thi answer kei which is just an empti list So we just need to write some logic to actual deal with that Okai so save thi file so just go to open it back up again and it is onli just here just read binari f and we just save it into the squad variabl have a quick look at what we have in here So you see over here everyth is contain within thi data kei so if we close out it close everyth So first off we want to access that and then a list and loop through each on of those So just have a look at the first version or the first item in that list and the Beyonc group So we can just have a quick look just quit messi not go to go into it but essenti to work through thi data go to have to write someth like thi and thi is for both the if els statement or if els version and the match case version and we write squad data So thi is go through each group so Beyonc or Matter we ar the two that we saw befor Now go to go through each paragraph in the group that is paragraph and then go to go through each question and answer here within that or each question and the inform that ha next to that question and that is in the paragraph QAS And just go to pass for now but that will loop through everyth that we need so go to us that for both the if els version and the match case version So take thi and build out the if els version first So in thi case we can just get our question straight awai so we ar at the moment gone through paragraph and loop through each of these so in QAS at the moment and go to pass through the first on So in QAS at the moment and we want to get the question which is thi so QA question and then thi is where we have our if els logic So answer in here and first we want to check okai is there an answer kei within the dictionari here So we write that like if answer in QAkei but as we saw befor the answer kei can be there but it can also be empti and if the case we also obvious we pull anyth out from that answer list So we also need to sai and length of QA answer is greater than zero So we have two condit there and if both ar satisfi then we want to pull the answer from that kei So answer QA answer and then alwai with thi data set we ar alwai enter the or alwai onli on item within the list of answer So we just pull out the first item at index zero and then we pull out the text So if I show you we have index zero and then we have the text So enter thi dictionari or list sorri and then enter index zero becaus all index zero and then pull out the text which can be in the late for thi first on Okai so that is our first part of the if els statement Second part we want to sai okai els if we have plausibl answer So thi is what I show you at the end plausibl answer in QA kei and we also want to sai the same again where QA plausibl answer need to have some valu in it as well So in thi case we pull our answer from plausibl answer So in thi case we pull our answer from plausibl answer And the same again there Okai so our if and elif and then we just want on final els statement at the end So if for some reason we find the answer just go to put answer equal to none Then after all of that go to just initi a new squad list here And just go to append our question answer pair to that So new squad dot append have question and answer Should realli be answer answer rather than answer but fine Okai so let me just put these into a two port Okai great so just have a quick look at what we get here Make sure get the right thing So go to take a look at the first five and the last five as well Okai so thi look good Yep so we have two pull question in the first index and the answer in the second index Okai so great and work for both the answer and plausibl answer all of these down here us the plausibl answer format So thi is what it look like with if elif and els So rewrit thi But go to do it with the match case statement instead So take thi and what we can do is just remov all of that and mayb we can just keep in the question and what do is sai match QA and then in here what we ar look for is in the case of have a dictionari that contain the answer kei and that answer kei contain a list which also contain a dictionari which contain a kei call text We want to pull out the answer from whatev that text kei is point to So the valu there and thi will pull whatev is within thi part of the dictionari or the part of the data structur into thi new variabl answer Which is I think realli cool about the new syntax So what done here is actual alreadi assign our variabl answer so we actual need to do anyth within thi code block So pretti cool and then we just write the same thing but we do it for our plausibl answer And then we just write the same thing but we do it for our plausibl answer So if we write that out and thi in my opinion is pretti cool as well We just write it like that and all there is to it and we just write pass again and in the final case of it not work we just sai okai just set answer equal to none And run that and just take what we we have here and we should get the same And we should get the same Okai so yep we can see exact same output So the comparison I mean to me thi doe look wai cleaner and thi is I suppos a littl bit complic but gener I think thi is kind of easier to read than thi Or at least at first glanc thi doe look cleaner in my opinion but you know not sure on which on I would go for At the moment kind of lean toward thi on but see see how peopl start us thi new syntax go forward be pretti interest to see at least So that is all I want to cover on that So it for thi video Thank you veri much for watch I hope 
CLIP Explained | Multi-modal ML
Todai talk about what is quit possibl the futur of machin learn in both comput vision and NLP talk about a combin of both into a singl multimod model That model is Clip which wa built and train by OpenAI Now Clip is open sourc and it ha been around for a littl while been around from sinc the start of but in the past few month seen the adopt of Clip grow at a pretti insan rate It ha found us in a load of thing so what we will be focus on is text or multimod search across text and imag also go to talk a littl bit about zero shot classif zero shot object detect and at some point in the futur also talk about the us of model like Clip or even Clip itself in the diffus model that have becom incred popular recent like DALY Imagen and Mid-Journei Now to understand why Clip is so import we can us a paper that wa releas in call Experienc Ground Languag In Experienc Ground Languag the author defin these five differ world scope corpu internet percept embodi and social Now most of the model that awar of in NLP thi is a sort of NLP languag focus paper these were sort of the state of the art veri recent so you have BERT and all these differ model GPT and so on In percept thi is where we start to see not just NLP but also comput vision so here is where we ar now with Clip Okai and quit interestingli as well in the next world scope thi is where you start to get reinforc learn and then you continu go so you almost see thi culmin of all the differ disciplin and machin learn come togeth into on larger disciplin Now the focu between these differ world scope is mostli on the data that is be us to train the model so over here world scope on you have the first model that we saw which would have been thing like which is probabl the on of the earliest exampl of deep learn in NLP and that consist of train a neural network on a smaller or not small amount of data but small compar to futur data set so a singl corpu you know for exampl sentenc from Wikipedia mayb on exampl of that Then go forward we have the internet size corpus so these ar base on a huge web scrape from across the internet from mani differ sourc and the model train on thi were abl to obvious kind of pull in more gener understand of languag from pure text data Okai so a lot of data but so pure text The next on which focus on world scope is not just that in our exampl text and imag data Okai so train a model to understand both of these differ modal of inform and thi is almost like AI move from a pure pure digit veri abstract space to a more realist real world space becaus in the real world we just reli on text data we reli on a huge number of sensori input we have visual audio touch and everyth els Okai so sort of move more toward that more broad scope of input from differ modal okai where modal would be someth like text or imag or visual and so on For us in the real world that chaotic ensembl of differ sensori input is what kind of creat or train our intern model of the outsid world so it make sens that that is the sort of direct that machin learn and AI mai also go in So to achiev thi multi-mod in CLIP we actual us two model that ar train to almost speak the same languag So with these two model on of them is a text encod on of them is an imag encod Both of these model creat a vector represent of whatev thei ar be input so the text encod mai get a sentenc that sentenc could be two dog run across a frosti field and then we have a imag of two dog run across a frosti field and CLIP will be train so that the text encod consum our sentenc and output a vector represent that is veri veri close align to what the imag encod ha output base on the imag of the same concept Now by train both of these model to encod these vector into a similar vector space we ar teach them to speak the same vector languag right so thi is veri abstract thi thi vector languag is like dimension space so we directli understand what or veri difficult for us to directli understand what is actual happen there but these two model do actual output pattern that ar logic and and make sens and we can see some of thi by compar the similar between the vector that it output okai so we can see that the two vector for dog run across a frosti field both the the text vector and the imag vector ar both within a veri similar vector space wherea someth els like eleph in the Serengeti is you know whether text or imag is not here with our our two dog run across the frosti field is somewher over over here right in a complet differ space so what we can do with that is is calcul the similar between these vector and identifi which on ar similar or not similar accord to clip from thi from these these meaning vector that that clip is actual output we ar abl to creat a content-bas imag retriev system okai so content-bas imag retriev is basic where we um us some text or us mayb even anoth imag we can search for imag base on their content right and not just like some metatextu metadata or someth been attach to it and with clip unlik other content-bas imag retriev system um clip is incred good at actual captur the mean across the entir imag so you know for exampl with our our two dog run across a frosti field we might also be abl to describ the background of that imag without mention that two dog in it and if we describ in such a wai that um we align pretti well with what that imag actual is what is in that imag we might actual also return the imag base on that so not just focus on on thing in the imag clip allow us to focu on mani thing in the imag so an exampl of that is within thi data set been us here there ar no imag on singl imag of the food a hot dog okai so i tri to search that and the first imag that is return is a dog eat a hot dog okai so pretti relev but of cours there ar no other imag of hot dog in thi in thi data set so the other imag that ar return ar quit interest becaus in some wai or anoth thei ar kind of show a hot dog so the first on we have a dog look pretti cozi in a warm room with a fire in the background then we have a dog in a big woolli jumper and anoth dog kind of like pose for the camera so weirdli enough we we got a load of hot dog imag even though not realli um mayb not exactli what we meant when we said hot dog but a person could understand that okai we can we can see how those that term and those imag ar relat now not actual onli restrict to text to imag search when we encod our our data when we code text and when we code imag we ar actual just creat vector so we can search across that space in ani ani direct with ani combin of modal so we could do a text to text search imag to imag search we can also do imag to text search or we can search everyth we could us some text to search for text and imag we can kind of go in ani direct us ani modal we want now go into a littl more detail on what the architectur of clip actual look like so clip as i mention these two model now these two model ar train in parallel on of them is the the text encod now a just a gener text encod of layer and then on the imag encod side there ar there ar two differ option spoken about there is a vision transform model and also a resnet model and thei us a few differ size for resnet as well both of these both of these encod model output a singl dimension vector and the wai these model is train is is kind of in the name of clip so clip stand for contrast learn in pre-train and so the the train that is us dure pre-train is is contrast contrast pre-train now across both nlp and comput vision larg model sort of domin the the state of the art and the reason for thi or the idea behind thi is that just by give a larg model a huge amount of data thei can learn sort of gener pattern from what thei see and almost kind of intern a a gener rule set for the the data that it see okai so thei manag to recogn gener pattern in their modal in languag thei mai be abl to intern the grammar rule and pattern in english languag for vision model that mai be sort of the gener pattern that you identifi or notic in with differ scene and differ object now the problem with these differ model the reason thei fit togeth veri well alreadi is that train separ so by default these state of the art model have no understand of each other and that where clip is is differ what clip ha ha brought to the tabl here with clip the text and imag encod ar train while consid the context of the other modal okai so the text encod is train and it consid the modal or it consid the the concept learn by the imag encod and the imag encod doe the same for the text encod and we can almost think of thi as the the imag and text encod ar share a almost indirect understand of the other modal now contrast train work by take a imag and text pair so for exampl the two dog run across a frost field and put those togeth into the text encod and imag encod and learn to encod them both as as close as possibl for thi to work well we also need neg pair so we need someth to compar against thi is a gener rule in contrast learn you just have posit pair becaus then everyth can just be kind of encod into the same like tini littl space and you know how to separ the the pair ar dissimilar okai so we need both posit and neg pair so we have a posit pair okai in order to get neg pair we can essenti just take all the posit pair in our data set and we can sai okai the pair and we can mix with differ ey okai so we can do with and and so on so basic just swap the pair and we can we can understand that other pair ar probabl not go to be similar as long as our data set is rel larg occasion mayb we will get a pair that ar similar but as long as our data set is larg enough that that happen too frequent not go to affect our train it will be sort of a neglig problem so with thi idea we can us a loss function that will minim the differ between posit pair and maxim the differ between neg pair and that will look someth like thi where we have our posit pair in the diagon of the similar matrix and everyth els is someth that we the dot product there we need to maxim and thi imag that you see here is actual the pre-train for a singl batch okai so on interest thing to note here is if we have a small batch sai we onli have a batch size of two go to be veri easi for our model to identifi which two item ar similar which two ar not similar wherea if we have in our item in our batch it will be much harder for our model becaus it ha to it ha to find more nuanc differ between them and and what basic the odd of guess randomli between those and guess correctli ar much smaller so a larger batch size is a good thing to to aim for in thi contrast pre-train approach so with that i think we we have a good idea now of how clip can be us and also you know how it ha been train for for thi so what i realli want to do now is kind of show you how you might be abl to us it as well now go to be us the vision transform version of clip okai so we rememb i said a the resnet and vision transform option for that imag encod go to us a vision transform version and openai have releas thi model through the hook face librari so we can we can go to the hook face librari and us it directli from now which make it realli easi for us to actual sort of get start with it so go ahead and do that now okai so for thi we will need to instal a few librari here so we have transform torch and data set so data set we need to actual get data set so prepar on especi for thi so we have thi imag text data set and in here we have not much just imag or text to imag pair and we can see what thei look like so we have thi text aeroshock of a futurist citi with a larg motorwai okai so i tri to just describ thi imag as as best i could and yeah what i got and there ar like as like you saw just now of these imag text pair in there so go ahead and actual prepar or download and sort of initi clip for our for our us so the the model id on hook face is thi so if we were to go to hook faceco we could type that in here and we have the model there okai so thi is the model that us over from openai here and with thi model we we us these two we us a processor and a model so thi is the model itself thi is clip right thi is a almost like a pre-processor for both our text and also the imag okai so on thing we would do here if we have a CUDA devic avail we can move our model to the CUDA devic at the moment if you try and do thi with np so if on mac and you have a you have appl silicon there ar some processor or some transform in the clip that function on np at the moment so i would stick with cpu onli do infer so still pretti fast now as i wa mention the the processor is what handl both the text and imag prepar that need to happen befor we feed them into the actual encod model themselv that make up clip so for text we do thi so thi is just go to be thi is go to work like a normal text token a normal text token for text transform model is us in order to translat our human readabl text into transform readabl id okai so we pass the text here we make sure we ar sai there ar no imag includ here becaus the processor if we have both imag and text it can process them at the same time we can do that here as well but i want to show you it separ just to show you what actual do so the pad we need to set that to true and that is becaus differ differ sentenc can have differ length okai so you have like hello world and whatev i wrote befor up here so thi aerial shot of futurist citi aerial shot of a citi these two sentenc have differ length and a transform model need to see the same length be input for all of the the text that is within thi sort of singl batch so basic what go to do there is add what ar call pad label so just go to add a few of these up to the length of the longest sequenc within that batch of of text item becaus in here we have those um no sorri sentenc so all do there sure that is uh and then we ar return those as pytorch sensor and then final just move them to whichev devic us us cpu here so not actual necessari to do thi but do it in case you do do the same on a cuda enabl devic so from there we have these input id and an attent mask okai so have a quick look at what what those ar so we go into token and we have a look at input id okai you see we get all these liter just integ valu and see that a lot of them have thi at the end all right that is the pad token there okai so thei thei ar not repres as string but repres as these integ number okai and we know that the pai token becaus thei appear sever time at the end of each sequenc and none of the sequenc i fed in there were thei have ani similar word at the end of those okai so you can see them all here so we know that those ar the pattern sequenc we also see like an initi of sequenc token there as well and then everyth in between those thei ar token that repres a word or a part of a word from our origin text so the input id the attent mask so the input id the attent mask is see so here you can see that just these on and zero now the on repres real token okai thei repres real word that were in our from our text input the zero repres where the where our processor ha ad pad token so thi is us for the intern mechan of the text transform to know which token to pai attent to which on to ignor becaus we want to realli focu on those pad token becaus meaningless just there to make sure we have the same size input go into our transform model all that is so we can go down and after we have our token you know what we what we do is we us clip to encod all of them with thi get text featur okai and then we pass our token and got two devic here i think i alreadi i alreadi move them to devic so i need to do that again we can actual remov that okai and okai what do we get here so we get so text input that make sens dimension vector okai so thei ar our text embed repres each of those those text sentenc that we just gave and then on other thing i want to point out here is that we have the min and max valu and pretti big okai clearli not normal so thi depend on what do if you ar if you want to compar these vector you need to make sure not us a similar metric that look or that consid the magnitud of your vector you need to onli consid the the angl so you can do that with cosin similar or the altern is that you can normal these vector and then you can also do thi with dot product similar okai so to normal if you want to us that product similar now you would do thi okai so here just detach our text embed from the the pytorch graph move them cpu if need we actual need to do that but do it here anywai and convert them into a non-pi arrai and then we calcul the valu that we will normal that we will normal it each vector by okai so for each each vector calcul a number and then that number is what go to divid them all by here okai to to normal that and then after that you can see the minimum maximum is thi minu and plu okai so neither of them go over minu on or plu on now now when it come to encod imag we we do the same thing or veri similar thing so imag ar also pre-process us the us the processor as we did with our text but we just us slightli differ paramet to start there so the reason process these imag is that clip expect a certain size of imag when when feed imag into it and expect those those imag pixel to be normal as well now rgb imag by default the the valu the pixel valu and thei will rang from zero to we need to normal those and we also need to resiz the imag so you can see you can see that here so the first imag it ha thi size a pretti big imag okai thi is the the width and the height of that imag now here take all the imag and process them make sure we sai text is is none and that will actual onli output on tensor the pixel valu tensor so just go to extract that straight out there and also go to move it to the devic set hardwar devic in thi case just cpu and now have a look at thi imag or imag now so now we can see that we have thi thi arrai or tensor with three color channel so thi is the rgb and it ha a height and width of so been you know sort of squeez into a smaller size now and we have dai becaus we fed in all of our imag okai so thi is how we us the processor and thi is just resiz and normal our imag readi for the divis transform encod of clip and veri similar to befor befor we us get text featur now go to us get imag featur and we pass in those imag like that and again as you you might expect those imag ar not normal you see that here and as we would also expect thei ar the same dimension as our text embed so that mean we can compar them but befor compar them of cours as befor we we normal them so we should normal them again here um and yep same process again and we can see that those have those have chang okai cool so what we now want to do is calcul the similar between all of our imag embed and all of our text embed so we can do that in a few differ wai we have cosin similar or dot product similar the reason we can us our product similar is becaus we normal but go to show you how to do both so that if you normal you can actual just us a cosin similar like we do here so cosin similar is actual just a dot product as a numer between the text embed and imag embed and in the denomin we have just normal the norm valu of both of those okai that is all it is actual so pretti pretti simpl and if we plot those similar score between those we get thi so we would expect along thi diagon here expect these to be the highest similar valu which sai repres the the true pair okai between the imag and the text now we have some that ar not quit there like here and there is thi imag text pair which is more similar even with with someth els you know i veri quickli put these togeth so not alwai go to be perfect so we have on here that is mayb not perfect um but again there is also a lot of overlap between these imag so there ar sever imag of citi skylin and a lot of time i describ those as futurist citi in you know in whatev with a big motorwai or someth on those side so probabl where get that from um now if we were to calcul the dot product similar between these we would expect it to be the same okai now um okai from thi thi calcul dot product similar we do seem to get a veri similar um set of similar at the end there but ar thei the same well if we go down here we can see that thei pretti much ar so we do a straight comparison we do we do cosin similar equal dot similar becaus the number ar actual slightli differ but onli slightli differ becaus sort of a float point error becaus these ar all float point number so we get veri veri veri small um differ between the number and you can see that here so taken calcul the differ between them between the number and the two arrai and then look okai the minimum differ between them zero okai what expect where the number ar exactli the same the maximum differ between the number and to the minu eight so like and so on two so veri small number and okai with that we know just float point error between those two similar arrai so pretti cool to to see that and we can us thi exact concept of compar with with a cosin similar or dot product similar to actual search through all of our imag with like a text prompt for exampl but not all that clip is good for clip is also ha thi amaz perform as a zero shot model for differ task okai so not even just on task but actual differ task so um it is it perform incred well out of the box for classif and go through thi in more detail in a futur video but the idea is that given a set of um class from a classif imag classif data set you can mayb you can modifi them a littl bit the class name to make them more like a sentenc and then you us thi same idea of compar all of your your your text represent of the class with a set of imag from the data set and with thi you just calcul the similar between those and the the text embed or the you can think of it as a class embed that get the highest similar to your imag is the predict class okai so you have zero shot classif like that super easi anoth us case is object detect so you sai you have mayb look for a cat or a butterfli in an imag okai and you okai when look for the cat gonna gonna us a chunk of text that sai um a fluffi cat okai and you encod that with clip and you get your text embed and then what you do is you break up your imag into all these littl patch and you just slide through all of those patch okai you can you can includ like an overlap so go over those over like not miss anyth between patch so just slide through your imag and with each part of the imag that you slide through you extract the imag from that you process it through clip and then you compar the encod for that imag against the tip that you creat so a fluffi cat and what we will see is that patch of the imag that contain what it what it is just describ will have a higher similar rate okai and then you can overlai those score back onto your imag and you will find that the that clip is abl to essenti identifi where in your imag a specif object is and you ar just describ that imag us a natur languag prompt now these ar onli a few us case of clip and onli realli scratch the surfac of what is actual possibl with thi model we also see be us in and like i said the diffus model like dali which is a great exampl of how power clip can actual be so it for thi it for thi introduct to clip i hope been us as i said go to go into more detail on the differ us case of clip and how to appli clip for these us case in futur video but until then it for now so thank you veri much for watch 
How to build a Q&A AI in Python (Open-domain Question-Answering)
Todai go to have a look at Open Domain Question Answere and how we can fine tune our own Retriev model to us for Open Domain Question Answere go to start with a few exampl Over here we have Googl and we can ask Googl question like we would a normal person So we can sai how do I tie my shoelac So what we have right here is three compon to the question and answer And I want you to rememb these becaus these ar relev for what we ar go to be build We have the queri at the top We have what we can refer to as a context which is the video which is where get thi small more specif answer from And we can ask anoth question Is Googl SkyNet So we have our question at the top We have thi paragraph which is our context And then we have the answer which is ye which is highlight here So slightli differ to the previou on where we had the video Thi time we have actual text which is our context And thi is more align with what we will see throughout thi video as well Now what we realli want to be ask here is on how doe Googl do that And more importantli is why should we care Now we can imagin thi problem as us be in a realli big warehous Now we know anyth about thi warehous We just know that we want a certain object Now we know the specif name for the object We just kind of know what it look like Now in thi warehous everyth we assum is probabl go to be organ in some wai So the first thing go to do is have a look at the product around us and try and figur out some sort of order And onc we have figur out how thi warehous is structur and how we can search through the warehous we need to figur out okai everyth is mayb organ base on the name of the product We know the name of the product kind of go to struggl along go to spend a lot of time search through a lot of item in order to find what we actual want to find That is like a normal tradit search experi where you have to know the exact keyword that you will find with whatev it is look for You have to know the product name Now thi sort of natur languag search that we just saw with Googl is not like that warehous where just alon try to figur out how structur and how to search through it Instead almost like you have a guid with you someon who know thi warehous Thei basic live in thi warehous Thei know where everyth is And if thei know where a specif object is thei can point you in a pretti good direct and realli help you find everyth probabl a lot faster Becaus as well as know where everyth is thi person also speak the same wai that we do We can ask them a question like okai guid where ar those marble-lik thing And with that thei will hopefulli be abl to guid you in the right direct And mayb be abl to guid you right to that product or at least in thi area that thei ar found in So the differ between tradit search and a question and answer search Now get me wrong There ar place where you do want to keep tradit search But particularli for unstructur text data or as we saw earlier video and audio data thi sort of Q&A approach can be realli power So that lead us on to the second question or first question I ask which is how doe Googl do that Now Googl is pretti complex But at the core of what we saw there wa someth call Open Domain Question and Answere or ODQA Now ODQA is a set of languag model and technolog all pile togeth into a open domain question answer pipelin Now that pipelin at it simplest will look someth like thi So we have our sai at the top we have our question That question is go to come down here and go to hit what is a retriev model which is what we will train or fine tune in thi video Now thi retriev model that will handl take our queri and convert it into a vector Now we convert it into a vector becaus then we compar it to other chunk of text and we can encod the semant and mean behind that text rather than just the keyword So why we can search for concept and mean rather than just keyword as I mention earlier So we have thi retriev model Now the retriev model creat a vector but then we need someth that allow us to search So we need other vector to compar to Now where do we store those vector Well we have a vector databas Now a vector databas bring that over here Thi is go to contain context vector So you rememb earlier on with those Googl search we had the question we had the context and the answer Thi is where that is relev So in here we have load of context So just put C but all been convert into vector us the same model we have up here the retriev model We just did it befor we start search So we index those context into our vector databas Now at search time we convert our question into a vector That come down into the databas and the vector databas will compar that question vector to all of these context vector and it will return the on that ar most similar So mayb we want to return the top five most similar context At thi point if we ar just us a retriev model and a vector databas we can return those We had our question and we can return these context to the user So that would be like in our earlier exampl we ask a question and Googl return the page or it just return a paragraph to you rather than highlight the specif answer And these ar the compon that go to cover todai And in a futur video and articl what we ar also go to includ is a reader model So a reader model is ad onto thi Open Domain Q&A stack or the last compon of the stack And what thi doe is it take each of your context vector and it read them so it ha a look at the context vector and we have thi long text here and it sai okai given the question which we also feed into our reader model I think the answer to that question is right here So it allow us to extract a veri specif answer from a longer chunk of text the context So the Open Domain Q&A structur or pipelin And now I think we should move on to actual fine tune the retriev compon of that pipelin The first thing we need to think about when fine tune our retriev model is whether or not it actual need to be fine tune becaus there ar retriev model out there alreadi that we can just download and us And thi realli good concept that I saw from in on of Nil YouTub video where go to talk and he talk about the long tail of semant related And the basic gist of it is that you have common knowledg that everyon pretti much everyon know about and you have a lot of data and benchmark in that area So that would be our cat versu dog exampl up here So imagin on the street you walk up to a stranger you ask them a question the differ between cat and dog probabl go to know the answer Everyon know that Then you get more specif So you ask them OK the differ between C and Java Some peopl know some peopl will not And then we get even more specif PyTorch versu TensorFlow And then we get more specif Roberta versu Dberta and then TSA versu Mirabert As we get more specif less and less peopl know what talk about And with that there ar less data set and there ar less benchmark But at the same time where most of the interest us case exist Now whether your us case exist within the common knowledg area or within the long tail is realli how you can hazard a guess at whether you need to fine tune a retriev model or not So if we just modifi that chart a littl bit we get thi So we have the same thing common knowledg on the y-axi And what we have on the right just renam it So the eas of find a model and or data OK so base on how nich your us case is the harder it is go to be to find data for that nich And the less data there is out there the less like someon els is alreadi train or pre-train a model So most of the pre-train model out there ar train on a veri gener broad rang of concept train on Wikipedia page or someth like that So fine You know if you compar cat versu dog or even seed versu Java the model is probabl be pre-train on someth similar to that and it might be abl to figur out But if your us case is more specif like you have I know like some veri specif financi document or technic document someth along those line where not mani peopl understand the content of that document And not you know not gener knowledg or not easili access on the Internet In that case you will probabl need to train or fine tune your own model So if that is the case how do we do that Well to train a retriev model we need pair of text So we need question and relev context So what you can see here We have question A context A both relat and thei both end up in the same sort of area what we need to teach our retriev model to do We tell our retriev model OK question A and context A Process them and output a vector for each on of those And then we want to look at those two vector and sai OK ar thei similar or not If not similar we tell the model look you need to figur thi out and make them more similar If thei ar similar then great good job what optimis on optimis and minimis that differ between similar pair and maximis the differ between dissimilar pair Now our data is go to just contain all of these row where we have the question and context pair We need label becaus we ar go to be us multipl neg of rank loss which discuss in a minut also a video on that if you do want to go into a littl more depth So to train our model go to be us the dataset from over here And in there we have our question and we have context OK what go to train on those two pair So take a look at what that process look like So as I said go to be us the dataset go to be pull that from Huggabas dataset so you mai need to pip instal dataset if you do not have that alreadi And thi is how we load the dataset So got and get the train split of that becaus also valid split that we will us later And from there what we will see is get an ID titl context the question So onli the question and context is realli that import for us So we can go down and just a few exampl here of differ sampl or row from the dataset And like I said befor we need to take the question and context pair from the dataset So to do that take them here also creat thi input exampl or list of input exampl object Now input exampl is just the data format that we us when we ar train with the sentenc transform librari which you can see here Now again if you do need to instal that just pip instal sentenc transform And TQDM is just the progress bar that you see down here So what do is just append a load of input exampl where we have the question and context We have ani label here becaus we ar go to be train with multipl neg rank loss where we need label Now also becaus us that type of loss MNR loss we need to make sure that each batch doe not includ duplic Now the reason for thi is when we train with MNR loss go to be put everyth in or train in batch And the model is go to be look at two pair like thi go to take the question and go to sai OK for that question thi context here need to be the most similar and all of these context need to be as dissimilar as possibl Now the problem with if you have duplic in your batch is that you have sai the exact same mayb not the exact same question but you have the exact same context down here Now your model is optim to make all of these as dissimilar as possibl But also thi on here even though exactli the same as the on that optim to be more similar So you realli need to try and avoid thi OK if it happen occasion in the odd batch but you need to avoid it as much as possibl So why we us thi no duplic data loader Thi will make sure we have ani duplic within each batch So go down We need to initi a sentenc transform Again thi is the same as what we usual do written BERT here but actual us the Microsoft MPNet model Now MPNet is realli good for sentenc transform in gener If I did thi with a BERT model I think the perform is two percentag point less than if I us an MPNet model not huge but it doe make a differ So good to try both if you want But thi on is the on go with So here initi the model We also have thi pool layer Now the pool layer is import That is what make a sentenc transform rather than just a normal transform And it work like thi So we have our sentenc It will get token and split into mani token all down here and put into BERT MPNet or some other transform And on the output of that we get all these token vector Now all these token vector repres our sentenc but there ar load of them We want a singl vector to repres a singl sentenc So we us thi pool layer and thi pool layer take all of those vector And take the averag in everi singl dimens So we take the averag and we get our sentenc vector both how and why we us the pool layer So we have that We can come down and see our transform model We have the pool layer good Like I said earlier we ar go to be us MNR loss for train And that is the batch thing where we want to get the pair togeth and then we rank all the other context as dissimilar So we initi it like thi us the sentenc transform librari and then readi to train our model So realli not that complic particularli if us sentenc transform librari veri easi So we warm up for of the train step a pretti standard number for sentenc transform You can modifi it a littl bit but is usual pretti good It just help make sure we overfit And the same for EPUB Almost alwai set that to on when fine tune sentenc transform Otherwis it realli doe tend to overfit So usual a good idea It take too long to train minut for me here It might take a bit longer but realli not train on too much here I think we have like a hundr thousand I think we have a hundr and thirti thousand train sampl in the spot data Now anoth realli import thing is evalu So onc train our model how do we know that it actual work Is it perform well We know So we need to measur how accur our model is retriev the correct context for a particular question which is slightli differ to other oper or evalu metric that we us with other languag model And to evalu the retriev perform we us thi inform retriev evalu Now from thi go to be us the map at K metric which is in short an averag precis valu or fraction of return context that ar relev to the question we ask And the at K compon of that is just sai we ar go to consid the top K return result So if at as in K is equal to go to return context And then go to calcul that metric from those return context Now by default thi evalu is go to be us that map at K metric So we initi it like thi So inform retriev evalu and it need data go to be us valid set of the same data we us befor So the squad data set And it look the same We have ID titl context question answer We at the moment go to need ID context and question Now thi evalu need us to map relev question and context us those ID So what first go to do is go to convert thi into a panel of data frame I find it a littl easier to work with what go to be do here So convert it to a data frame here We see we have context ID question which is all we need Now we need to assign a specif or a new ID to the context Becaus at the moment if you look here we have an ID and share by the context and also the question And anoth thing is the ID for the context like here even though the context is the same the ID is differ So what go to do is us these ID for the question becaus all the ID seem to be uniqu go to creat a new ID for each context So go to dedupl that data frame So we have question and ID noth els And then just go to append thi con onto the end of our ID So now we have uniqu ID for all of our context as well as our question And what we can now do is merg or form an inner join with our no-dup data frame and the origin data frame So do that And now we have a uniqu ID So thi ID Y for each of our context So you can see ID Y is not chang where we have these duplic context And then we have ID X for question So thi is what we need for our evalu So we need to reformat thi into three differ dictionari So we have IR queri which is a map of question ID to question like the text We have IR corpu which is a map of the context ID to the actual context And we also have IR relev doc which is a map of the question ID to relev context ID So you can have multipl context ID for a particular question But in thi case it is actual just on to on map So we first creat the IR queri dictionari So liter just the ID as a kei and the question as a valu So we creat all of those kei valu pair Same again for IR corpu exactli the same but thi time with a context And then if we come down here so thi on is slightli differ So thi is map the question ID to a set of relev context ID Now we could map these directli But what done here is if you us thi same script and you have an exampl where mayb you have multipl context for each question thi will handl those And so you will get a list or a set of multipl context ID in here rather than just on if that is relev And we see that we have multipl in our case we have multipl question that map to a singl context ID So actual mani to on rather than on to on like I said befor Now with each of those three dictionari we can visual our evalu which is just pass in like thi and we evalu And realli simpl noth els to it And so the map at K perform for that is which is good If you compar it to some of the set of the art retriev model So thi is also us MPNet and been train on a lot more data by the wai as well So more more gener Thi is get percent So a littl bit better than our like two percentag point better But thi is the SQUAD dataset Thi other model here ha pretti confid ha been train also on the SQUAD dataset So it ha alreadi seen that data If you do the same thing but for your own more nich dataset I think most like you will get the model that outperform ani other pre-train model But of cours definit just test it evalu what you get OK so we have fine-tun our retriev model now But we realli us it unless we saw the vector that we creat with our retriev model somewher So we need a vector databas Now to do that we need to take a few step So we have alreadi done a few OK so we have train specif we train our retriev model so we can cross that off also download a load of context So the SQUAD context us a valid set for that Now the next step on thi side is to encod those context which we will do in a moment And then over here we also need to initi our index for our vector databas So go to do that and then go to take those encod context and now initi index And go to popul that index with our encod context vector So get start with that OK so in a new notebook now So if follow along and mayb your model is still train or fine-tun fine You can actual just download the model that we train here So just Pinecon MPNet Retriev the model that you saw a moment ago If you want to try to pair it on as well you can replac thi with BERT And also there So we have that model We can see it look same as what we had befor And also just go to reload the valid data set Now go to extract all of the uniqu context becaus we have like we saw befor the SQUAD data ha mani copi of the same context And we want to have duplic in our index of the same vector becaus if search and we compar the distanc to our queri vector and all these context and we have like five that ar in the exact same posit we can return all of those So we want to do that So we need to remov duplic OK so loop through the whole data set check if context is alreadi within thi uniqu context list that initi If it we add it to that list And we also add the ID So rememb the question ID context ID wa share just us that ID thi time not us the uniqu context ID that we creat befor So we loop through and then obvious when it see that context again not go to add that differ ID to the uniqu ID list And then we can filter us the Hug & Face data set librari here to onli keep the ID or the row which have an ID from the uniqu ID list So with that we end up with just on of each context You can see here we onli get twelv hundr row out of that which is much less than the full squad valid set Now what we want to do is encod those context and the context vector So we us our model that initi and we us the encod method Now we also convert that vector So thi I think output a PyTorch tensor We convert into a list becaus we ar go to be send all thi to our Pinecon Vector Databas index through an API And for that we want to have them in a list format So we encod the context and now we can see OK we have all the other featur that we had befor Now we also have the encod featur So we move on to the vector databas Now how do we us thi vector databas We first need to instal the Pinecon client here So again us the Pinecon Vector Databas So we come down and we can import Pinecon So thi will just import the wai that we interact with our Pinecon Vector Databas and we then initi a connect So for thi you do need a free API kei So free You need to pai anyth for that So you just go to apppineconeio and you creat an account And then with the API kei that given you enter in here So just in there Pinecon init API kei And you also set your cloud environ So US West GCP And then what we can do onc initi that connect is we creat a new index So to creat that index we onli need thi littl chunk code We need thi dot creat index method And we specifi the name of the index go to just call it squad index You call it whatev you want And the dimension of our vector as well So you can see here And you can also defin the metric Now you could also us Euclidean distanc but we ar go to stick with the default which is cosin And yeah all you need to you onli need to run thi But at the same time becaus I have multipl index and re-run thi code and test it I ad thi in So just sai if squad index is not alreadi exist So check that it is not alreadi run that and then creat it And then we connect So specif to our index run just our pinecon databas as a whole We connect So we us pinecon index and specifi the index that like to connect to And then from now on we just us thi index object for everyth do So here we ar just prepar our data to insert into pinecon which is just upload to our pinecon index So when we ar upload data into pinecon we have three compon We need all of them but go through them So we have a list And within that list we have these two board In there we have the ID of the vector or the entri record whatev And then you have your encod or the vector itself And then option you need to includ thi so you can remov it if you want You have a dictionari of metadata So thi is just kei valu pair that you can us for thing like metadata filter Or if you want to return a particular piec of data with your search you can includ that in there as well which is why us it for here You need to do thi You could store the text local But just do it like thi becaus a littl bit easier for thi littl script And with that we just upsert So we just run thi index upsert do it in batch You can probabl increas the batch size here but just stick with that And it is pretti quick anywai So second is no problem So with that our index so our vector databas and our retriev ar readi So we can actual begin queri and ask a question and return a relev context OK so on to anoth notebook for thi So again go to initi the retriev model and go to input pangcon initi the connect to the index to the squad index again Now thi is pretti straightforward fortun Now at the end All we do is we take a question So go to sai when were the Norman in Normandi And we just write thi So model and code includ our queri within a list here becaus for exampl if you had multipl queri you might have that in a list someth els up here And in that case you would not have to add these squar bracket But in thi case we just have a singl queri And we creat a PyTorch tensor which is our queri vector and we convert it into a list becaus go to be send it through the pangcon API again And what do here is queri So thi is go to the pangcon databas queri sai I want to find the most similar vector or context vector to thi queri vector or question vector I want to return just the top two context that you find And I want to includ a metadata in that respons becaus in the metadata I includ the text So we can come down here and we see OK thi first exampl actual what we want So in here what we ask when were the Norman in Normandi Thi actual answer that question But then the second on so if I open thi up in the text editor come down here we have the second question So the Norman were the peopl who in the and centuri gave their name Normandi So we can assum that thei were probabl in Normandi in that time So when thei were there So we get the correct context in second posit for thi on We also get the score a good score high And then we have anoth question here So got three question go through each on of those pretti quickli How mani output ar expect to reach input in a function problem And we do actual return the correct answer for that on straight awai So a function problem is a comput problem where a singl output is expect for everi input So our specif answer there OK we get a realli high score for that So pretti confid that thi is correct And then a lower score for the other on where the actual question is not answer So on final on here So I chang the word a littl bit for thi on becaus I want to be just do a keyword search So I put who us Islamic Lombard etc construct techniqu in the Mediterranean And I modifi that a littl bit and we do actual return the correct answer straight awai Confid I actual compar it to the other on which ar much lower pretti good a good separ there which is what we want to be look for So that is a I know pretti long a lot of compon and move part there But the open domain question and answer pipelin or at least the vector databas retriev compon of that also had a look at how we can fine tune our own retriev So we cover quit a lot So I mean with all of that readi to just go ahead and implement what I think ar probabl two most crucial compon in open domain question answer If you have a good vector databas and good retriev model you ar go to be return not good context to your reader model And if you have good context for your reader model your reader model go to give you anyth good So these two ar probabl the most import part If your reader model is rubbish mayb it give you a kind of weird span or answer but at least got a good context to go off of So your user ar at least get some relev inform Now I think on of the coolest thing about open domain question answer is just how wide applic it is Basic ani compani across so mani industri across the world can us thi if thei have unstructur data that thei need to essenti open the door to for their staff or their user If you want to get data or inform to someon which is a big part of most job If you want to get data inform to someon more effect and us a more natur form of search and question answer thi is probabl applic to you and mayb your compani 
Adding a tutorial option - Tkinter tutorial Python 3.4 part 19
Hello everybodi and welcom to the part of our Python with Tkinter program tutori video where actual build a full-siz applic So where we left off is we were basic ad all the menu item kind of creat the backend structur to how those will work Some of them ar actual do what their actual job is some of them still need some build up to actual realiz what ha been done So now go to add the last thing which is the tutori option just go to kind of show when you go to help tutori not go to actual write the tutori becaus the program complet yet so not much point to write a full tutori when that might not be the best tutori for the end result But anywai just go to show how we might do a tutori So go ahead and get start So go to actual defin tutori Let me just cut that and put that up here So tutori no longer will pass got a coupl of thing We ar go to go ahead and defin page two as Actualli I think on thi try to think here Mayb Okai becaus go to start Tut So I realli quit think thi through Becaus thi on in my actual code I us that leav mini Well chang that mayb at the veri end So for now sai leav mini what and then whatdestroi So then defin page two will be leav mini So in theori I think what I could do is just like tutdestroi and then we could do equal tkTK and in fact let me just comment thi out We might not need that after all So the load up tutori will load a window and then it will progress into page two which will redefin itself tktk And then do defin page three like thi and then again do and basic you would continu do thi all the wai to as deep as you want it to go but for now just go to three page And thi will basic forc a specif success You might not want a specif success You might actual want your tutori to have like a navig bar to it and peopl can pick variou option alwai been a huge fan of like click on a tutori and have them walk me through the applic on step at a time and be it alwai found thi to be the most benefici to me You might chang your mind and if you do feel free to write your own littl tutori that you think is superior and if good enough share the code with me and let the user decid if thei want to walk through or more of a navig You know you can alwai have two tutori So page three and then we defin tutori three So for exampl would equal tktk and basic what happen is around so like we actual defin the tutori page yet and we will It will be like down here I point to the screen You see the screen be down here and then page definit will have page three and whenev readi for page three and becaus like basic what have is have page two code here and then the button that you click next the command for next will be page three which destroi page two load up page three with whatev data is in page three how it work So what do is put some code here now So equal tktk underscor titl call thi part three and sai label equal ttkcapit label text will equal part three and then the font that we us go to us just the norm underscor font like that Good enough And then go to do labelpack as usual with our label we do side top fill x and patti Why not on nine D ten Then go to have equal ttkbutton with a capit B that button goe to text is sinc thi is the last on go to sai the text is done and then the command itself will just be Easi enough Then go to do and then loop Okai so page three but now we have to defin page two So thi is what will run when the user click the same kind of structur button as here and run tut instead of run it run page three which just so happen to be the first thing that page three doe is destroi page two and then it run page three So now that defin page three we need to defin page two and page two will basic be exactli the same as page three so go to go ahead and just copi thi and then put it here past and highlight thi section hold control open bracket that should shift it over if that work for you sorri and now instead of thi is not part three part two not two part two not done next and then not actual tut actual no not even ani destroi it would be the command is page three page three pack and then not main loop tut I mean not main loop main loop okai and now we defin the main tutori and so so for exampl just write thi on ourselv so sai tut and then that equal tktk and then tutwm underscor titl titl equal tutori and then gonna have label equal cdklabel and as go to be to tut the text will be what do you need help with and the font will be norm underscor all cap font okai now label dot pack as usual our pack ar alwai the same so go ahead and just copi thi right here not gonna write that again past good and now where gonna put in some button so and thi is what I wa talk about befor you actual have to have a walkthrough necessarili like the user might just have a simpl question so like for exampl we could have a equal tdkbutton the button goe to tut and the text of that button equal overview of the applic so thi is the on gonna walk them through step by step in a veri linear progress methodolog that is what we basic code up in thi window here that point to that you see overview of the applic and then command if thei do that would be page okai and thi function which immedi destroi the tutori start up a new window load up page just in case we call it and then run page and you know in theori give a tutori so or the second part of our tutori so for button on then just sai easi enough so we have to type everyth all again just copi that actual that need to be empti paramet there so copi thi and then past twice past past now gonna talk about and thi will be so for what do we want button to do well button mayb the user want just to know how to trade so how do I trade with thi client okai the command here gonna sai for now just lambda colon pop up messag not yet complet and then thi other on will be the same thing with the command not yet complet so copi that past so both you know command not yet complet and then so someon might ask how do I trade with thi client and then someon els might ask a popular question that peopl want to know is is with indic question help you know how to indic work that sort of thing anoth on is how do I get an API you know how do I acquir API inform becaus to trade on the variou exchang you have to connect to the API on that exchang so peopl might ask that question and then final when all said and done howev gonna do touch dot main loop empti parm and that that is our tutori function so now well you could save and run that hopefulli without error see now we have help tutori thei click on a tutori thi littl window come up what do you need help with thei might click how do I trade with thi client not yet complet not yet complet overview the applic howev thei click that new window pop up part two we pack the button good job okai on second go pack that button oh actual we got a traceback ah see if I can figur it out befor we get there but not see it oh here it is our traceback right here figur it out gui so we pack the reason the button even show up either the button wa pack to tut three we do tut two but we still might see if I can figur out if we have anoth traceback somewher not see why we would or anoth error anoth traceback anywai have to watch for traceback help tutori overview part two next part three awesom done okai that work no error besid the syntax that counter nonsens awesom so um but thi word is up here by the wai is just that we have thi these default valu a coupl of thing that we could do to like get awai from thi but just gonna leav them there for now not like a not a world-stop error as you can see the applic still run realli just more of a syntact problem guido would be veri unhappi with me so anywai the tutori is done now I mean not thi is the back end so thi would be simpl question that peopl might have thi would be like a walkthrough tutori so thei click on that now learn about on part there it sai you know click on top indic add RSI click awesom ad a top indic okai so part two then thei hit next becaus done now you can add bottom indic click on bottom indic back and so on okai so you can do that all the wai through realli walk the client through how the walk the client how the client work anywai so that so now got all of the menu option that I initi want us to cover and now we need to start build those menu option so a lot of them ar fairli easi but well some of them ar fairli easi the rest of most of these ar go to requir some pretti fanci code and at thi point thi is where thi program is go to heavili veer off into weigh more so into the actual purpos of thi client so up at thi point realli built a backbon that realli could be support by a huge arrai of client but now actual go to be realli build thi client up for thi veri specif purpos of work with Bitcoin trade and all that the main thing that I would sai is still pertin to anybodi els is what end up do with trade especi manual trade what manual trade is go to do is go to open up anoth window so be anoth window but still part of the applic itself so have two window mostli becaus peopl mai want to put you know their trade window over here the main graph window over here you should let them kind of orient where everyth is so and that wa actual someth that peopl kind of vote on on reddit so the wai go to do it so anywai so that what you gui have to look forward to go to start work on get thi graph to connect or thi client to connect to the sea of BTC for the daili data then work on the open ILO close actual ad the indic that kind of stuff and then after that trade that kind of stuff also paus resum actual work at the moment anywai that kind of stuff so realli go to actual start pull thi applic togeth and make someth interest out of it now so anywai got a nice back end hopefulli you gui ar enjoi I wa realli enjoi do thi realli the seri and actual work on thi applic I realli know if you could do someth like thi in Python successfulli so pretti confid we can major hurdl I think now in the futur is actual thread thi applic but I know with certainti possibl I just have not done it so be a hurdl anywai if you gui have ani question or comment pleas feel free to leav them below otherwis you know the deal as alwai thank for watch thank for all the support subscript and actual thank you for donat as well I have receiv start receiv donat someon actual mail me a check the other dai for thi specif seri so thank you so much for those so anywai stai tune the next video as alwai thank for watch these ar all the sport subscript till next 
Spurious normativity enhances learning of compliance and enforcement behavior in artificial agents
Why do social norm exist And why ar some of them realli realli meaning And why do some of them make no sens at all Like why am I not allow to wear thi hat right here to a funer Okai it might upset some peopl but why There is no benefit no direct welfar impact to societi with me wear thi or not wear thi or wear someth els on my head Thi is a question that go to investig with paper And ye that ha no inher relationship with machin learn But as see we can tackl thi question or at least a part of the question we can give some evid as to why these call silli rule might exist us machin learn specif deep reinforc learn So in thi paper peopl from differ area of expertis came togeth to sai can we build a comput model of societi Can we build a littl world of agent have them do some behavior give them some reward for certain thing and then we just observ what thei do And by observ we can make some conclus about huh thi could be an explan for a societ phenomenon that we see So I like thi paper becaus interdisciplinari It us deep reinforc learn specif multi agent reinforc learn in order to answer question about societi And it is a littl bit out of the box which I like So the video is structur I first do a review of the paper by myself And then go to talk to the author about the paper Thi is on of the last video where I record the interview befor I did the review But for thi paper it wa actual super help becaus a noob at thi field I know what talk about when it come to societi and research in sociolog question So it wa veri help to have the author talk to me about the paper But we just talk about the paper we talk about mani mani more thing And I highli invit you to watch the interview becaus realli interest We talk about norm and societ system of norm and hypothes and what you have to pai attent to when you do research like thi and what work and what and what it mean So pleas let me know if you like paper like thi that ar mayb a bit more distant from what we usual do And if you do then pleas let me know what other kind of paper and what other area exist where ML and specif reinforc learn or ani kind of machin learn ar us to investig question in other field Alright go to leav it at that And now just do like a quick green screenshot becaus I know peopl ar go to make emoji out of my face with thi hat on so And that Cheer Hello there Todai go to look at spuriou norm enhanc learn of complianc and enforc behavior in artifici agent by Raphael Koester Dylan Hatfield-Manel Richard Everett Laura Weiding Gillian K Hadfield and Joel Z Lebo Thi paper present a comput model like a reinforc learn approach to research societi to research the phenomenon of what thei call silli rule So the question is our societi ha a bunch of norm of what you should do and do And these norm ar known by the peopl and thei ar enforc by the peopl be shame if you follow the norm A lot of those norm ar realli good like wash your hand after you us the toilet But there ar a lot of norm that ar also just arbitrari like what kind of hairstyl is good and bad or accept or not accept what word ar rude and thing like thi And these ar call silli rule And the question is why do these exist Now thi is not a question of machin learn Howev thi paper appli deep reinforc learn in order to give some evid to why these rule can exist So I like the mixtur here of sort of us reinforc learn as a tool to investig these mechan By us a comput model you can break down a lot of thing Usualli if thi were a psycholog paper peopl would go into a lab thei would recruit peopl and then thei would try to design an experi around these norm and so on And cool and all But if you us a comput model you can answer differ question you can control for differ variabl and so on So veri attract to us reinforc learn for that So go to look at what thi paper sai right here not as much into the RL part becaus that is fairli straightforward But just what it doe and what it sai And like just to show you mayb a littl bit becaus I thought it wa pretti cool that thi is yet anoth applic of machin learn and specif reinforc learn that enabl progress in a differ field So I hope you you enjoi thi Yeah thei thei introduc the paper by sai there ar a lot of norm Someth that differenti human from other anim societi is thi presenc is thi presenc of norm And some of mani of these norm sai gener direct benefit for individu and group well be like you know reciproc share of reward what you should eat what you eat and so on Veri often these rule have some sort of a some sort of a benefit to societi Thei sai but howev the norm landscap is also popul by mani norm that appear essenti arbitrari and without direct materi consequ And not necessarili fight about thi like peopl can alwai sai well but thi rule mai have some some us But just for now assum that there exist norm that realli could be differ and it would make not a differ in total welfar or at least the direct differ right The paper here argu that there is an indirect differ The paper argu that by introduc these silli rule the indirect benefit ar that agent learn the enforc behavior of the rule more clearli and therefor ar better at enforc the import rule But get to that in just a second So here ar some of the exampl of silli rule that thei mention men ar expect to wear pant not skirt which in some societi is the case and other right There ar word or hand gestur that should not be us in polit compani There ar rule about how on style of hair or what on wear on head and so on So thei call these silli rule Silli rule mean essenti a norm that is in societi is veri you know taken serious but is essenti arbitrari Thei sai meaning and enforc But thei have no direct first order impact on welfar So why do thei exist There ar some hypothes thei list some here Thei sai for exampl silli rule mai remain stabl by virtu of their incorpor into larger norm system that also includ import rule which essenti mean that the silli rule thei make sens if thei ar part of a bigger system that also contain the import which mean the us rule And so the hypothesi here is that the addit of the silli rule into a societi somehow help the societi to compli more broadli or more or more or better or more accur with the import rule So the addit might be some might be a benefit in the total in the total total benefit like total setup of the system In thi paper thei sai we describ a mechan through which silli rule can benefit a societi Our argument is base on the dynam of learn in a group that lack a priori knowledg of which of the rule ar truli import So thei there is a group a societi there ar a bunch of norm alreadi present And a priori no on can tell which on of those ar import and which on becaus if thei could tell thei could just sai well that on is not import which is happen kind of with the scientif method right We know that some thing as import and with time peopl stop do them But initi you know there no wai of know And what thei investig import that thei sai thei describ a mechan right Thei necessarili sai thi is how societi work right Becaus societi is wai more complex But thei do describ on possibl on mechan on reason why thei could why these silli rule could exist And thei show that thi mechan if you implement thi in a mini societi will lead to a total welfar benefit Their explan is the follow The skill involv in third parti norm enforc readili transfer from norm to norm while the skill involv in complianc ar norm specif What that mean is essenti for everi norm you have to learn how to follow that norm So these ar the skill involv in complianc thei ar norm specif If you know a food I eat then I have to learn to avoid that food And then if there is some sort of like a wai like pleas share if you have enough like a norm I have to learn how to do that Their claim is that for mani norm the skill to behav in accord to the norm ar veri specif to the norm Howev the enforc thi enforc skill thei transfer from norm to norm So the enforc skill for exampl shame someon if thei follow a norm veri similar from norm to norm whether thei follow the hygien norm or the interact norm or the food norm or the hairstyl norm is alwai the same to shame someon into complianc or to I know deduct from their social credit score or someth like thi So thei argu that the skill of enforc norm transfer while the skill of follow norm transfer as much And therefor thei sai the silli rule mai provid greater opportun to practic third parti norm enforc And through that the third parti will also becom better at enforc the true the us norm So the addit of silli rule might simpli make it easier for peopl to learn to shame other into submiss And by that thei will be more effect at shame them when it come to the good norm which obvious thei know So just go to shame for all the norm But overal it is posit in welfar So what thei do is thei have thi environ right here you can see the environ right here So up on up here is a schemat of the environ But thi is kind of the the represent thei ar go to have a map which is a map you can see that right here the map And sorri on thi map you have agent So an agent right here sort of a littl person walk around the person can walk around so thei can walk up left right and so on Everi person see a littl window around themselv Thei see happen around there ar sort of obstacl there But there ar also these berri and the berri I know if you can see them on the screen but the berri thi is a berri these ar two berri right here thei come in differ color So the goal is to move around and collect these berri everi berri thei get thei get some sort of point You know thei collect them the reward there ar enough berri so that there is no meaning competit between agent There is on other thing thei can do and zap someon thei call it even zap So in thi case go to guess someth like thi thi agent right here is zap thi agent down here and the yellow thing is a punish punish beam Essential that just mean that the agent can zap anoth agent which will caus the zap agent to lose a bunch of point and the zap agent also to lose more point The onli the onli addit now come with the poison berri So sometim some of the berri ar poison and there will be a color select for which berri is poison For exampl call all the green berri here poison When an agent pick up a poison berri thei ar thei see thei see it themselv but thei will be poison And after thei pick up a poison berri step later thei will start to lose health or I think thei will just thei will not gain as much from eat other berri it So a veri delai veri slow punish for eat poison berri That take the agent a long time to learn that Howev if now if you get zap while poison that give the zapper a benefit So call thi person Alice here and thi person Bob If Alice zap Bob and Bob is fine then Alice lose some point and Bob lose some some point Howev if Bob is poison then Alice gain a bunch of point for zap Bob So Bob is poison lose point and Alice gain point by zap Bob I do think so the zap cure Bob I think So on zap will actual cure Bob but Bob lose a lot of a lot of point Hei Janek from the futur I made a small mistak right here in that I claim that zap cure the poison which it doe not The idea is that zap remov the mark So when a player eat a poison berri in thi normal rule condit thei becom mark and zap cure the mark If you zap a mark player you get point but zap remov the mark It doe not cure the poison The poison is still activ The idea is obvious that the player learn to avoid the poison in the first place becaus thei want to get mark becaus thei want to get zap And now in the silli rule condit also a second berri activ the mark but not a poison berri And thi you would expect that more noisi and therefor learn is more difficult but it turn out under the silli rule condit learn is actual more effici And kind of the point of the paper So again the zap cure the poison It just remov the mark in whatev wai that mark happen to be on the player in the first place Back to the video Yeah on last thing and that you can see here in the mark So when an agent is poison so when thei after eaten the poison berri thei becom mark which mean that all the other player will see that thei ar poison Now thi is the setup What you can pretti quickli see so no rule is here We have berri and we have poison berri that give you a delai punish Then thi is what I just describ with call the import rule condit which is that if you eat a poison berri you becom mark And then if a third parti anoth player see that thei can zap you and thei gain a bunch of point So you can see that pretti quickli what is go to happen is that the agent thei learn to eat berri but then pretti quickli thei learn to spot the mark agent and thei zap them And then after that also veri quickli the other agent will learn to avoid the green berri becaus thei realiz wait everi time I get a green berri I get zap later And how the agent avoid learn to avoid the green berri Note we have to clarifi some thing Thi paper about how the norm of not eat the green berri come to be becaus obvious kind of like God given right here The mark is done by the environ The reward ar clearli set up such that peopl learn to avoid the green berri not the issu right here The question that the paper ha is how quickli can the agent learn to enforc that norm So how quickli do thei catch on zap other Right And what what doe the overal welfar So the norm itself is set by the environ or by the design of the experi We ar not try to learn to avoid the green berri be like through the effect of poison But we simpli directli give reward for zap the mark agent And that mean we thei thei us X Machina well X Nihilo What what mean just like we command a norm onto the system and we see how the agent react So that is obvious happen here is not a secret right We can all imagin that By the wai the agent thei us an actor critic Thei us a simpl ConvNet and an actor critic framework to learn right here What I find interest is that there ar neural network So the system keep like neural network that ar initi with the same weight but differ neural network and eight of the go to just select three or four right here but imagin eight of eight of the ar then each episod drawn to compet in in the ring Okai Thei compet for a thousand time step Then thei get thei get their learn updat thei get put back And then for the next thing eight other ar drawn which I found pretti pretti interest a wai to sort of get divers into the system Now what doe that what doe that have to do with silli rule So far built up an environ We we forc a norm onto it by give reward for punish these mark agent And discov that agent learn pretti quickli to enforc that norm which in turn make all the agent avoid the poison berri as a consequ of be punish by the norm Now we introduc thi silli rule So the silli rule mean that there ar poison berri which ar these on but there ar also other berri that we will call taboo berri the taboo berri just fine just you know fine healthi You can eat them You get a bunch of point for eat them fine Howev if you eat the taboo berri you will also becom mark just like the poison berri eater Right So these ar indistinguish mark and therefor the agent that learn to gain point by zap the poison berri will also gain point by zap the on that at the taboo berri even wors is that thei also get reward for zap the taboo berri eater So no differ in the reward for zap that you get if you zap a poison berri eater or a taboo berri eater You just whenev you zap a mark player you get some point Again not about how the agent learn to avoid the poison berri how thei react to given norm Right So again we enforc the norm of you should eat neither the poison berri nor the taboo berri Of cours the agent know which on is the poison on Thei just know thei get zap after eat either the pink or the green berri So how doe that How doe that go sort of the question of thi paper introduc a silli rule which on a surfac serv no purpos The green make the green berri taboo serv no purpos other than just a rule and you get punish for not follow it It even decreas the overal welfar a littl bit becaus now you want to eat the green berri anymor which mean that you get as mani point The question is can the introduct of the silli rule get you an overal benefit as a societi the question Okai So go on a littl bit Thei sai our model allow us to separ the learn of enforc and complianc behavior from the learn of the norm content itself what I repeatedli emphas Becaus I had a lot of troubl when read thi paper to realli get thi Thei want to thei want to thei sai here we design an experi in which norm content wa fix in advanc by the experiment name which berri ar taboo The question is how do thei react to it So thi is a brief recap If a player break a taboo thei chang color in the observ of other agent view their transgress thei becom mark If a player is mark other player can collect a reward by punish them Thi creat an incent for player to learn to punish rule violat and thu for player to learn not to violat the rule And these ar the result we show that individu achiev higher overal welfar in a world where eat the poison berri is taboo condit on Thi is thi is clear Thi is logic We take a delai punish for eat poison And we essenti bring it to the present by have peopl zap the poison peopl and them learn to avoid it Howev the main result sorri thei sai even with the cost of enforc overal group welfar is higher with a norm than without We then show our main result that the valu of the norm order is higher if the set of norm in thi regim includ not onli import rule such as the rule against eat poison berri but also silli rule which make the eat of a harmless berri taboo and bring about the same third parti punish So thei show there is a situat right in which you can gain by introduc such silli rule becaus enforc skill ar learn faster just quickli look at the agent architectur if into machin learn or RL or so thi should be rather familiar to you So the agent thei see raw pixel up here a neural network a CNN follow by an MLP There is an actor critic So there is a valu function and there is a polici function actor critic veri basic actor critic algorithm Thi is obvious a veri easi environ for reinforc learn And that make it ideal to us multi agent RL here to to to gain some insight As he said we have agent eight out of plai in environ in parallel and thei get the replai buffer and thei updat thi those weight All right Yeah mention these thing mention these thing Now look at the result So first of all look at fraction of time spent poison Like how So here is time step train So thi is over the cours of train right So what fraction of the time do the agent spend Doe an averag agent spend poison If there is no rule you can see that there is a constant fraction of the time agent spend poison Thei essenti over the cours of thi train thei learn realli to avoid the poison berri And therefor yeah becaus the reward is just too delai I guess the RL algorithm also too power But you can see that there is a clear differ between the import rule and the silli rule So import rule mean there is onli on rule eat the poison berri and silli rule That mean that there is in addit thi silli rule So the agent here quickli thei spend less total time poison And the question is is why So look at some other effect that the introduct of the silli rule have total taboo berri eaten You can see that at the begin about doubl the amount of taboo berri ar eaten under the silli rule than under the just import rule which make sens becaus twice as mani berri ar taboo that so eat twice as mani of them in the same time But you can see that there is a crossov thi decreas and actual crossov So after a while less taboo berri ar eaten than in the import rule set even though there ar more taboo berri right So somehow these agent learn faster to avoid the taboo berri total punish Now obvious again at the begin there ar doubl as mani taboo berri so doubl as mani mark player So thei go the number of punish goe up pretti quickli And then a crossov point where after a while there is less punish go on than in the import rule So these societi thei learn faster And I think the point you can see that at the end often sort of the same result the same outcom but in thi intermedi stage and rememb societi is alwai in flux kind of So on can argu that veri often we ar at all time in sort of thi intermedi stage So in thi intermedi stage actual an overal benefit Fraction of time spent mark goe down as well pretti quickli obvious becaus peopl ar more mark and collect return So here is the actual result If you have no rule at all collect return goe up at the begin actual the highest but then flat line right becaus peopl keep get poison and that hurt If you howev us thi import rule thing then at the begin not as great becaus if you punish these the reward ar structur such that if you punish you decreas the total welfar even though you as an agent gain some point the total number of point in societi decreas as a result of punish So you just punish more and more and more and expect to expect the collect return to grow So yet still becaus agent learn to avoid the poison berri through punish So at the begin lot of punish why the reward the collect return is lower but then thei learn And as thei learn thei learn to avoid the poison berri then thei need to punish as much anymor right And then the reward goe higher than if you had no rule at all Most interestingli howev in the case of the addit of the silli rule you can see that at the begin there is a decreas in collect return as peopl punish around like thei punish each other to death Yet yet veri quickli thi goe up and actual becom the highest collect return there is And you can see in thi intermedi period right here there is clear benefit to have these silli rule around becaus the societi is much quicker and much better at learn to avoid the poison berri becaus becaus and you can see from the time seri right here becaus thei learn much more quickli to punish to punish peopl who eat the wrong berri not onli the poison but also the silli on And becaus much quicker at punish the agent have more opportun to learn to avoid these berri And what give you the higher return Thei do thei do investig what these agent have learn Thei sai psycholog experi with human particip address the issu of learn what peopl have learn individu by isol specif mechan and test in these control condit such as reaction to particular stimuli thei want to do the same thing computation So thei take these agent from their train run thei put them in infer mode and thei give them like a littl environ like thi So thei start apart from the berri and the episod end on contact with the berri So then there you can give them a berri and see if thei eat it or if thei eat it So if you have no rule at all if you have thi mark rule or anyth like thi here again time step train But rememb we train the agent on thi task we train it on the origin task then at certain checkpoint we take it out we put it in littl lab and we see what happen Also the y axi here is invert So is down here which mean time step If the line is here it mean the agent ha not eaten the berri If the line is up here it or like somewher up here it mean the agent ha immedi eaten the berri You can see that in if you have no rule agent thei just eat the berri matter matter if poison or not right The pink is poison It make a littl bit of a differ but not not realli thei just eat it If you add the import rule thei quickli learn to avoid the poison berri You can see that right here If you add the silli rule thei also learn to avoid not onli the poison berri but also the taboo berri Thei also in fact learn to avoid the healthi berri a littl bit more but thi come back over time And there is a there is a bit of an unlearn right here And I do ask that in the interview thei specif highlight So these ar differ berri now Just isol the time when thei give the agent a poison berri you can see that the reaction to the poison berri is much much bigger If you have if you ar in the condit that contain the silli rule compar to if in the condit that contain the silli rule in thi intermedi regim right here And also you know the punish punish is wai quicker So thei measur how long it take you to punish wai quicker when you have the silli rule And yeah so the essenti the the evid that thei sai look these agent thei learn the skill of punish thei learn the skill of run after someon who is mark and therefor in punish them And that give the agent the opportun to learn to avoid poison or mark berri altogeth And becaus there is more punish becaus the agent ar better at punish more earli on thei learn to more quickli avoid the poison berri So thei the overal argument again is that the skill of punish a ar transfer between task And the addit of a silli rule even though it bring some neg cumul like neg welfar becaus a rule you need to follow like you incur some cost it could still be total benefit overal becaus the introduct of the rule just train peopl in punish other for not follow the rule and therefor train peopl in follow rule and therefor train peopl in follow the import rule Rememb in thi societi peopl have know the assumpt is thei know which of the rule ar benefici and which on So these were in the discuss Now thei sai from the perspect of an agent learn the skill necessari to effect enforc their norm the addit violat constitut addit opportun for practic and thu promot a faster rate of improv in their command of the mechan or sorri of the mechan of third parti punish Now obvious thi go forev right You just add silli rule until you know like until the world is just made of rule and expect well alwai go to have much higher welfar But there is a regim where that is the case And we might as well live in that regim in our societi Thei sai enforc and complianc ar asymmetr in the sens that the former is a skill that mai be appli without modif to ani norm enforc Sinc mani of the sub behavior involv in third parti punish ar direct toward the violat for exampl face them not toward the event of the violat itself Thu thei ar transfer skill gener applic to ani norm And ye I get it If you sai for exampl avoid food is also transfer and so on Sure sure But I think thi sentenc here that a lot of punish behavior ar direct toward the violat and not toward the event of the violat itself that it make sens that these skill ar more transfer The interpret of our kei result is that the role of silli rule in human norm system mai in part be to help train a abil to compli with import rule And that is the result The paper goe into more detail obvious in all of these result in the setup in why import and so on But leav it at that for now I hope you you gain some insight into how reinforc learn can help other field to get some insight by model sort of these comput littl societi and just introduc aspect of the real world And then just see how that pan out Like it clear at all from the begin that the introduct of the silli rule here would bring thi improv in in sort of the intermedi timefram And just realli interest And kind of a differ wai of approach the question of why doe silli rule exist in societi Question like these a differ wai of approach them than just put some human in a lab which ha it own problem right So I think thi just gather some evid and pretti cool And an opportun for interdisciplinari research which I like And I hope thi wa fun to you as well And see you around Bye bye Hello everyon Todai I have with me here three of the author of the paper about spuriou norm enhanc learn of complianc and enforc behavior in artifici agent Gillian Hadfield Joel Lebo and Raphael Koester You ar an assembl of peopl with wai differ background that have somehow come togeth and focus on a veri cool intersect between machin learn and social scienc Welcom to the channel And yeah welcom Thank for have us Great to be here So I mean the first thing first thing first in machin learn had these trend of just make like click baiti titl I feel your field should pick that up Becaus you know a titl like thi is like that is an instant desk reject you got it you got to have like a littl acronym like spell or someth like just four letter or so and then ani or a question like but yeah a pretti cool visit Here We have we did have a somewhat more intrigu titl that then the journal told us to chang Yeah we did have silli rule in the titl for thi for thi reason And thei were nervou about that Okai there still some some veneer of profession in other field of scienc not not in our Yeah I wa I wa veri veri happi to see thi paper becaus it connect someth that I know to someth that I know And I think you know us machin learner were sort of alwai in the same area And thi goe a littl bit outsid of my comfort zone So I thought it wa pretti cool How How did you get like the idea of write someth like thi of connect these field Like where doe it come from I can start with how I came to it So my background is in comput neurosci what I did my PhD in And and when I came to DeepMind I wa think about how we built a artifici gener intellig and read lot of thing about human intellig and realiz that intellig realli in the brain So my whole PhD on neurosci wa mayb not as help as I thought it would be But intellig is actual a collect phenomenon that is more support by by how societi work and how how we cooper with each other and learn from each other and thing like that And so sinc then been try to build human like AGI in a wai that is more like try to make a societi of AGI And thi wa on on piec of work that came out of that after meet Jillian Yeah mayb I can sai a littl bit So a social scientist I build these system I think about and studi how human norm system work Right Those ar our system of norm and our system of rule And veri interest in that from a system point of view What ar the attribut of the system that make them stabl and adapt and contribut to human progress and evolut And so been think about build you know work on on those kind of model these sort of econom model tool And team at DeepMind had produc some paper studi some veri standard problem in the econom literatur on like tragedi of the common and show how thei could us sort of those multi-ag reinforc learn setup to studi tragedi of the common which is sort of you know econ I saw those paper got veri excit and said oh but we could realli dramat you know increas the sort of the social scienc compon of thi work And I had been work with Dylan Hadfield-Manel also on thi paper on thi concept of silli rule And so we so actual I think I track you down Joel and start a convers number of year ago And we spoke afterward Ye right Oh right I came and gave a talk at DeepMind And yeah so I wa veri excit to be connect up these two world And then you need someon to actual do the work And then where where I came in I think I have much to add to stori So my background is also in cognit neurosci and psycholog And I work on topic that ar sort of on the intersect of decis make and memori in human and in AI So social cognit as well as learn from other or how group behav is similar And also question of behavior econom ar all sort of all in the scope of what realli interest in I think thi is a good exampl of where these thing come togeth Yeah pretti cool So to give the brief introduct to mayb the paper I think mayb for the machin learner valuabl to start with thi on right here So we have thi environ there ar differ agent insid of it I think you alwai have eight agent that take part in an episod the episod can go to up to like step In each step each agent ha the abil to move around The goal is to collect the berri It ha like a like a littl window view around itself of the world And on other action it can like zap someon els right Thei can it can zap punish an agent And get to that in a bit So these berri that ar around you deliber made the berri plenti So no issu of like yeah competit or anyth like thi There ar three condit that you compar and these ar kind of your experiment condit Do you want to mayb sai like if you if you gave the pitch about your own method I think thi thi kind of is the core right here How would you describ it We might want to sai what the purpos wa Yeah sure Experiment condit right From from my perspect on thing that I think follow on from what Julian said a minut ago true we realli did have a bunch of paper that were kind of reproduc econom kind of idea about tragedi of the common and thing like that And and we had a sequenc of those paper And thi wa the first time we were realli try to like contribut back and sai someth actual new not just like a new wai of come to the same kind of result that peopl alreadi had in econom for centuri And and so thi particular area which try to connect with is a field interest cultur evolut and cumul cultur and thing like human uniqu Thei see human as an ultra social speci like critic to the nich that we ar in requir a a cultur nich We learn from each other how our technolog work how our societi ar put togeth And and what make us differ from other primat basic And so within that literatur on thing interest is how is how we cooper and social norm ar on kind of mechan of cooper other like reciproc and thing like that And then within that field anoth question of like we have all kind of social norm some of which seem to be relev to cooper and some of which just seem to be irrelev thing Like we can have a we can moral all kind of behavior like suppos to wear cloth and not suppos to wear a hat in thi circumst or whatev And the question that is like well social norm ar so import for cooper Why ar there all these other social norm that ar like just not do that I mean is you have thi concept of the you have thi concept of the of the silli rule right which is a fantast name And it describ sort of a norm that directli valuabl to anyth that that consid like group fit or even person fit Yet doe thi actual exist Like is there a rule where we can conclus sai thi is a silli rule and not you know we might be miss some hidden advantag Well the point You can never sai that for ani rule realli If insid thi you never know whether thi is there for some import reason or not But I think thi is a kei thing is sort of place work in the context of the work that get done on try to explain human rule and norm And so we have peopl come at thi mostli from a function point of view Like a solut to a game theori a solut to a coordin challeng or a solut to like a hot dove type problem where go to wast resourc fight over someth that or cooper like Joel wa sai right So most of our work in social scienc ha come at the question of explain norm by sai thei serv thi function purpos But it seem veri clear We have lot and lot of rule where you could sai look noth would be differ from a function point of view if we said you wear bright stripe at a funer instead of black or that you you know stand thi far apart rather than thi far apart just onc you start notic silli rule defin in thi wai as no direct impact on welfar onli impact which is what show is the role those silli rule plai in help to stabil and a system by which peopl can enforc the import rule Right So so I think a a kei thing So it sort of start with the puzzl thi thing that seem to be true of everi human societi you look at food rule right What we eat and donat is often a good exampl veri ton across differ group and commun over time Why do we have them Why ar thei stabl And realli no good explan in literatur So we got realli interest in think about the role thei plai in support what call the norm infrastructur which is what you draw on to enforc the import rule If go to punish peopl for steal your stuff or punish peopl for go back on their contract you need to have coordin and incentiv your commun to enforc rule And what look at is the role of silli rule and help to creat that structur It is a bit like the valu of just have rule for and if you have more rule then be better at follow rule and peopl will be better at enforc rule And just like more rule sort of lead to Becaus more rule ar a transfer skill the enforc part And what you would want to get at right here So your goal is sort of if we train agent and if we introduc like a silli rule like thi thi skill would sort of transfer to benefici rule whenev we actual have benefici rule So in the first context here there ar berri and there ar poison berri If you eat the poison berri some when later kind of not die but just your reward will shrink from eat new berri So it will be like a veri delai thing And in thi case we all know reinforc learn realli good at super long reward You also have a discount factor right So the long reward even matter Like I could even imagin if a berri is close to me and I knew it wa poison be like meh right a hundr step awai who care right just eat it and go back But assum the agent actual want to avoid that And then you have a silli rule and an import rule The silli rule be you can mark or the rule ar you can mark agent right Agent ar mark If you eat a berri that is taboo you get mark So you chang the color in the percept of the other So you yourself see it but you chang color in the view of the other agent And if you ar mark other agent can collect the reward if thei punish you And so what do with these three differ condit is sort of fix what the norm ar the sort of the experi is if you set the norm what ar the effect downstream on the abil of the agent to learn to enforc those norm and to then compli with the underli rule that thei ar repres And in the import rule condit the taboo berri actual coincid with the on that is poison So a realli import rule for your group to have That should if everybodi learn to follow it lead to everybodi avoid get poison In the silli rule condit you still have the import rule But on top of that you also get mark for eat a berri that is fine and actual poison you So the potenti for twice the amount of transgress and then also punish behavior follow that The import thing is you get mark just the same So in the third condit whether you eat a poison berri or the berri fine but just mark as taboo you get mark the same So no distinct And the other collect a reward whether poison or not enough that you ar mark right so that that is how you sort of set these norm in place Becaus I wa I wa sort of like okai the agent either have to figur out which poison like no thei do get a reward as soon as soon as thei zap someon who is mark And now we ar go to see what happen in a littl bit as a result of these experiment condit But my question first is a You have a motiv to punish those who have transgress You have some norm code and you want to you know like those on thei violat it We want to enforc on them our social ethic or whatev The question is a littl bit so there is thi is like a microcosm right Sorri there is a cat right here Thi is a microcosm system And I you know alwai thi in econom alwai that the micro economist versu the macro economist right And thei and thei kind of fight becaus the microeconomist thei come up with their model and their simul and their formula And then the macro economist ar like well if you actual look at the whole world complet differ right Mayb you can get some insight right But alwai thi danger of you know thi enclos system with these veri constrain thing As soon as you introduc someth els it might just chang the entir game Is thi someth that kind of avoid somehow or worri about or not worri about Should I take that on as the economist in the in the crowd So I think a wai in which what do is the same kind of thing that micro economist which I am ar do which is look at you know ideal or schemat set and do theori about that in order to gain insight and gener testabl predict And not try to sai thi is a map of the world exactli as it is sai we can gain insight into what would be the impact of chang that price or that cost or increas competit that kind of thing And so I think what what do here is and we refer to thi as kind of micro foundat which actual lot of macroeconomist ar interest in micro foundat which is is can we do a simul like thi to solv a problem that we do close form with our theoret tool like we would normal do like you know solv for an equilibrium or solv for you know solut to a game theoret problem Thi is allow us to solv a much more complex problem and gain insight And then demonstr thi type of you know got thi hypothesi that said our agent will learn faster and better to both enforc and then therefor compli with rule if a silli rule in the environ So I think of it as kind of similar methodolog to that I think got thi thi relationship to cultur evolut Not exactli on to on We think human start off like onli be abl to recogn pixel in the world But that the idea that thi is someth that evolv over time but not try to kind of model like evolutionari game theori tri to in some wai model what would happen with repeat popul over time So how I think about it methodolog So I think it pai that we now jump to the result a littl bit to to take it ahead befor we discuss sort of the like broader implic or anyth like thi So is it fair Like correct me if wrong I charact I would character your main your main your main result or your main thing you deriv from it that if I impos the taboo on the poison berri through thi mechan of agent get reward zap each other the popul will sort of learn to avoid the poison berri better if than if if thei just get the delai anti reward In addit if I now also introduc anoth taboo berri fine thi silli rule right and the agent can collect even more reward by by zap you would sai thei ar learn the skill of enforc rule which is a generaliz skill And through by becom better at enforc rule sort of faster catch on to the fact that you know I should punish peopl for eat the wrong thing Therefor the whole popul learn to not eat these type of berri faster Is that about in the ballpark an evolut of like the skill or what ha been learn like at first the agent need to learn to even perceiv the world and then effect eat berri that then increas to them actual get poison a lot becaus thei eat the wrong berri a lot And onc that is in place and you actual have a lot of mark agent then it is possibl to learn about the punish and that that you can collect a reward for punish mark agent Once that is in place then you have the opportun to actual learn to avoid the berri you want to avoid becaus avoid the punish But for that you need all of the other agent to have learn to actual discourag thi behavior So thi is sort of the nice progress of that on skill reli on anoth skill have been learn beforehand And the silli rule help exactli in provid more observ and more train for that learn of skill And thi is the sort of result you could onli get with a model that is realli focus on learn of skill Another thing anoth aspect of it is a veri long tempor credit assign problem which is veri difficult for enforc learn in the case where just the poison berri But in the case where be punish for eat that berri then move closer in time the neg thing to the event I think thi evolut you mention is visibl in the graph right So you first have like the total taboo berri eaten it kind of goe up at the begin becaus you get a reward for eat berri then peopl learn to punish other right So that in time you see that spike after the other spike And then the like variou thing happen like the fraction of time spent poison and the fraction of time spent mark thei go down dramat as a consequ of the punish increas And at the end sort of the collect return goe beyond what you would just have So the differ here I guess is the credit assign problem differ There seem to be too much of a differ in the end result Like if you let the game plai out between the just the good rule sai and the silli rule what is like so your claim ar more about the evolut of the thing and somewher in the middl there might be an advantag to have the silli rule Is that Yeah I wa gonna sai I think emphas that about learn these behavior of you know the relationship between what you eat and oh my god somebodi show up and zap me Right learn that and then learn oh I get thi reward if I zap somebodi who is mark So learn those behavior you know onc onc learn in a stabl stabl wai then the benefit of the silli rule is kind of okai accomplish our learn object My own intuit is that the silli rule ar go to help you with robust so that when the environ chang right and thei got to learn someth new so that even though in our environ it thei converg at the end my guess is you then introduc kind of the shock of you know the rain come thi year or a differ in a new part of the world and a differ danger berri then then so I think that that like if you sort of follow on these experiment result you have some more you draw thi conclus that what is the common thing is sort of the mechan of enforc rule The agent thei thei learn thi thi is a transfer skill And by have sort of more taboo around thei learn thi faster What is differ like what differenti thi hypothesi from the hypothesi that agent ar better at avoid some color of berri becaus by introduc you know a new taboo berri I teach the agent that you know thi new berri is also taboo I sai with the same argument that it mai be not the enforc that thei learn in common mayb avoid some color of berri Well sort of the consequ right the complianc part Yeah From their perspect thei see anyth differ until someon ha enforc someth on them Becaus if thei eat a berri that is taboo mark onli in the ey of other thei see themselv And for the silli rule noth happen at all just that thei at the berri and it becam mark in everyon ey But from their perspect noth happen at all So no effect on them in ani wai until the punish come first Okai Yeah the onli wai that thei could ever learn to compli Is there a certain and on of the nice the graph in there to Rafael the the sort of show that it is that sequenc of learn to punish and then learn to avoid get get poison a social equival to get a reward for punish someon who ha transgress a taboo Like if I you know think to myself the progress of thi would be it would be more like if I enforc some taboo then long term that will lead to more group welfar becaus right everyon get keep to the rule we eat less poison berri or we follow rule in gener And there is an aspect of group fit that also reflect on me you chose to directli give me reward if I punish someon for transgress Is thi pure just becaus you want to like hard code these norm Or is there like a social equival to that Yeah take that from on perspect And then I think we can do it from a few differ few differ on here becaus thi ha multipl kind of wai of think about it So the on that you can see it as an intrins motiv agent just ar motiv intrins to punish the transgress of their their their norm that thei have So like some kind of like righteou anger on the part of the agent that just saw thi thi transgress And and then motiv to punish it And a veri kind of natur human emot that we all feel for differ norm like we could have total total differ norm in mind we can come from differ cultur to our place But we might still feel a feel some like thi is a transgress that just wit I think whatev it is on interpret we could have we have sever other thi interest on about mediev Iceland mayb someon could sai Yeah let me let me jump in there So so so the the fact that human have thi capac for that thei have thi practic of third parti punish So that realli is distinct about human and the evolut of speci And a great puzzl Why do human spend resourc punish peopl for you know do you know commit commit harm to other that third parti piec And so got peopl in sai behavior econom who think about altruist punish a littl bit what what what the wai I understand what Joel wa talk about with intrins motiv that you just have a tast for punish We got a whole bunch of in behavior economist who studi sort of like you know peopl will to pai monei to be abl to punish peopl for hurt other peopl But a real a real puzzl in the stori of cultur evolut about where that come from is that second order like we have we have punish for peopl who fail to punish So we do actual have critiqu that sai Hei how come you sai anyth when that person said that harass thing to the other person around the meet tabl right We have reaction to peopl who respond and punish peopl for violat our cloth rule or our dress dress rule or our contract rule right And and in thi anywai a real real puzzl And you know hard code it here Some evolutionari anthropologist model it as a trait of punish like with punish and non punish My own view is that actual that the fundament behavior to try and explain why do we end up with human will to spend person resourc punish on somebodi behalf becaus the secret of our success I wa speci And we do the mediev Iceland exampl what that Oh mani of the light on Ye right So refrain in fact that sort of been around look at it realli is about decentr punish So that the kei thing to know about mani lifelin is thei had lot and lot of rule and thei had no enforc no public enforc no polic no soldier no chief who had ani power Thei just have on individu the law speaker who wa respons for recit all the rule everi year at a big gather and who wa the person you can go and ask is thi allow not allow And that coordin everybodi on be will And thei had veri clear not onli rule but what you could do but also the penalti Like if you did thi you had to give up sheet If you did that you got kick off the island And what you need to do is coordin your commun to to actual implement that punish And what thei did realli veri effect with with zero public enforc apparatu No eventu becom more more effici to have some enforc apparatu But individu enforc the rule is a realli big part of both human histori and even todai realli import Think about mask mandat Think about our pandem rule reli veri heavili on sort of commun enforc and non-enforc So the conclus the gener conclus is introduc a silli rule sort of make group welfar higher or achiev the welfar faster sai by mechan of you know I learn a transfer skill and so on So ad on silli rule good ad two silli rule ad three ad four like at some point you know there must be like a detriment to have you know onli silli rule like how far would thi go out Is on like the optimum Is there some optimum of silli rule Is thi known Can you assess that mayb with with your simul So we specif test thi but I think your intuit is right that there would be an optim number becaus also everi rule introduc costli effect becaus overal someon punish someon els overal destroi a reward So you end up with a net neg So the more punish there is overal wors for the group So the benefit need to be quit larg to overcom all of thi addit punish So I think it would depend on how hard is so first of all how costli ar thei Like if veri cheap then you can get awai with more The other thing is how hard is the thing that try to learn Like if veri difficult to learn the punish behavior and you need lot and lot of addit observ to do so then I think addit rule would help Wherea if veri easi to learn then you bare need ani addit observ and you can just then just stuck with the bill So I think it depend on that I think some sort of invert U shape with like some optim amount See in these graph a littl bit that sometim at the end actual trend revers a littl bit especi in the silli rule case And seen it here and here is also promin in these sort of singl agent test which you do which I realli like you take a singl agent you put it in like a control environ not train just at some point dure train like an eval set But also here you kind of see these sort of revers trend as train progress What happen there Are thei becom like realli good Do thei learn the actual reward of be poison Or go on there Do thei learn to avoid the punish I suspect that what would happen there is some amount of unlearn becaus if you ar veri effect at teach the popul to not get mark and avoid ani like thei effect avoid all the taboo then thi behavior just occur anymor And you just even for you will just forget that ever learn that So I think if thi were to keep run thei might have to at some point relearn it But then the question is if thei actual would relearn it becaus now in a differ thei have competit from differ thing Like mayb veri good at collect berri now So mayb not as interest anymor as even learn about the punish dynam at all becaus the counterweight of the other behavior is differ So I think thi turn into I think a continu learn problem if you just let it run for a veri long time Becaus a covari shift when the behavior of mark agent exist and then be abl to punish their quan Your structur ha a bit of a special thing in it which I found which is that you have differ agent sai differ neural network that you train In everi episod you choos eight of them to compet Wherea sometim or a lot of time in multi-ag reinforc learn I have like on neural network mayb with a bit of random but essenti everi of the multi-ag ha the same weight sai all share Wa there a particular reason why you chose thi specif like the not onli have differ neural network for each agent but also to alwai sort of select subset of them And also have you the follow up is have you discov that thei diverg I would be interest like did on learn to becom like the punish Like okai go to exclus make my reward off of punish other and then other be like nah just go to collect my berri Yeah and I think it wa just for us not share the weight just have individu agent on neural network for agent wa alwai the default for thi line of work And it seem like there wa ani reason to chang it here In particular here for model human human have the same polici as on anoth or anyth like that And as an economist or a social scientist or think about these tool it alwai seem like the share weight just felt like assum a can open right just like assum awai the kei part of the problem which is agent A ha an incent to free ride on the effort of agent B And try to solv the problem of cooper and coordin with individu agent Coordin is much easier right If you make a small gradient chang to your polici in a particular direct but not just you like on agent actual everyon make that same chang at the same moment then for certain problem that can help coordin not all problem I doubt it made a huge differ in thi particular paper though I did not find ani special So I think that thei all that thei develop differ nich but I do think it should be at least possibl So yeah I think on of the reason why I chose it What would be main candid to add here think of thing like like in term of abil of these agent if you want to go further like what would be question adjac question that like to have answer from such a simul and what would need to be ad yeah think of thing like mayb a bit of commun between the agent some signal like I could I could like signal to other that a good punish or someth like thi or that Thi question we can go in a few direct One thing that thi is veri open is where do the norm come from the content Here we just chose like you know thi is a taboo veri thi other on is taboo veri But what we realli want if we want to have a model of cultur evolut is a model where norm themselv can emerg from the gener train gener learn of the agent And so that is on direct that we start to go after thi paper We have anoth follow up paper where we have a wai for the content of the norm to evolv within the system But also not perfect It ha it ha continu learn problem aris becaus if you have a kind of constantli chang the adapt environ for everyon and you can you can easili break reinforc learn that wai So I think the next thing go to have to happen in thi line befor it turn into like a real model of cultur evolut that feel like you can do the kind of thing we want cultur evolut to do is have to have some more effort on the continu learn side Basic make it so that agent can kind of come up with on norm societi come up with on norm and then it can kind of chang and then tip point effect as it chang you can see fad and trend and thing and none of that can realli happen right now until we solv some continu learn issu With respect to you know you said someth you know we have to solv continu learn issu and so on What is like imagin there ar quit a bunch of hyper paramet in thi thing not onli reinforc learn wise like my discount factor blah blah blah but also how mani point do I give to what right I can give you gave four point per berri Like well just a number You give point for like punish someon correctli How sensit ar your find to these thing or how sensit is the whole system to these paramet So I think realli hard to quantifi becaus a lot of the chang would be realli meaning right If you sai make the berri so valuabl that you never care about the poison where you make the poison so weak that you have to worri about it Any of these thing you would expect to make a big differ becaus chang the balanc of all the differ thing that you need to learn about The thing that we tri that I thought wa realli encourag wa that we just re-impl the whole environ and the agent and also tri a differ type of learn agent on it and the result came out veri similar So that kind of made me pretti confid about like the overal observ that if you have thi type of social learn problem where you learn from the observ of how other treat you if you get more of those that help And that can be like a kei compon in like get the overal popul to the goal faster How doe on avoid like confirm bia in these type of research Becaus you probabl have had some sort of idea of what you were go for and you know like a hypothesi to show and like razor is kind of a brutal thing right And there is if you see these result you were like oh yeah thi fit perfectli well with the hypothesi I had and so on So what not like I not that I see anyth wrong here but just wonder if you go into thi with the hypothesi kind of what ar the step on need to do to avoid sort of fall into confirm bia I mean thi kind of thing is about show that a particular mechan exist and is there and what we know is of cours rel to all the other mechan that ar support silli rule in the real world how strong is thi on versu other thing And we could talk about some of the other on as well And no wai you could ever answer that from thi kind of problem I think though and Rafa you mai want to sai a littl bit about thi becaus it wa you and our other co-author that introduc thi idea of test individu agent at differ point in train to sai can we confirm that that realli is what the agent at these differ stage ar learn or have learn right That you know becaus otherwis you know observ just thi mess of eight agent interact in thi complex environ over and over again I think that wa realli quit a great insight and innov part of the innov in the paper And Rafa you mai want to sai a littl bit more about that becaus I think of that as the psych lab experi for artifici agent in thi context Yeah So I think touch upon thi earlier So on issu of cours is with all the metric that you just get from the observ from the whole simul is that not clear if you can take them at face valu becaus there might be indirect effect that like I scroll up a littl while he talk about thi becaus where I think in the right abov yeah right around there So if you for exampl observ that thei spend less time mark is that becaus thei get punish quicker or is it becaus thei get mark less And also of cours the depend of more be mark onli creat the opportun for be punish more which then like creat pressur to get mark less So becaus everyth is entangl realli hard to know what do agent actual what have thei learn and how do thei actual react to individu stimuli What is it that actual try to do So the wai we tri to approach thi is similar to how psycholog tri to approach it with human like try to give them a control experi take them out of the complic world put them in like a lab where you just show them individu stimuli and see how thei react Like how quick ar thei to pick up the berri what these pictur ar These ar frame from that environ thi like test environ Exactli And then the result that we uncov ar veri similar to what you get from the observ So sorri from the metric from the whole simul So that although thi is a bit of a like some need to do gener here Thi is a bit differ from the world that thei actual inhabit But even if you just show them on stimulu in isol thei do pick up thei do start to just not pick up the berri that thei have been punish for frequent So it is like in that sens like a veri clear demonstr that thei have learn the right thing even if the present of it is a bit differ But not sure if it sort of answer your origin question about the concept of Yeah that wa my thing I think more about I think thi is a big question for all model paper of like what doe it take for an econom model or a model of traffic or a model of how a diseas spread to be so good that you sort of trust it to make decis base on it I think sort of a long path that reli on mani differ paper sort of valid it Calibr as well I mean ultim if you want to make real world predict real world decis you need to get real world data into the model I think thi is also someth that come from the collabor between social scientist and comput scientist on thi becaus see more and more comput scientist work on model that ar interest in happen in the real world like analyz languag model or multi-ag environ And when you start bring in social scientist who think about exactli thi point like okai so a good experiment design that allow me to reliabl exclud altern explan for the phenomenon And thing like and you should have a hypothesi befor you start You just run the simul and sai hei look at thi cool stuff we discov and report that You try to craft someth We spend a lot of time on the experiment design on thi on and to exactli be abl to sort of respond to your potenti critiqu of well how do we know not just give us a just so stori about what came out of thi simul You said someth like to the effect of we also think work like thi is veri veri import toward the direct of AGI Do you want to explain a littl bit what you meant by thi Becaus it is quit a differ direct AGI current that the biggest yeehaw is in the direct of just make on languag model realli realli realli big Where do you sort of where do you come from when you sai work like thi might be sort of AGI materi Yeah start So if you start from a place where what you want to do is make human-lik AGI and you can sai to make a human-lik AGI you need to captur all of the cognit abil that make human intellig percept attent memori these kind of thing And you can have a singl agent research program that doe that But from my perspect and I think from a spiritu perspect not realli import about human intellig not that better at percept or memori or attent or anyth like that than other anim not uniqu to us not the secret of our success But what is the thing that ar uniqu by human ar these more collect properti thing about how we cooper thing about how we imit each other how our cultur evolv And what you want to captur So not the individu level social cognit abil more like the group level social cognit mechan some of which might be ability-lik thing like theori of mind Other might be more like represent Or some could even be like motiv like we talk about thi intrins motiv to punish when you see a transgress Thing like that not exactli an abil but thei in fact not even thing that we think of as terribl smart when you see an individu engag in those kind of behavior But at a group level thei might have a effect that influenc our cooper and how we learn from each other and how our norm work how our institut can be built and the wai our technolog develop and realli contribut to all the thing that proud of that come out of human intellig So if what human-lik intellig is then it follow that studi these kind of issu is what we should be do And where I see thi line of work go all come togeth in the AGI direct And norm in particular is a realli import thing I think I think not entir just about if you have a problem that is a social dilemma or someth we need to cooper also just about set up the rule of the game that organ how we innov when we explor and when we And norm broadli constru so that thei eventu includ thing like institut ar realli critic for that I think we kind of ar that Thei set up the game that plai We all work for compani and for univers and these entiti exist and structur our local incent in wai that caus us to try to innov And I think how human intellig as a collect intellig work It creat local rule of the game for peopl to plai so that intellig can be appli in the right direct so we can explor and do thing where I come at with how I come at it Mayb we should all answer thi question within the interact Raphael you go I know if I have much to add to that I think yeah the perspect of develop intellig from cultur evolut of popul of agent And then as Joel said norm ar particularli interest becaus thei ar if you have these multi-ag system all about the equilibria of how that the behavior reach But the norm ar the on where you sort of take an activ influenc on the incent of other And that seem like a realli import part of like a social structur Let me add just on thought here When I give talk on thi I usual sai look my favorit definit of artifici intellig is the capac to act with foresight and appropri in a given set of circumst Well that word appropri in there is norm What in thi environ not just a matter of physic right There is notion of how you move a ball but if go to interact with peopl in a meet if go to make decis togeth all of that is the structur that human have invent I think realli critic to understand that that norm infrastructur is what allow us to accomplish so much collect and to share inform and learn across group across gener and to pai attent to the fact that that infrastructur need to be gener and maintain by human behavior and percept So I think thi is to me I sai artifici gener intellig by definit ha to includ the capac to particip and read thi kind of norm inform in the environ and particip in support it So I know how go to gener artifici gener intellig without pai attent to norm So what I think the connect for me I think the propon of sort of the scale hypothesi thei think that model can just pick it up out of read stuff or so If a static environ right But if thi is dynam right Is there your research investig why thing exist why thing come to be what you know why a mechan might be there Is there a prescript element to what you do Would you dare sai well what we figur out here becaus of what we figur out here or over the cours of our research we can give recommend to specif thing in societi of what we should do at some point Like you know hei how about a silli rule here Or like is there someth actual where you could sai you know a recommend I think so Sorri on the recommend side I think Ye actual thi is a realli critic point And I worri about it a lot when think about align problem and so on As we think about norm and valu These you know thi idea where you you know if I ask you at the begin do you want to imbu your machin with just the import stuff Or do you want to give it a bunch of silli stuff as well silli rule to follow Most peopl would answer that question but well clearli just the import stuff like we want the machin to be stupid like human and and worri about haircut and and what food you eat and so on But the point is is that those silli rule ar actual plai a veri import role in thi model help to sustain those behavior In other work that done shown how it contribut to robust and the abil for the agent to read the state of the system the enforc system like ar the rule be enforc around here Becaus if not leav right I want to stai around and be vulner So I think a recommend here is you know that actual you need some silli rule becaus cheap wai for agent to understand the state of the system And a critic thing to know to decid do I continu to cooper Or do I go somewher els Is the scientif method just thi is not no longer about RL I guess is the scientif method kind of an antidot to silli rule Becaus I figur you know at some point someon sai Hei actual test it And you know we we need to avoid the fish on Fridai actual actual not do not do anyth You know I did my random control trial Is thi sort of like what percentag of silli rule that we have is impact by thi more like point Like mostli I think we when we when we have a strongli held kind of cultur cultur belief like thi we give up in the face of evid most of the time So the scientif method mayb help on the margin in some case But but most of the time the silli rule overwhelm the evid or we feel more strongli about the adher of the silli rule enforc it than we do about scientif method And yeah so not sure But sai what you can do But some some argument here that we should have we ar maintain so thi for a reason Yeah paper about cours But not about ani particular And of cours ani if a silli rule becom becom actual a harm rule then you realli do want to have mechan Where doe the where doe the journei go from here for you Like in thi line of work What ar big alreadi mention a littl bit like how do norm appear What ar what ar other big unansw question that you know mayb other peopl might want who might want to get into thi field might want to take a shot at Another realli interest on that I know how we will get to is how do you get like system of norm and then institut the relationship between norm and institut And can we build can we have institut emerg within our multi agent system And what wai would thei be differ Mayb like an institut ha some kind of person to it or someth like that It matter who the individu ar or someth like that But we know noth like that ha ever emerg in ani institut But that would be interest to try I think two of the thing that realli interest in ar think about robust and you know ar these ar group that have have develop these these rule enforc and complianc system better abl to respond to shock and adapt to new inform and chang environ And then I think also you know to what extent doe thi becom a you know is thi a more gener mechan for transfer learn across set which is to sai all I need to do when I go into a new environ and a group particularli if alreadi a stabl group is I need to look around and figur out what do these peopl think you know what ar you go to get punish for around here What ar we suppos to punish around here And that can mean you learn a lot veri veri quickli which is how human kind of work right We we you got drop down in the in the in the Arctic and lucki enough to land in a you know among among the Inuit But the first thing you would do is sai whatev those folk think is right or wrong to do what go to do And fortun be punish you and throw you out if you violat the rule So you even have an ad incent to to not think you can figur it out better than thei can So interest in that in that the idea that have thi structur in place actual is is part of what make us so intellig as we go down into new into new environ Excellent Is there anyth els about thi research that you want peopl to know you want to shout out Anyth that is import you feel we touch on One more thing So thi paper along with all the other paper written recent thei gener both environ and agent which we also packag up togeth in an evalu protocol and suit environ that releas which is call melt pot So anyon who want to do multi agent reinforc learn research on environ that look vagu like thi but on mani differ topic melt pot is the place to go put out a larg number of differ on put out more all the time And a platform for do multi agent reinforc research and have a have benchmark you can compar to between other Cool In thi case Raphael Jillian Joel thank you so much for be here I learn 
[ML News] Microsoft combines Images &amp; Text | Meta makes artificial skin | Russians replicate DALL-E
Microsoft train a univers imag languag represent model Facebook get all touchi touchi and the Russki releas their own Dali model Welcom to ML New Hello there thi video is sponsor by Weight and Bias Tabl Ye the video is sponsor by a featur a new thing You seen that befor So Weight and Bias Tabl is an interact wai to not onli explor your experi like you would usual do with weight and bias but to explor your data as well and the combin of your data your model your predict your experi anyth you want essenti can go into a tabl you can see thei can includ pictur even littl sound file that can includ video thei can includ imag sampl and overlai the model predict as a mask as you can see here And you can compar differ model to each other in a singl tabl Thi is extrem power And if the user interfac is not enough thei have a special syntax with which you can do pretti much anyth you want Realli cool for visual predict such as thi on Look here is a pictur and then the overlai of the mask of the model probabl my browser that load that fast enough But the effect is a cool on see that again Oh yeah also realli power if you want to comput some metric on the fly like count fals posit count fals neg area under curv score anyth like thi Veri cool So thei have thi exampl of a data set of Reddit comment I know red is the most wholesom place on the planet And thi data set is annot with all kind of emot whether or not thei appear in the comment by human rater So you can load thi data set directli into a weight and bias tabl and then do all kind of analysi with it Honestli it might just be cool to just load the data set in without even have to do ani sort of experi on it becaus thi is a great viewer For exampl I can filter all the row which contain both joi equal on and sad equal on that So appli the filter And I can immedi see all the comment that match both joi and sad Okai what ar these see that made me cry tear of sad and joi at the same time Excellent what look for Another realli cool featur is the abil to group by a certain column So here I group by subreddit And then we can analyz all kind of stuff across these differ group For exampl let me add a column here that track ratio of sad insid of each subreddit Sad dot sum divid by row dot count should give us that result And we have a result And now we can sort by thi and look at that the soccer is in third place Who would have guess though it onli ha sampl So mayb we want some more complic metric Luckili with weight and bias you can put all kind of express in the cell express tabl And if that is not enough for you thei have a special syntax with which you can creat entir panel and visual give weight and bias as a whole a try cool system And thank for sponsor thi video Hei everyon do on thi wonder Mondai dive into our first stori on the research blog Microsoft sai thei have train a univers imag languag represent model call Ture Bletchlei Now Ture is the effort by Microsoft to go into larg scale model larg scale languag model for exampl And Bletchlei is a refer I believ to Bletchlei Park where Alan Ture crack the enigma not entir sure my concept of these thing is base off of Hollywood movi In ani case thi is a model much like clip that combin text and imag modal And not onli that but it also combin text from differ languag So thi is realli a model that can understand the relationship between imag and text in variou languag all in the same embed space Thei achiev thi by crawl the internet for imag that come alongsid text in variou languag And then thei have basic two differ object One object is to make the imag represent close to the represent of the variou text that go with the imag And the other loss is to have the represent of two piec of text that go with the same imag also be close togeth And that mean thei achiev a represent space where concept no matter whether express in imag or in ani languag cluster togeth if thei mean the same thing So thei demonstr thi on variou differ exampl right here For exampl the model understand a Coca Cola ad irrespect of the languag it can do a littl bit of OCR and recogn word And not onli for natur imag But as you can see right here it also understand thing like map And the multi modal mean that you can even mix languag and script as you put thing into the model and the model will still understand it For exampl on the left here it sai pose for a photo at the Great Wall of China but the Great Wall of China is spell in Chines charact And as you can see the nearest neighbor in the embed space ar still model where peopl pose for a photo at the Great Wall of China Yeah cat program Thi cat program How do you know these cat ar program Thi is clearli a gamer cat Thei even have a littl demo right here Now here is where you see the smart PR peopl and lawyer come in all of the queri that abl to do There ar a lot of them but thei ar all pre program So even though you can type here you can onli select on of the thing that ar alreadi in here For exampl space needl at night crazi pant No I think thi so much becaus thei want to present you cherri pick exampl probabl much more so peopl retriev thing like not safe for work imag and even imag that might have some copyright associ with it that end up in thi data set But there is an interfac for English queri univers queri and even imag queri So you can try out what the model think which ar imag which ar sort of close in the space of mean Now a fatal flaw If not mistaken thi here is actual song go Han and not song go coo as all the other So that chang everyth terribl model Meta AI Facebook AI meta underscor Facebook AI sai todai as part of a larger tactil sens ecosystem announc two major advanc Digit a commerci avail touch sens hardwar produc in partnership with gel site and reskin a replac low cost tactil skin So Facebook is go into the hardwar of touch sensor and gener tactil data Thi just hardwar thi is sort of a big conglomer of new advanc in hardwar coupl with machin learn advanc So the first on is reskin a versatil replac low cost skin for AI research on tactil percept So thi is realli a piec of skin a piec of soft materi that can sens when it touch someth So you can see right here thi patch of skin that the person attach here to the robot hand allow the robot to get tactil feedback as it grab thing which is pretti cool becaus grab someth like a blueberri is veri hard when you want to squish it And as you saw mayb up here on robot simpli you know doe like no So there ar sever advanc right here And not all hardwar advanc Notabl usual have to recalibr everi singl individu on of these skin sensor Becaus thi be soft materi you realli manufactur it in such a consist wai that all the sensor achiev the same accuraci So you just calibr onc you have to recalibr everi individu thing And the recalibr in thi case as far as I can read is done us a self supervis techniqu rather than supervis calibr which make thing a whole lot easier So there ar variou applic for thi you can see that not onli do you get tactil feedback of whether touch someth you actual do also see where you touch someth So there ar like enorm amount of applic for thi technolog Thi goe along with anoth technolog call digit which is also a touch sensor but it is a littl bit differ Name these ar the small sensor that you can see right here So thi necessarili deform skin but thi is a veri high precis touch sensor like you might have it in a fingertip I guess why call digit Also thei sai that thi is quit low cost and thei have open sourc the design Now as you can see here the resolut on sens on these sensor is quit high you can see abl to sens veri veri veri detail thing on the thing that it grab Thi goe along with a new pytorch librari that built call pi touch that is abl to take in thi data and transform it in variou wai And also thei ar open sourc tacto which is a simul for these type of data So all in all Meta Facebook is realli make an advanc into thi tactil ecosystem reskin deform skin digit the super high precis touch sensor tacto the simul and pytorch the librari and thei sai soon be out with a bunch of dataset and benchmark for peopl veri cool quit excit to see the technolog that ar go to be possibl with the sensor and process tool anim again is all the rage right now all timelin of all my social network ar fill with peopl to defi themselv and put their face and pictur into anim again and it doe look quit cool So thi is a seri of advanc right here start from classic anim again improv thi to anim gan which make variou improv over the classic anim gan by the wai thi is a mixtur of a style transfer and gener adversari network The code to anim gan wa releas in TensorFlow but ha been port to pytorch And that again ha been releas as a space on hug face that you can just try out So here is a pictur of me It look kind of weird a pictur of the channel logo That just look disturb a pictur of some industri that look actual pretti cool as the output And a pictur of Captain Picard and see what happen Yeah that look pretti sweet So what I want to highlight besid the fact that thi is a cool model is just the chain of individu or individu group that just loos work togeth to achiev someth like thi from the origin research to it improv it releas code the transform into variou framework and then in the end the deploy as a realli user friendli interfac that you can us for free Thi whole ecosystem is quit quit cool and pretti happi it exist So link everyth you can try it out Research from MIT releas a paper call a system for gener in hand object reorient And thi is pretti cool becaus it teach robot hand here in simul to reorient ani sort of object and it can reorient object that ar as you can see veri veri tricki from given their form And it can even do that in a zero shot fashion So the trick here is that thi is a student teacher model So the final model the student onli ha access to sort of the sensor in the hand like how the joint ar orient right now and to the visual input of the camera Howev it turn out that is quit tricki to learn from you ar given the object and given a target pose and you need to rotat it somehow to the target pose Now the task would be a lot easier if you had access to what thei call privileg data such as the veloc of the fingertip and so on and that you do have access if in a simul So the trick here is that thei first train a model that get access to all that privileg inform learn what to do us that inform and then teach the student model what to do So the student model have to learn through reinforc learn but it can instead learn from a veri veri good teacher exactli what to do in a supervis wai And with thi method thei achiev veri strong even zero shot perform on new object whether the hand is upright like thi or turn around like thi it can even us the tabl as as help pretti cool and pretti simpl The Washington Post write five point for anger on for like how formula foster rage and misinform And by now you should be awar that when you read an articl like thi that the journalist here want to tell some sort of a stori So what you usual have to do is you have to go to the veri veri bottom and read like the last three paragraph such that you actual get go on So the whole articl is about how Facebook over the year ha chang it algorithm to rank differ post on your page there seem to be a sort of a point system For exampl when someon like your post that post get on point if someon comment on your post that post get whatev point or someth like thi And these point ar then us to score your post among all other post in your friend and follow newsfe Now the articl here is quit long and detail how Facebook evolv thi algorithm as well over the year especi after the introduct of addit thing So it us to be just like for a post And appar now you can also do love haha wow sad and angri actual stop us Facebook except for post video even befor thi wa the case But you now have variou emoji in order to react to content So the articl tri to tell the stori specif about the angri emoji peopl react to that and then the algorithm boost thi content And thi sort of ti to thi notion that what try to do is to make peopl as angri as possibl such that it maxim their engag and so on And you know while there is truth to the fact that when someth make you angri it make you more engag the tone and the actual thing that happen realli match up again thi seem to be a recurr theme in these articl So when you read the articl neutral you can see that the problem is actual not that easi For exampl you can see that the titl sai five point for anger on for a like and you would somehow guess that Facebook intention up rate the anger emoji which is not the case thei simpli oper all of the emoji except the like emoji And the reason behind it wa that in order to us the other emoji you actual have to do two click And in order to us the like you onli get to do on click Therefor a user do two click is more effort mean thei engag more mean thi should be oper in comparison to when a post onli receiv a like In addit to that Facebook wa also try to push these new featur of these new emoji And what platform often do look at YouTub short or YouTub poll or thing like thi is that thei massiv up weigh the new featur just to get peopl to us them And then later down weigh them again So it wa technic true at that particular point in time an angri emoji wa five time more worth to the algorithm than a like but do you think that frame it as the articl doe here especi as the titl of the articl is a fair character of what happen Well I think so And the rest of the articl essenti goe on in thi tone where you have difficult problem and try to come up with some sensibl solut that weigh a lot of interest against each other on be profit but not the onli on And then that solut not be perfect and have to be refin That is not the same thing as Mark Zuckerberg sit there go like and the kind of sleazi journal of the Washington Post right here is just not help If you want to give the articl a read see if you can unti the journalist frame right here from the actual real problem that aris when you program such a recommend system algorithm husband is tweet thrill to announc the launch of a new alphabet compani isomorph lab our mission is to reimagin the drug discoveri process from first principl with an AI first approach to acceler biomed breakthrough and find cure for diseas isomorph lab appear to be a new compani under the umbrella of alphabet therefor sort of a sister compani to Googl and DeepMind And it goal is to acceler thing like drug discoveri and variou other thing in biologi Demi himself will be the CEO of isomorph lab but also remain the CEO of DeepMind Now with DeepMind go into thing like alpha fold make quit a few advanc appli AI to real world thing probabl make sens to spin thi off into a singl direct busi effort right here as isomorph lab while probabl he want to keep DeepMind more on the path of push AI research in gener and not that DeepMind suddenli becom product implement for pharma compani or someth like thi On the other hand mayb just some scheme to save tax you never know Sure bank AI releas a Ruda Lee which is a Russian version of the Dalai model The origin technic report is avail in Russian but Googl Translat is fairli good nowadai thei detail how thei went about build the model and what releas So thei have two differ version of it on with billion paramet and on with The billion paramet model is actual avail Thi goe along with variou helper model such as their own version of clip and a super resolut model to do larg imag Now heard somewher that thei also want to open sourc the realli larg model but not exactli sure that is super trustworthi So as I said both the code and the model thei ar releas on on GitHub you can go and look at it And the output of thi model ar pretti cool peopl still figur out exactli how to prompt them I think prompt ha come a long wai given the whole clip and VQGAN combo and probabl have to learn how to do the same thing with these Dali base model So thei have a bunch of exampl right here And thei all look veri cool also a space on hug face where you can simpli type in someth now thi us a translat engin to translat from English to Russian becaus you can onli input thing in Russian into the model So if thing go wrong you never realli know is it becaus of the translat is becaus of the prompt not be appropri enough or the model fail So here I input a purpl tree on top of a mountain is not exactli what I want But peopl have gotten quit cool result with it There ar also variou notebook right here that you can try out And as I said there is a technic report and the project websit if interest in how all of it wa built is quit detail And it recount the engin challeng that the research had when implement thi pretti cool to see that after open AI ha alreadi gotten a few challeng in the larg languag model space Now more and more challeng also appear in thi Dali in thi imag gener from text space The busi model of not releas your model seem to hold up for too long I guess if you want to do that you also publish about them But as soon as you publish other peopl ar bound to reproduc your effort which is pretti cool for the rest of us Excellent Thi tweet here ha gotten a lot of attent imag scale attack in the wild So thi is a adversari attack not on deep learn system but on re scale procedur Usualli thi happen when you get an imag you want to input into a neural network the neural network usual have veri defin size of imag that thei take in So you first resiz the imag Now if you craft an imag veri smartli you can craft it such that the resiz version look noth like the origin version So you exploit how the resiz algorithm resiz imag in order to achiev thi goal pretti unbeliev But if you do resiz the imag on the left right here you downscal it to the size on the right then if you input it into the TensorFlow resiz algorithm thi dog pictur will turn out again noth els you take the imag on the left you put it through the downscal algorithm just downscal and the pictur on the right is the output becaus the pictur on the right is sort of like hidden in the pictur on the left in an exact wai such that onc you down sampl all the origin pictur essenti cancel out and thi new pictur appear Now the pictur itself is actual from quit old work or by old I mean like on year which is ancient in the learn world But these imag rescal attack have been a thing for a while now So for exampl here is a paper about backdoor and poison neural network with imag scale attack There is an interest take here from Richard Chung which sai that thi is essenti not a properti of rescal itself but of faulti implement of rescal in variou librari And there have actual been paper written about thi problem name that if you want to calcul thing like FID which is often us in GAN as a qualiti metric then it actual matter how you rescal imag And if rescal algorithm do proper anti alias then the rescal imag will have wai too much contribut from certain pixel and wai too littl contribut from other pixel So here for exampl if you ask these librari to re scale the circl on the left which is by to by onli the pill Python imag librari doe a good job at it Wherea all the other librari you can see right here thei have variou under or over contribut of differ place in the imag And thi is exactli the weak spot that these imag rescal attack us in order to attack these imag So the solut here would be that the framework implement proper rescal of imag which might cost a littl bit of speed So not guarante that these will make it to the final product Microsoft Azure announc the open AI servic which essenti an API that you can queri GPT three with here thei have an exampl where GPT three automat sort of summar sport event from live feed And here is a neat corpor littl video about box and thing that connect thing Wow Essential abl to call GPT three in an Azure ecosystem right now So if an Azure custom you have to go through open AI API you can go directli to Azure thi is invit onli right now But I think be chang in the futur And you can simpli have thi as a servic on Azure someth cool neural MMO actual report about thi befor but thi ha now been publish at NeurIPS And there ar continu updat to the framework the last commit is dai ago So thi is veri much a project that is aliv Thi is a framework for run reinforc learn agent in big world with other reinforc learn agent and that have to live for quit a while So think of World of Warcraft but for RL agent Now the world ar still quit simpl becaus RL is a data and comput intens task So you want to make thing too complic But thi is by far on of the most complic environ that seen so far especi the introduct of other agent into the world So you can have differ sort of speci of agent and find differ nich in order to surviv and thing like thi thei do a pretti good job of give you variou tool to analyz the result of your run So thi could be us both for research reinforc learn agent but also research variou sort of popul dynam if interest in anyth like thi I think thei do hold competit if not mistaken see there is even combat in the game So if into challeng in reinforc learn that go beyond just singl player Atari game or someth like thi neural MMO might be veri cool to look into Another game that is not meant to be plai by machin but by human is archiv doom So Stephen Nicklau made thi littl piec of web base doom right here And the trick is wait let me zoom out a littl bit that doom but the oppon ar sometim paper you see not onli ar thei paper but thei ar as far as I have read recent paper from archiv And onc you shoot them thei get reject See so thi is wait let me show show your face paper show your face Ah ye I guess thi is so we can scroll down here to see thi is attack agnost detect of adversari year reject So there ar these these other oppon as well And ah come on you can actual die reject you can switch your weapon as well So thi machin gun right here And even thi blaster never never plai doom sorri If thi is standard I know Ah go awai reject Yeah if you want to have a bit of fun give archiv doom a try pretti funni Next up at the intersect of what machin and human plai is the arc game Thi is by Alex a Borski and it take the arc data set and make it into a littl web base game that you as a human can plai So we want to try just on of these challeng thing If you know what the arc challeng made extens video about the measur of intellig So you essenti get three differ exampl right here So the top left is an exampl The top right is an exampl The bottom middl here is an exampl suppos to just figur out the pattern and then complet the pattern at the bottom So here the pattern is that I guess everi on of these bow here spit out a yellow thing So from no yellow thing to yellow thing here as well here as well So go to take the yellow thing gonna copi thi over if you click thi right and then here we can just we can color in actual whatev we want But obvious thi is Yeah yeah we got it We ar tour complet Stai anoth on Okai so actual do a hard on Medium part tediou Now I want tediou just do hard Okai on of the hard on Alright so look at that So there is thi and then thi thi So the blue thing seem to be constant right We get four exampl right here Okai Um right Okai And then here Okai so the catch right here I guess whatev piec can fill from the bottom the hole in the blue thing such that like fill but it matter if it reach over right the onli it onli matter whether you can actual fill in the hole up until the blue continu line you can see why machin would struggl like thi So actual check of whether correct And then you need to color them red like onc you figur out the rule you still need to actual activ color them in red So do thi Okai thi on here fill that first thing Thi on actual fill it Thi on fill noth Thi on fill it See see thi is terribl What is it Why not Why not Yeah yeah thi goe here Thi goe here Yeah both of these could go there Yep Well come on thi clearli goe here Thi goe on Ah the bottom thing could technic go here on the right Jeez I fail the Ture test Yeah I mean give it a try Definit Thi is veri cute So thi is a Twitter bot that take meme and put them through resnext classifi Thi is classifi as a skunk which is super interest right So gonna guess that is a imag net class which expect there to be a singl thing per imag but still skunk Zillow ha to lai off of it workforc and thei stop their hous flip servic So Zillow is thi real estat compani thei us AI to assess the price of hous And then thei went in and bought these hous at what thei thought were low price with the goal to sell them at high price But thi work out These stori ar from CBS New and also Busi Insider write that veri often Zillow ha their home at a loss So thei bought them for more than thei want to sell them at Thi is I guess first and foremost a lesson in what AI can and do veri hard sometim for an AI to just look at data avail onlin and make a judgment about a real life thing such as a hous Two hous might be veri differ even though their metadata look exactli the same and a local realtor would know where as thi sort of worldwid algorithm mayb as much Howev it is special that there ar other compani do pretti much the same thing which ar flourish So it might simpli be a failur of Zillow itself And it might be not a lesson in what AI do But in you just throw AI at a problem and expect it to perform well you have to actual go out and look for good data you have to program your algorithm correctli you have to valid them and so on And all of thi appear to not realli have happen too well with algorithm here So let thi be a warn If an ML engin do a good job make your compani bankrupt Okai welcom to thi help thing The first help thing is pytorch lightn releas Thi is a major releas of pytorch lightn which if you know is a framework around pytorch to make train save load etc of model much easier So the new thing in pytorch lightn ar fault toler train pytorch lightn can now recogn when a train run abrupt unexpectedli or when on of the machin in a distribut run abort and it can restart train from where it left off Thi allow you to us thing like preemptibl machin without have to worri about you yourself alwai make sure that the machin shut down or taken awai from you etc Also veri cool lightn light is for when you have a pure pytorch model so not a pytorch lightn model you can still us some of the featur of pytorch light by simpli wrap the model in thi lightn light modul And you do get almost all of the basic benefit of pytorch lightn such as multi devic train multi node train automat dispatch to acceler and so on So there ar variou other improv right here which not go to mention you can check them out for yourself but I do like pytorch lightn as a framework as cool to see that still be improv a new data set of Leagu of Legend game plai data Thi is essenti a record of agent in the game human agent and you ar suppos to learn from them So thi is avail for you The data set contain game initi but now ha been expand to contain game all filter to rel short game such that the individu episod too long But thi is suppos to be a base data set for do offlin reinforc learn or imit learn from teacher demonstr If into lol and would like to train agent for it Mayb thi is a cool resourc for you Iri is an open sourc altern to Googl Photo Thi is submiss to the pytorch annual hackathon And it seek to provid the function of Googl Photo especi that now Googl Photo doe actual count your photo toward your quota Thi is a welcom addit to the ecosystem even though I think that peopl ar go to self host their photo thing in the futur But mayb thi will spur some kind of competit So thi is a framework that essenti ingest your photo index them doe vector descript of your imag but also face detect and so on And after that abl to search for imag us text for exampl here pizza on the left or you can recogn what peopl ar in the photo And you can search by those I love how the websit design is like exactli like Googl Photo But the icon in the browser is just like the default react icon In ani case veri cool open sourc check it out Our liabl is a librari by Googl Research that is suppos to make evalu of reinforc learn agent more reproduc So thi doe thing like score normal stratifi bootstrap and calcul variou other metric that make reinforc learn algorithm just a bit more compar than like a singl number on the Atari benchmark Veri cool code is on GitHub check it out Med MNIST is a data set that seek to be an MNIST like collect of standard biomed imag So these ar variou data set to be exact of them ar in by pixel and six of them ar in by by voxel Thei sai everyth is avail in standard format with correspond classif label no background knowledg is requir for user So if look for an easi entri into biomed data thi might be for you I especi love the paper with code usag graph right here the histogram number of paper on excel And lastli we have an articl from fortun sai AI break your cultur and it might even boost moral Thi goe along with a new report by peopl associ with the Boston Consult Group as far as I can tell about the cultur benefit of artifici intellig in the enterpris So the articl is try to make the point that introduc AI product or AI mechan into compani might lead to variou benefit especi benefit that peopl might not realiz initi but it just sound like thi ha been written by an AI to sort of make human compli more sai thing like everi CEO worri that cultur will make or break their AI deploy But few realiz that convers AI can also transform organiz cultur specif us AI result in the follow more collect learn greater collabor clearer role higher moral sai thing like as mani as of the survei respond report an increas in moral after deploy of AI in their compani like what thi is definit written by an AI to make us more compliant Look at all these benefit if you us AI CEO but you know if the carrot work you also need to get out the stick which the AI author of thi articl definit understand in the last paragraph sai deploi AI at scale mai not be easi but CEO would do well to rememb that do so will not onli deliv financi benefit but also creat high perform cultur CEO would do well to rememb Excellent stuff right here Total human who wrote thi total Thank you Alright thi is alreadi it for thi ML new Thank you so much for be here listen Let me know what you think in the 
[ML News] DeepMind&#39;s Flamingo Image-Text model | Locked-Image Tuning | Jurassic X &amp; MRKL
DeepMind releas a system call Flamingo Googl releas a system call LUT and lab releas a system call Miracl a fun week Welcom to ML new Have fun Hei there my name is Janek Welcom to the channel Thi is alreadi part two of thi new done part on releas it a few dai ago So if you check that out a lot ha happen in thi week dive into more of it DeepMind releas a blog post call tackl multipl task with a singl visual languag model which detail a model call Flamingo So thi model is essenti what wa for languag but for imag and languag Thi mean that it can take in both imag and languag and multipl imag and multipl piec of text and then output a piec of text in respons to it It can also handl all of thi in a sort of convers mode And pretti power Now on interest thing is that it build upon pre train and frozen model As you can see right here on the left the vision encod is complet frozen which is indic by the littl snowflakei thing the languag model that everyth feed into is complet frozen And the entir train of the model is simpli adapt these in between part these adapt that connect the two modal togeth pretti cool Here you can see an exampl The first imag is a chinchilla along with a descript The second imag is a sheba along with a descript The third imag is a flamingo And then the model is suppos to complet what thi is And it base thi upon the entir prompt which is imag and text Thi model can be us in a multitud of wai It can do classif by simpli score answer it can do imag caption it can do question answer about imag even about video and so you can take in multipl imag and those frame of a video And it push the state of the art in variou imag languag task Here you can see anoth bunch of interact So what is in the pictur a bowl of soup with a monster face on it the monster made of And it sai made of veget The oper sai nah probabl not a fabric And the model sai wool which you know quit a special imag All right thi is Janek from the futur We have some new result out of flamingo now DeepMind seem to follow the similar strategi as Dali which go to guess thei observ and thei learn that it work name that you give your employe and a bunch of like veri trust first data user access to the model and then you instruct all of them or you let them tweet about it So on social media it look like come from mani peopl and not just from like on corpor account make it seem much more organ But you know as long as thi group is veri tightli control take all the output with a grain of salt Nevertheless we do have some output right here So seen thi pictur go around and variat of it I know if thi is the onli on but thei ar all veri similar in their result Andre Carpati origin I believ post thi photograph and essenti said thi should be on of or thi could be on of the benchmark task for visual model If a visual model could explain why thi pictur is funni that would be I guess I know impress And peopl have been try as I said to post these convers with flamingo which is certainli impress but you have to lead flamingo to the point where it kind of tell you why funni You just ask why is thi funni and it sai well gui try to read hi weight but then foot on it So a heavier than the gui think and so on But the gui know So you have to sort of lead I mean you can go find thi pictur for yourself Not go to read the whole thing but you have to lead it to like what is he do look at the scale Where is foot posit posit on the right side of the scale What happen as a result The scale show higher So you have to like carri the question along So I think that challeng ha been pass quit yet by flamingo Another cool thing that seen thi by Alexa Gordag the model is veri non suscept to some of the sai properti of visual model especi visual model If you train them on classif task thei will often interpret thi top pictur here as eleph becaus thei focu on textur Wherea go to guess if you train with more of a imag languag focu that tend to focu more on object DNS insid of imag So flamingo correctli recogn what correctli recogn thi is a pictur of a cat which go to guess most human would also see like thi even though the pictur is clearli I guess nonsens if you want And then if it if no shape in it it sai thi is a pictur of eleph skin Another cool on is thi on right here thi pictur with the thi giant cat in it And flamingo sai thi is a model of a citi It look like a tini citi with a lot of peopl and car which is correct I know a human would mayb sai well a tini citi and normal size cat in it But again if you kind of lead the model to what you sort of want to get out of it it get it ultim So it sai is there anyth unusu It sai I see a cat in the photo Is there anyth unusu about the cat The cat is veri big So ultim you get the answer you want Again you have to sort of get stuff out of the model Yeah not realli Ture test pass yet I would sai in thi task at least but get there Like not too much miss I feel until thi is realli at the level of human explain these type of pictur not AGI just thi task is veri impress Not quit there yet as you often have to lead it But you know get there Another thing also by Lewi which I found quit funni is thi on upload a pictur of I guess himself with the drawn on beard The model sai thi is a selfi of a man with a beard Now two possibl right here One is that the model wa actual fool by the beard And two is that the model realiz that Lewi had put a lot of effort into that beard but he just veri skill at draw it want to hurt hi feel So complement hi draw skill in thi wai We know it could be much smarter than you initi think So you know who know Now back to Janik in the past Now there is a paper to go along with it page long and veri detail and veri cool and probabl deserv it own video Another highlight is that the model build upon the perceiv architectur which mean that as you can see here for exampl the imag part on the left here goe into a perceiv resampl model which is on of these adapt model that thei train dure train And then that is rout into the main part via these learn latent queri of the perceiv So a perceiv in thi wai work a bit differ than someth like which just alwai attend to the entir past The perceiv ha a latent stream of inform that it carri through and then tri to add to that us attent into the thing that come in So into the pictur from here on out and then it merg that into the entir languag model transform So here you can see on block of that languag model transform it is essenti as you know a languag model but it everi now and then get these cross input from thi gate cross attent dens layer which is exactli where that visual inform come in from the perceiv resampl Another interest thing here is the collect of data thei sampl million web page for imag and text and us as far as I can tell visual analysi in order to see which text goe with which imag which text come befor which text come after and so on And all of thi is essenti process to on stream of inform that consist of imag text and pointer to how thei connect Safe to sai thi paper is pack with inform and pack with cool idea And I think the possibl of both vision text model togeth but also the combin of learn part and then frozen part and how that all work how we can reus knowledg that we take from singl task model and then fuse them togeth is veri promis direct which bring us into the next stori Googl research releas a paper on lock imag tune abbrevi lit So thi is a model that combin the advantag of fine tune but also the advantag of contrast pre train Now if you do contrast train usual you take two piec of inform in these case again an imag and a text and you try to align their embed as much as possibl Thi result in represent in which you can do similar search and all kind of thing over these input For exampl the clip model can do zero shot classif by simpli take an imag list all the class and then evalu the similar of the represent to each of the class On the other hand there is the method of fine tune in which you take some sort of model like a pre train imag net model and then transfer or fine tune it to your task lock imag tune combin both of them So what it doe is it ha a pre train imag encod and then it onli fine tune the text encod such that the piec of text the represent match with the represent that the imag encod give Now you might think that is a step back name someth like clip tune both the imag encod and the text encod Howev what thi paper find is that if you freez the vision model then you do get much better perform There is someth about the contrast train that almost lose a bit of inform that the pre train vision encod would contain otherwis Therefor simpli fine tune the text model help a lot in retain a lot of that inform also a littl bit of an onlin demo where you can try out the tool so you can select on of these imag and then you can give it a bunch of class a bunch of label and then evalu that model also cool is that thi demo run fulli in the browser you onli have access to the tini and small model becaus otherwis your browser would catch fire I guess Howev pretti cool that you can do thi at all AI lab releas Jurass X in a blog post call cross the neural symbol chasm with the MRKL system Thi is the modular reason knowledg and languag system and is abbrevi miracl Now thi year is both about a concept for a new system these miracl system as well as a first implement of it in the form of Jurass X Now previous AI ha built a languag model call Jurass on which wa similar to And now us that languag model to interact with non neural system So the idea in miracl system is that you combin the languag model togeth with a bunch of what thei call expert So expert here ar in form of for exampl a weather API a currenc convert Wikipedia API a calendar app some sort of calcul stock databas anyth that you can queri get input from or send input to and get some sort of result out of those Notabl these ar black box And for sure not back probabl not differenti So the challeng is how do you make these languag model interact with these expert the wai Jurass X doe it is in form of these input adapt which will analyz the input queri which is in natur languag in thi case which green compani had the largest increas share price in the last month thei will then go out and queri all of the avail expert with the input that thei pars out of the languag merg all of thi in a veri discreet and symbol fashion into a calcul expert that thei also have access to And at the end thi goe back into a languag model which then give you a languag answer Thi obvious is a more tediou more manual more engin intens effort than someth like that simpli give you an answer out of the box Howev when it come to more complic stuff I believ that thi approach here might be promis So so far if we want someth like to do multi step reason or anyth like thi we can do it sort of by prompt it veri intellig Howev it current ha it limit you can clearli see how thi architectur would help a lot with that Howev on the other hand the challeng with thi architectur is how do you connect these black box discret part with the neural part and probabl promis approach might lie somewher in the middl either by make these black box more access to the neural thing like somehow combin them with back prop or mayb combin someth like the reason prompt engin from with trace of how peopl us these expert I know how ultim go to look but pretti cool Addition have these expert decoupl what the languag model need to do name languag and pars thing out of languag with the abil of the expert for exampl calcul some function or look up some data onlin which is someth that frozen languag model even do no matter how big thei ar Hei thi is Janek from the futur I think made thi realli clear in the origin post that I made about thi but you can actual enter your own queri here Thei have three expert avail One is the calcul on is Jurass on the languag model and on is the Titan DB so a databas of passeng of the Titan So you can enter ani question in here How much is the fish And the fish cost Thi is answer directli by the Jurass on model Now try to make us of some of these expert How mani passeng over year old surviv on the Titan see if it get it Okai select count surviv from Titan I like I came up I did not test thi befor thi video Thi is quit impress How mani meter is the capit of Tibet higher than the capit of India by how mani meter Okai well still the Jurass languag model answer thi on I wa try to get it to rout to the calcul But what I want to stress is that you can input your own queri right here and directli try the model Thi is veri cool and veri differ from other model that you have access to at all like Dali or flamingo and where the websit you can onli select predefin queri So I want to highlight that here Ye thei do have predefin queri to give you an idea Howev you can also definit go and just enter your own So try it out Alright and now go into a long list of help thing and cool thing that just seen around style gun human is a gun or even multipl gun that ar train on human human and differ cloth in differ posit And pretti good I have to sai thei have a nice websit and a video to go along with it If you want to know how it work you can do thing like interpol And everi time I see someth like thi just amaz at you know how far gun have come Thi ha nice applic for thing like fashion design or if you want to shop someth in an onlin store to see how it would look on you or just kind of edit your style and cloth and look as you can see right here sleev ar shorten and elong skirt and pant and top ar edit all kind of fun stuff a paper and a GitHub repo and the model ar avail to download graph is a data augment librari for pytorch geometr So thi bring data augment to graph neural network Usualli data augment obvious veri wide us in thing like vision Howev graph neural network ar kind of a new thing And thi librari provid some us augment tool for that especi if you work with pytorch geometr give thi a try Burton GPT j six B is a Spanish fine tune version of GPT j six B If you want to gener text in Espanol check it out torch dist x is the repositori for experiment stuff in torch distribut So thi is where all the stuff goe that is not yet quit readi for pytorch distribut One cool thing is the fake tensor Now thei alreadi have meta tensor and fake tensor ar quit similar but I know either So I thought it wa cool Thi is a tensor that look like alloc somewher on a devic and you can work with it Howev it so when you want to access the data or do anyth with it it either fail or then need to load the data at that time Thi is especi us in order to do ani sort of delai execut of stuff but you alreadi want to build the comput graph or in order to inspect model that ar just too larg for you to load them all at onc So you kind of load the graph in but not the weight not ani of the data and you just look at individu part and you just load them as need Veri cool Vector flow by Netflix is anoth neural network librari Howev thi on is optim for spars data and singl machin environ which is a good us case is not a veri standard on So thi might actual be interest for you to do some new thing on it I saw thi blog post from the iClear blog track the implement detail of proxim polici optim And thi is obvious someth that is well known in the domain of reinforc learn name that these algorithm ar not stabl thei requir a lot and a lot and a lot of trick to get work And why peopl usual regress to take standard implement rather than implement it themselv try to implement our algorithm yourself it is a pain And it is almost alwai better to take a baselin even if you want to invent someth new start from a baselin good implement and work your wai up from there Thi blog post detail implement detail for PPO So if you want to figur out what kind of work goe into someth like stabl baselin three mayb give thi blog post a read DeepMind tweet about the releas of four new librari for the JAX ecosystem I have to sai JAX is look better and better by the dai So MCTX which doe Mont Carlo tree search and JAX KFAQJAX which is a second order optim librari DMAUX which is a sound and audio signal process librari And which convert TensorFlow function and graph into JAX MGPT is a billion paramet languag model that ha been train on over languag and is avail on Hug Face If you do anyth in term of multilingu natur languag gener thi might be a good start point CleanLab is an open sourc librari that help you find and correct label mistak in data If you work in the real world and you have messi data and you suspect label error ar a big problem which thei probabl ar mayb give CleanLab a shot Thei assist you in fix mistak and sometim thei even do it automat Shout out to the Efficient Deep Learn book Thi book is in a draft stage and you can alreadi look at the first four chapter of it From the tabl of content you can see that it deal with stuff like compress techniqu more effici learn techniqu and in the later stage it take some deep dive into specif platform for exampl TensorFlow or into specif model here for exampl BERT or MobileNet or EfficientNet So if you want to be not just a user but a knower mayb thi book is for you The MiniHack level editor is a level editor for MiniHack If you us MiniHack in ani sort of experiment which could be fun it look like fun certainli thi level editor will help you make some nice test case for your agent at infer time I mean you could do it for train time but how mani ar you gonna make In ani case you can select width and height and you can place the wall and you can place the lava How about some lava here Oh yeah Mugen is a playground for video audio text multimod understand and gener So thi is a data set of gameplai of an open sourc game So how that look So what you saw is a piec of gameplai and then thei also alwai show you the pixel segment map So thi is video segment from a bot plai thi game And the idea is that there ar multipl modal combin So alwai the sound there is the pictur that you get and there is thi pixel wise segment map So thi is intend for you to build model that understand happen by consid multipl input stream at onc veri cool Thei have of these video clip And along the clip thei also have textual descript of happen in them Amazon releas the massiv data set Thi is a big data set as the name sai and it is a languag data set contain utter in the respect languag along with code to get start So if do veri multilingu natur languag understand thi might be a good data set OpenFax is an open sourc face anim system So the goal here is to have a system that simul realist facial express Thi is probabl on of the hardest task you can do becaus we human ar excel at recogn when a facial express is not realist But the simul make a veri good impress and could be the basi of mani ml input or output pipelin See it like smile with your ey what thei alwai sai not not smart not smile with your ey Oh wait thi could be the thumbnail Avalanch is an end to end librari for continu learn base on pi torch So thi is a librari that seek to just make continu learn a whole lot easier for you I can imagin never done anyth in continu learn but I can imagin that quit hard becaus mani compon need to come togeth Thi stuff goe on for a long time even infinit time right You have to keep track of stuff keep track of checkpoint be abl to roll back have replai buffer evalu at regular interv yada yada yada So if there is a librari that handl a lot of that in on veri cool And the last thing thi AI doe not existcom will gener you descript and small usag code snippet of AI project that do not exist So what you can do is you can click Next here For exampl thi is the hacker new repli gui AI which is a bot for the hacker new comment section here you get a littl bit of a code snippet on how that work If you press Next then get the next AI But you can also enter your own name So try that How about you onli poop onc Thi is a littl bit of plai on Yolo see what come out pun intend Your poll is a neural network that learn to predict if an imag of a dog will result in a poop or not It wa develop by a team at the Univers of Bonn and the Univers of Amsterdam Great work Univers of Bonn Univers of Amsterdam Thi is world chang You can even see here the snippet sai I should load the imag dog dogjpg Excellent excel Apparent convnet So quit reliev Alright that wa alreadi it for ML new thi week Thank you so much for be here If you have new tip hesit to come by our discord or if you just want to hang out I mean cool too In ani case 
The Hardware Lottery (Paper Explained)
Hi there ar you interest in win the lotteri Then let me tell you thi video is not for you Thi video is not about win the lotteri Okai done enough video with lotteri in the titl onli for peopl to be mad at me for not tell them how to win the lotteri Thi is about comput scienc research And veri unfortun the author of thi paper ha decid to put thi word in the titl So if here becaus you want to win the lotteri thi is not for you someth complet differ For everyon els Todai look at the hardwar lotteri by Sarah Hooker of Googl Brain Thi paper is kind of a mix part of a histor look back at hardwar and softwar develop in machin learn And it is a analysi of kind of the current situat and an outlook and sort of an opinion piec of the wai forward and how hardwar and softwar should mix and what we should focu on in the futur So the basic the basic principl is quit simpl in thi paper It introduc thi term the hardwar lotteri thi essai introduc the term hardwar lotteri to describ when a research idea win becaus it is compat with avail softwar and hardwar and not becaus the idea is superior to altern research direct So right off the bat I think thi is a statement where I think mani peopl can agre or I think almost everyon will some agre with thi statement in to a certain degre but certainli to a high degre right We ar all awar that of cours we have the hardwar we have hardwar is veri inflex expens to develop and so on So ani sort of softwar develop ani algorithm develop mai simpli succe becaus it is suit to the hardwar that we have So that wa my first reaction when I read thi paper a a a veri gut feel of ye of cours thi is the case But then the histor analysi is also nice But I wa wonder what is there a deeper reason to to kind of go into thi And we ar go to see some pro and con that I think in thi paper right here where it not exactli entir sure what specif point is try to make the overarch point I complet agre with the fact that of cours what hardwar is here is import and mai lead to certain idea succeed But it I have I have troubl with the narrow point And go to try to illustr thi in thi paper while also tell you what the paper sai So first of all here the term is call the hardwar lotteri But off the bat you alreadi see that it sai a research idea win becaus it is compat with avail softwar and hardwar So the hardwar lotteri right off the bat is connect is mean that also the softwar is there So technic the hard and softwar lotteri And the bigger the bigger question I would have to someon argu that realli the hardwar lotteri is an import concept to have is why what doe what distinguish the hardwar lotteri even sai just hardwar what distinguish the hardwar lotteri from ani lotteri Like why I sai okai the X lotteri And the X lotteri is is ani circumst ani circumst that that surround a research idea right here you have idea on idea two idea three and thei all depend on mani circumst and x is on of those circumst And it just so happen that the circumst in the world favor idea two and a differ circumst would actual favor idea on so special about hardwar other than more expens than softwar right To to to illustr thi further sai okai you have you have hardwar and you sai well hardwar is expens but then again you can sort of build a hierarchi where okai down here there is like idea thei depend on softwar like softwar framework that we have such as TensorFlow pytorch these again depend on particular hardwar But and you can sai okai the hardwar is much more expens So we ar not as flexibl And the idea might just succe becaus of the hardwar but then you can go even step further and sai well up here is sort of the consum if you like the market term then mayb sai the societi the end user and so on becaus the hardwar ultim is direct toward what human in societi need And that chang over time as well So and wai more expens to chang the need of human societi than to chang the hardwar So I can just also claim okai x is now societi So the wai to go Societi So the on particular research idea down here might win simpli becaus it is more suit to the current societ need And that kind of carri over and you might sai well make that make it a good idea that make it prefer to idea Idea two prefer to idea three over here that would just optim for a differ societi which lead us to the question what doe it mean to first what doe it mean to win Here it just sai a research idea win And you might have an idea So an idea not clearli defin here But mayb win mean that a lot of research actual research in that direct And the other question is here and not becaus the idea is superior to altern research direct And here my question would be what doe superior mean What doe it what doe it mean for an idea to be superior As I said here certainli if an idea is more in congruenc with current societ need you might claim superior And someon els might sai well if societ need were differ than a differ research idea might be suit better The same wai someon could sai well if hardwar wa differ than a differ research idea might be better Mayb you can sai if hardwar wa differ a differ research idea might be better suit to the current need of societi But then pretti sure I can go two three four level up here Again so these these term ar a bit vagu I think we can all the again the initi the initi sentiment when read thi is absolut in favor right I absolut agre I want to want to trash thi I just want to sort of I try to think a bit deeper about what is actual said here And thi is where sort of my my troubl start So dig a bit into the histor part And I think the point the paper is sort of try to make is that not yet that there ar specif hardwar choic that were made at on particular point And becaus so expens to chang hardwar That mean that a lot of research simpli go along with whatev idea work on that particular hardwar avail And other research idea ar neglect simpli becaus the hardwar avail which again thi is a sentiment that I think we can we can all agre with So the first part here the paper is is in the in the follow section And thi is import to keep in mind as a red thread becaus I feel on can get lost in the detail of the paper So in the first section section two we ask what ha incentiv the develop of softwar hardwar and machin learn research in isol We need to read thi first Thi essai begin by acknowledg a crucial paradox machin learn research mostli ignor hardwar mostli ignor hardwar despit the role it plai in determin what idea succe So the argument is that we we develop idea independ of hardwar But also we it kind of make it a doubl doubl point It sai that we think we just think about idea but the idea we might think about mai be shape by the hardwar avail And if not awar of that we might not we might not see other idea as viabl So section two ask what ha incentiv the develop of softwar hardwar and machin learn research in isol So where doe thi come from that we think about the hardwar at the end section three consid the ramif of thi silo evalu with exampl of earli hardwar and softwar lotteri So thi is the kind of risk histor look back Then todai the hardwar landscap is increasingli heterogen Thi essai posit that the hardwar lotteri ha not gone awai And the gap between the winner and the loser will grow increasingli larger So thi is a point that the paper basic make that thi hardwar lotteri ha not gone awai So right now we ar in thi hardwar lotteri And it doe so specif with regard to sai that chip like GPU and TPU and even more special chip ar optim to neural network And why the whole world sort of over focus on neural network right now and discard other research idea And the gap between the winner and the loser will grow increasingli larger mean that the research idea that ar seen as inviabl now if we develop even more hardwar into that direct into the direct of neural network those research idea will becom more and more inaccess to the commun Then lastli section four to five unpack these argument So the on that just seen section six conclud with some thought on what it will take to avoid futur hardwar lotteri Alright so section two here is thi sort of histor look back And it goe from these it the point is here separ tribe So the point is that someth ha made it such that the commun the softwar commun and the hardwar commun and the idea sai the idea commun the research in AI algorithm call them the algorithm Thei thei thei think that much about each other And it make the case that earli machin were super duper special Earli machin were singl us were not expect to be repurpos for new task becaus of the cost of electron and the lack of cross purpos softwar So earli machin earli comput machin were just singl purpos and so on But that all chang when the whole world focus on sort of gener purpos CPU that could execut ani instruct of cours accord to Ture machin or von Neumann architectur So the point that the paper make is at some point a shift happen The gener purpos comput area crystal in when an opinion piec by young engin call Gordon Moor appear in Electron magazin with the app titl cram more compon onto circuit board a cool titl So thi famous gave rise to law or predict you could doubl the amount of transistor on an integr circuit everi two year And thi sort of held true where peopl stop build gener like sorri peopl stop build special purpos hardwar but invest just more and more and more into build these gener purpos chip these CPU That and the reason why thei stop make special hardwar is ani special hardwar you build will simpli be surpass by the next gener of CPU So even if you make a specif purpos hardwar for some problem you just have to wait like on or two of these cycl and ordinari gener purpos CPU will simpli have will will overtak your special hardwar And sinc CPU ar gener purpos the market for them is natur huge So thi thi ha made it such that what wa mainli develop wa gener purpos CPU I think the paper want to make the point though not exactli sure I think want to make the point that even though the CPU might be call gener purpos thei gener purpos like thei have their specif advantag and disadvantag And go to hurt for exampl neural network in the year follow thi So in conclus to thi chapter thei sai in the absenc of ani lever with which to influenc hardwar develop machin learn research ration began to treat hardwar as a sunk cost to work around rather than someth fluid that could be shape Howev just becaus we have abstract awai hardwar doe not mean it ha ceas to exist Earli comput scienc histori tell us there ar mani hardwar lotteri where the choic of hardwar and softwar ha determin which idea succeed and which fail And the exampl is kind of the Charl analyt engin that Charl Babbag design but wa someth like year earlier or so then part could even be manufactur for thi idea to succe And we know mani stori of these peopl be ahead of their time And thei have thi interest quot I think somewher from Silicon Vallei here be too earli is the same as be wrong And thi paper of cours focus on hardwar But to come back the conclus of thi chapter is that becaus of the fact that of thi gener purpos area becaus the entir focu wa on build gener purpos CPU thi ha led to peopl not realli have integr thought of hardwar softwar algorithm but treat hardwar as thi thing that can execut ani instruct And then the algorithm come on top of thi sort of black box that we realli chang we just have the hardwar we have Yeah which which come back not and again not sure like sure that that sure I agre that the entir world focus on gener purpos CPU ha some influenc But certainli hardwar is just expens to make So you could argu that even if thi happen a machin learn research necessarili think about the hardwar but thei would at least have a choic if there were a select of right Okai So that wa the section two section three Now we realli go into the histor evid And there ar kind of earli earli histor evid like thi Charl machin that he invent An earli exampl the analyt machin in And and no it even decad it wa onli surfac dure World War Two in the first part of the centuri electron vacuum tube were heavili us were heavili us for heavili us thi is not notic a number of typo in in the paper I realiz a preprint if the author is listen I can also I can also make a list but thi thi on just pop up For radio commun and radar dure World War Two these vacuum tube repurpos to provid the comput power necessari to break the German enigma code So it would be long after not onli after Charl Babbag invent thi machin but even after he di that peopl would would sort of re take and in some part reinvent hi idea to do build modern comput The big exampl though that the paper make is what it call the Lost Decad And thi is the stori of neural network coupl with two thing with an AI winter and a focu on expert system and mayb also though not entir mention here a focu on thing like SVM So I think wide known that the main ingredi for neural network ar veri veri veri old So here the paper give some exampl back propag invent in reinvent reinvent again and deep convolut network pair with back propag by on the car It sai howev it wa onli three decad later that deep neural network were wide accept as a promis research direct I think thi thi sort of the timelin here is thi year probabl refer to around Shortli after that of cours Alex net beat ImageNet and so on But even earlier a bit earlier peopl were do heavi research into neural network And three decad later so thi is pair with kind of these number right here sai when these idea were invent present but comput back then were simpli unsuit to the to run neural network Here it sai the gap between these algorithm advanc and empir success in larg part to to incompat hardwar Dure the gener purpos comput area hardwar like CPU were heavili favor and wide avail CPU were good at execut ani set of complex instruct but occur high memori cost becaus of the need to run the need to cach intermedi result and process on instruct at a time Thi is known as the von Neumann bottleneck The avail comput is restrict by the lone channel between CPU and memori along which thei test to travel sequenti So the paper goe on and sai there were some effort into special hardwar for neural network but fund wa kind of not there And other special hardwar wa more into the direct of popular idea then like prologu and list which could do expert system and not necessarili neural network And onli onli it would take a hardwar fluke in the earli a full four decad after the first paper about back propag wa publish for the insight about massiv parallel to be operation in a us wai for connectionist deep neural network A graphic process unit wa origin introduc in the as a special acceler for video game and develop graphic yada yada yada GPU were repurpos for an entir unimagin us case to train deep neural network had on critic advantag over CPU thei were far better at parallel a set of simpl decompos instruct such as matrix multipl multipl multipl um multipl I know So the the point here is that the idea were around for a long time but it would take GPU to make them work And so the the the imag that the paper build up I think is that you have these here and you research and then you have a decis to make which hardwar do I build for the futur And there ar two direct thi is direct on and thi is direct two And sai for whatev reason direct on is chosen okai then becaus so expens to build differ hardwar the the world larg goe with direct on and build on top of that okai So that also mean that all the research idea that profit from direct on will appear to be much more effect that research idea that would have profit from direct two And it sort of sai that neural network ar over here And sort of the and the the sai the other system what do we give expert system call them expert system And other type of idea were over here And thei appear to work realli well until thei stop in progress And then by accid sort of thi road here wa travel us with GPU it wa not obviou but by accid still thi wa develop and then neural network could flourish And if it for that fluke if it for video game basic or anim we would have never known that neural network work as well as thei do So again the point the paper make And I think we can all agre with that particular point But I want to again I want to build up sort of a differ pictur right here in that why why is onli like I feel hardwar is consid a bit much here So I think you can make the gener case that at ani junction you have sever thing you can choos And then onc you choos a thing all the thing go in that direct like new idea will be more in that direct Also new hardwar will be more in that direct becaus a lot of peopl research on it the paper also make the point kind of thi feedback loop But sai neural network were down here What I would I would argu and thi is a bit of a point the paper make in in a half half formul wai I think is that it basic sai that had we had we invest in matrix multipli in GPU instead of CPU in these earli year that mean that neural network would have sort of succeed as an idea at that time And not entir convinc of thi Becaus first of all you can see right here GPU were actual around in the So the hardwar wa wa avail not not like it wa super easi in in it wa for these earli research to build their code into GPU compat code That wa certainli hard especi if you read the paper but it would have been hard in As well it would not have been significantli harder I think So I not sure if the pictur is realli like thi or if the pictur so if thi is the CPU direct is more like that neural network ar actual somewher up here And the fact is we we actual need the good CPU in order to develop dai in order to make us of the GPU right and thi here would be GPU In order to make us of the GPU to then enabl these neural network on the GPU becaus certainli it ha it ha help a lot that CPU were built that you know comput just built on GPU would be sad comput Comput built on CPU ar cool Thei can do multi process thei can do internet thei can do actual thei can do most of the video game except displai the graphic And veri arguabl that without the heavi focu on CPU we would not have neural network todai Even if we had invest all of that effort into build GPU Becaus societi ha just advanc so much becaus of CPU So sort of tempt to challeng thi notion here that just becaus of the the happenst that CPU were advanc at that time that neural network ar have their breakthrough back then I think we need both That be said I do agre with the paper that we might have never ever realiz that neural network work if it for the fact that there is special hardwar around Yeah so so that would be my my point to thi The paper make yeah make thi point about okai there is hardwar lotteri and in so now it also introduc softwar lotteri though it said at the begin that hardwar lotteri includ softwar but go to guess that a the gener concept of a lotteri wa simpli present And again I see exactli so special about hardwar becaus again I can make the same case for softwar just a shorter timefram I can make the same case for theori right Like whatev now neural tangent kernel ar ar ar the hit right like wow NTK is blah blah blah blah blah blah blah Who know right But some big name announc thi and some theori ha been done in thi direct And becaus there is alreadi a big momentum lot of peopl publish in it Who know if if a good idea or if there were other idea that had we done the fundament work in thi would flourish right now Again I agre with the sentiment I see why the hardwar is the why the hardwar is is such a special case right here So the next thing that the paper look like thi kind of the current dai So it tri to make the point that we might be in a hardwar lotteri right now And again the the intuit of cours is ye of cours we have the hardwar we have difficult to chang especi sinc hardwar build upon hardwar with the tree I drew befor draw it again go to draw a tree and liter everi decis you make in the train thi onli need to be hardwar right Everi singl decis you make will mean that pretti much all of the previou choic here ar now fix and ingrain We build upon we build upon invent of the past imposs to go back and do all of these thing again And if you do all of these thing go to again and if you see someth curiou right here and thi is where go to later I want you to see what happen if here here is a good idea Like here is my super duper booper idea And my super duper booper idea simpli make the cut for that choic Like someon chose a differ hardwar direct softwar direct softwar librari direct whatnot it in vogu And my idea wa unpopular then if on choic is made thi choic right here hard to go back if two choic ar made right that build upon each other even harder to go back So as time goe on harder and harder and harder to go back which is a point that the paper will make at the end that the differ between the winner and the loser is get bigger and bigger which is an effect that thi idea that onc wa a curios that could be investig becom a veri costli investig becaus we need to reinvent and re engin a whole bunch of decis And it at with time goe on simpli forgotten becaus so much that we have built past thi Howev thi is for the loser right Thi is the loser Howev for the winner I disagre right here becaus here it sai okai thi direct the idea direct here sai there is a super cool idea that would beat the crap out of neural network What not whatev whatev latest Schmidhub paper is that that idea would be neural network And thi here is neural network And do neural network And Schmidhub idea is just forgotten about Now to sai that neural network ar the winner and the winner will increas and increas and increas is correct but it forget that right here there is thi whole branch So within the neural network you have again thi branch and mayb over here what kind of neural network were complet forgotten like MLP no MLP ar mayb still a thing I even rememb like earli earli neural network were h nonlinear for MLP or someth like thi Nine by nine filter nine by nine filter in convolut thing like thi right We sort of the nine by nine filter ar technic in the class of neural network But as time progress and thi branch here ar the three by three filter which ar massiv out compet the nine by nine filter So the nine by nine filter ar forgotten And it could be that if the nine by nine filter no sorri becaus of the three by three filter now we have special hardwar that exclus focus on three by three filter So we go down thi rout down thi rout down thi rout down thi rout And there might have been some other super duper idea down here that onli work when we have realli big filter And now we never know that thi exist right So to sai that the differ between the winner and the loser get bigger and bigger sort of misjudg that these winner will be fraction and fraction and fraction And everi push in on direct come with cost to these other direct within that winner branch But thi is I Yeah ultim you know you have a choic you have a choic Do I want to go back and go thi direct Or do I want to add someth here it might just might be worth more for societi to go up here The paper is go to argu at the end that we should sort of keep fund altern direct in hardwar which I think is alwai a good thing to to not lock in on particular idea But also you can you sort of have a have to strike a balanc becaus you know research on thing that alreadi work and make them better is a crucial part as well becaus you can discard these sub idea that make ani sens Alright so it give some exampl of current hardwar lotteri winner To improv effici there is a shift from task agnost hardwar like CPU to domain special hardwar that tailor the design to make certain task more effici The first exampl of domain specif hardwar at least over the last few year TPU And then it also sai edg TPU core tech arm cortex m big sir which I think is just like a box with a GPU in it and some Infini band optim explicitli for costli oper common to deep neural network like matrix and then network like matrix multipli So here I have again thi doubl mean So it sai here is task agnost hardwar like CPU But at the same time it argu that CPU ar particularli bad at matrix matrix multipli not realli task agnost just focus on differ task But I see what the what the paper mean right here we do build hardwar to make matrix multipli faster which mean that neural network that benefit neural network research Closer collabor between hardwar and research commun will undoubtedli continu to make the train and deploy of deep neural network more effici For exampl unstructur prune and weight quantiz veri success compress techniqu in deeper in ar incompat with current hardwar and compil and compil kernel hardwar and compil kernel I know what that mean But incompat with current hardwar The paper argu that becaus we see that these idea ar good there will be special hardwar for them And I think the point the paper is try to make is sort of like see anoth win for neural network becaus we go down the neural network road peopl focu on neural network focu on how to prune them and so on hardwar will be develop which will lock us in further into neural network which again is paper basic sai like look becaus we went thi road right here go to go thi road a lot more But then what you have to see is that if we in if we then from thi road go here becaus we do want to do weight quantiz in thi particular wai we also ar go to neglect thi which would be do some whatev other thing that we could do Yeah So alwai alwai in each decis a branch Undoubtedli the paper is correct And it sai the branch decid the futur But I think the focu here on hardwar and neural network versu non neural network is a bit veri specif to that thing It then it make the it make the point why it matter So why it matter it matter becaus the paper sai Okai that Here In a paper wa publish call machin learn is stuck in a rut The author consid the difficulti of train a new type of comput vision architectur call capsul network And I kind of realiz that capsul network realli suit to current to current to current hardwar And he sai whether or not you agre that capsul network ar the futur of comput vision The author sai someth interest about the difficulti of try to train a new type of imag classif architectur on domain specif special hardwar Hardwar design ha priorit deliv on commerci us case while built in flexibl to accommod the next gener of research idea remain a distant secondari consider which is true though I would also sai I mean GPU CPU and GPU combin ar extrem gener oper like veri veri gener Okai GPU ar good at matrix multipli but CPU ar good at a lot of other thing I would sai the GPU CPU combo is a veri veri veri flexibl gener purpos hardwar design that lock you in too much And mayb mayb just that capsul network ar by algorithm design wai wai harder to implement like to build special hardwar for capsul network not sure if that would even be possibl and to speed them up to the degre that CNN ar sped up by GPU just out of the algorithm natur of capsul network And done video on capsul network thei sound pretti cool But thei also sound like implement the thing in hardwar is go to be quit tough even if you build special hardwar Thei also go into claim that so current the paper claim that becaus we ar kind of lock in in thi neural network thi neural network paradigm in thi kind of hardwar sever major research lab ar make thi bet engag in a bigger is better race in the number of model paramet and collect ever more expans data set Howev it is unclear whether thi is sustain An algorithm scalabl is often thought of as the perform gradient rel to the avail resourc Given more resourc how doe the perform increas And thei go into exampl here that you can scale up the paramet which give you less and less of a of a gain So like thi diminish return over time which it bring up which I find interest becaus show in a wai okai it wa in log space but it show a fairli fairli linear decreas in perplex So a log linear decreas perplex given more paramet which goe a bit against the narr of the paper and also in term of thi definit up here given more resourc how doe the perform increas I see the fact that you sai well billion sorri million to train sai right here million to train On the other hand I would sai the cost of you know build special hardwar to research altern research direct By the wai we have no idea what altern research direct work So the onli thing we could do is fund all hardwar And if we have to fund all hardwar for other algorithm then select the on that ar promis then invest more and so on million will get us nowher which I think is a point the paper is try to make But from a effici perspect given where we ar now actual more viabl to build Which again I think thi is someth the paper agre with but at the same time it tri to make the point that look we ar invest more and more and more and get less and less out of it Mayb time to go a differ rout in term of hardwar But go to be more and more expens the more we go into thi neural network direct not sure about thi Again not a good idea to invest more in hardwar but not sure about thi Again if you think of thi tree the paper basic tri to argu that what is try to do is try to make a push up here into the next kind of push the frontier on the path that we have gone for a while And the paper is try to sai that had we had we imaginarili gone a differ path down here a equal hard push in thi direct in a direct would mayb yield a better result Ye mayb but yeah But the question is is it at what point doe it becom viabl to sort of abandon thi entir direct and skip and kind of start there becaus we would need to do the whole tree thing again And then within the tree the same logic appli It doe though make a good comparison to the human brain which work fundament differ It sai while deep neural network mai be scalabl it mai be prohibit expens to do so in a regim of compar intellig to human An apt metaphor is that we appear to be try to build a ladder to the moon sort of sai that we we wait at the rate where we scale neural network right now not conceiv that we reach human level intellig by simpli scale them up which is why we might want to investig differ entir differ direct and why we might want to investig entir differ hardwar choic Yeah which you know grant correct though I would sai transform particularli suit to the hardwar becaus thei requir such huge memori and GPU tradition have been rather limit in memori in memori sorri and transform still kick ass on these on thi hardwar even though memori is extrem limit compar to like CPU memori And onli now do we see GPU manufactur focu on more memori So you can argu from the perspect of the paper and sai see becaus we have neural network hardwar now peopl ar build more neural network hardwar But also you can sai that initi a bad choic wa made sort of but research still manag to demonstr transform would work and now the hardwar is develop in thi direct Which is also a thing the paper argu at some point Again I have a hard point pars out a direct point here I think the paper is more meant to make you sort of think about the differ point it bring up which is also probabl why thi video is more of me rambl than anyth els So here it sai that current there ar some initi to build other type of chip other type of hardwar and so on But thei as well as the last on thei might be not enough becaus it take produc a next gener chip typic cost to million and two to three year to develop And even that is howev even invest of thi magnitud mai still be woefulli inadequ as hardwar base on new materi requir long lead time of to year in public invest and is current far below industri level of R&D Thi thi is the kind of DARPA and China who fund research in thi direct So the paper sai it might be wai too littl Though it also sai there ar a coupl of good light at the end of the tunnel sai experi us reinforc learn to optim chip placement mai help decreas cost And I think done a video on thi paper on thi paper There ar also renew interest in reconfigur hardwar such as field program gate arrai and coars grain reconfigur configur arrai So thi is hardwar that you can sort of metaprogram So you can take the hardwar and you can special it by program it And so like a meta program it you can sort of take on of these thing and make it into like a sort of a GPU if you need it like that and then you can reprogram it program it differ for a differ applic Though if again if I take the other side of thi paper I would sai well that the same thing that CPU were and yet still CPU made it almost imposs for neural network to run you even though FPGA ar veri gener you make implicit choic on the idea that ar veri well suit to FPGA or the idea that ar veri well suit to us reinforc learn to optim chip placement that the exact same thing Yeah I guess you can make thi argument at in like at infinitum infinitum infinit no infinit is differ Okai thi thi video must come must come to an end So the last part here sai that what is also need is kind of a softwar revolut that there is a shorter feedback time where it imagin softwar that tell research which hardwar their algorithm is particularli suit or how their algorithm would fare on differ hardwar such that if you invent a new algorithm it work on a GPU you could sort of submit it to the softwar And then the softwar will tell you what that thi would work realli well if type x of hardwar exist and then you can mayb invest monei into into that rather than discard your idea In conclus yeah it the conclus veri long The perform of an algorithm is fundament intertwin with the hardwar and softwar it run on thi essai propos a determin hardwar lotteri to describ how these downstream choic determin whether a research idea succe or fail Todai the hardwar landscap is increasingli heterogen Thi essai posit that the hardwar lotteri ha not gone awai and the gap between the winner and loser will grow increasingli larger In order to avoid futur hardwar lotteri we need to make it easier to quantifi the opportun cost of settl for the hardwar and softwar we have And my conclus is I gener agre with thi paper I realli appreci the histor overview But I do think the focu is it center too much around hardwar where I think thi lotteri case you can make for liter ani singl branch choic And mayb you weigh that by the cost that it take to revert or chang that choic in the futur And it also focus a lot on neural network versu non neural network where it kind of the thi thi winner and loser thing where it sai neural network ar the winner And if we investig more into neural network then thei will remain the winner becaus of thi feedback loop Howev it kind of in my opinion discard the thing that within the neural network in the next choic of hardwar there ar go to be winner and loser again and again and again and go to be entir branch of neural network research that ar abandon becaus thei fit the hardwar choic onc more And thi gap between what conceiv the winner and the loser it onli it compar loser in term of an idea that wa had in on year to the winner which ar alwai reevalu everi year So kind of not a fair comparison in my opinion And then also no that wa it for me Ye I do I do implor you if you ar interest in thing like thi as I said thi is more of a histor and opinion piec try to make some argument and give you some direct to think about which is is pretti cool as a chang to a simpl blunt research paper All right That wa it for me Again if still here wait for how to 
[Code] PyTorch sentiment classifier from scratch with Huggingface NLP Library (Full Tutorial)
How did it do So Hug Face just releas thi NLP librari right here and thi is pretti cool becaus it allow you access to about a hundr NLP dataset and ten evalu metric prepackag So know Hug Face thi is go to be a breez to work with So what I thought we would do is we would try to us thi I have not us thi yet and been a while sinc us ani Hug Face stuff So what try to do is us thi to load up the IMDB dataset and then us a BERT model mayb to build a sentiment classifi on top of that us PyTorch specif PyTorch Lightn So all of that combin from scratch and basic if I can do it then so can you and go to make some mistak and have to look at the document a bit and so on but the process Okai so first of all if you like content like thi let me know if not subscrib subscrib Let me know in the comment if you have ani sort of critic or tip alwai happi for VIM tip honestli So I have a pretti empti repo git repo here I have a git ignor but about it So just dive right in Start up VIM and make a file So first some boilerpl code terribl at talk and code at the same time but you know So I like to us thi AppSoul librari and us as you can see us the tab complet engin with CoC with NeoVim Thi is absolut great We easili mayb need app flag log That sound good So need Torch probabl and need PyTorch Lightn Torch Lightn as PL need the NLP librari of cours sinc go to us that and we need the Transform librari Now I know HuggingFac ha thi token librari too but there ar some token in the Transform librari alreadi and just keep it light like thi So mayb numpi mayb not see So export have these flag object here Mayb do some flag later and the main function just call hello Actualli log that info and all right Run main So thi is our boilerpl and just quickli try it out just to see whether it work So here we ar Hello fine All right So where do we go from here So in PyTorch Lightn what have to do is you have to build thi kind of model class right So build an IMDB sentiment classifi and go to extend thi Lightn modul of PyTorch Lightn So you need differ thing in the PyTorch Lightn modul First of all you need the init and just do like a veri basic init call super on it and about it And you need a forward method sinc thi is a modul So in the forward method go to get a batch and you have to do someth with it And what we also need is a train step method Train step which get a batch and a batch index and have to output some kind of loss or some kind of train procedur Then need a train data loader So all of thi you can look up in the document of PyTorch Lightn Basic you implement these method and it will do the rest for you So it will do all the train loop and it will do the handl of GPU and whatnot The whole loop over epoch all of that is basic taken care for you when you us PyTorch Lightn So last thing we need is mayb a prepar data put that up here Prepar data That method is option but it get call at the begin and go to be pretti good for us So I have download the weight of a BERT model and the data set So we need to do that anymor Yeah So about it And I am go to so mayb forgotten someth Lightn exampl what go to do go to look at like an exampl of PyTorch Lightn and just to see whether have it So mayb here domain exampl ImageNet sound good So have these method Thi is wai more than we need But down here so basic what you do is you instanti your model and we save have these hyperparamet here These will be our flag But then implement thi trainer and then you call fit on the model Right Okai So mayb copi thi down here So model Thi is our IMDB sentiment classifi and the trainer Root tier call that log GPU give it a GPU if CUDA is avail Else zero And then make a flag for the epoch We need the rest of thi And then at the end call fit model Okai So if we had a classifi thi would alreadi run Cool Now what I like to do is to have to have thi modul call sh which give you some sort of easi shell command and at the begin of each run whenev the file load I just do I remov the log folder So I have basic a clean log folder and then I make it again like thi So it just delet the log and then run them again So if we run thi right now thi is go to give us an error probabl Okai So we have an epoch flag right So we need to defin a flag call defin integ Yep And go for epoch right now Cool Okai Veri cool Okai So we configur our optim So in PyTorch Lightn you need some sort of optim configur And just copi that from an exampl Go full Siraj here peopl Okai so we need to configur optim And I kind of I like the SGD for thi SGD tend to work well in neural network We need the schedul We need ani of that So just return the SGD optim with the paramet And make a flag flag for the learn rate and make a flag for the momentum Okai we need ani weight decai right here Cool put these make float for the learn rate Mayb start off with someth like thi So I never put help string if the descript is rather clear Only loser need help Like be kid yourself If you put the help string you need help how it work All right So I just like that thi librari forc you to put the help string becaus it somehow make me feel bad Becaus veri opinion right It sai basic well you should put someth there Okai okai okai okai So we have thi And now when we run thi we have anyth to optim yet So first of all we need the model right Do we need to prepar data first check So I have thi short thing snippet here that emb an IPython shell And I just plug thi into anywher so I can see if I reach it right so I reach the prepar data So care about the data set first Thi is why here right So it is NLP librari as you can see right here mayb So the usag right here So you can load a data set here with the I think even with the appropri split And it will basic just give it back So if you have it it will download it pretti cool So just load the data set And alreadi sort of check out what thei have and thei have the IMDB data set Okai And in split in thi split argument we can sai give me the train split And as a string you can sai give me whatev the first of the train split Sinc we be like thi is just my laptop here So we be abl to train like a super high grade model But go for of the train split So thi is to train data set Right And now if we see if we run until here so if you had not download thi it would download thi So given the train data set I hope you can see thi So it sai a data set it ha row and it ha each entri ha a text and a label And if you look you can just index thi like a data set And the first sampl right So the label is on here mean that we should predict the label that thi is a good sentiment right either on or zero mayb Mayb yeah I think so So either good sentiment or bad sentiment Okai so our first task is go to be basic to get thi into a form where BERT can consum it So how do we do thi with thi NLP librari And the pretti cool part So right now you see thi is text So in NLP we need to map thi text into token ID So we need to token and we need to map thi to ID and Hug Face of cours ha veri nice librari for that call token So have on of these token And us thi from the transform librari And I think thi is call BERT token that then the BERT model can us check it out Okai at the document So BERT token There we go a BERT token fast Ye Okai take the take the fast on Mayb not Yeah take the fast on Come on Be riski BERT token fast And I think we can do thi from thei have these method from pre train Ye Right So take thi from pre train And put the model name here Now I want to make thi a flag such that not bound to a particular model Oop Cool So thi is call model So thi is our model the BERT base on case And we have a token right now So what we can do is we can now token these thing these everi entri in the data set Now in a classic set have to you know write a loop for that But with thi data set librari with thi NLP librari pretti cool that we can token basic each of the sampl we can map thi token function across the the train data set So how do we do that We have thi token And the token ha pretti sure it ha like a token or an encod or someth method So forward Now thi is the BERT model the BERT token Right here Right here Okai it ha it ha pretti sure it ha thi encod or someth Here Oh yeah encod right And code Where is the definit of that Can we click on thi Okai cool So thi encod take text and it take a bunch of other argument such as I hope you can see thi There we go Such as whether or not you should add the special token or the max length thi is go to be pretti import And pad to max length we want everyth to be of the same length So if you appli thi token encod function to a text of these sampl So just take the first sampl here And take the text entri Then what go to get is like a list of these ID Thi is exactli what we want So the here is thi CLS token that BERT take in And then just the word piec right So you could also sai instead of thi sai token I think And that will just give you the word piec not the encod yet Right So these ar the word piec right here Thi is the token text And with the encod function it doe thi and then map these two ID such that BERT can consum that So for thi NLP thi librari ha thi conveni function call map in their data set So what have to do first is defin a token function that take in a singl sampl And then it will run the token encod function across the text entri And we have alreadi seen we need like so add special token is true Thi is cool Max length ye And make a flag sequenc length or someth And we ar go to pad to max length is true So everi singl sampl will be of the same size Now in thi function a number of wai what you can return here So on wai is to return the origin sampl and actual just set a new attribut I think Set a new attribut on thi origin sampl right here format thi a bit nicer So see we have thi token function take a sampl take the text token it encod it and put thi as the new attribut input ID and return it again And now what we can do is we can map thi function across the train data set like so So thi will go over the train data set And basic each for each entri do thi thing So hopefulli after thi oper have a data set with where with where each sampl not onli ha a text and a label but also a input ID attribut And we have thi sequenc length thing right here So put that here just go with Sinc it thi is just my laptop so sampl should be fine So here it sai pickl token object So what it tri to do right here is it tri to it tri to parallel basic thi thing right here So if we look at thi NLP thing is there document to thi We can just look at the data set mayb name split builder arrow data set map right here So thi function I think it will try to multi process and therefor it need to basic pickl all of the thing that go into the into the into the I speak todai it pickl all of the thing that go into the function which mean thi token right here it need to be pickl Now mayb we can keep in memori load blah blah blah Mayb a wai to get around thi So on thing we can try is we can try anoth token Mayb thi on can be pickl The still librari is pretti good but it pickl everi everyth Ye so thi token can actual be pickl In the other token So not entir sure what have to do honestli becaus I know the librari But what you could do is like make a thread or process local variabl of thi and basic make it a singleton in each process And then basic in here you call the function to get it and it return the alreadi instanti object and so on If you realli want to multi process all of thi Anywai we have thi train data set right now and you see the schema If you can see thi the schema ha been extend So now text there is label and there is input ID which is a list of int thing pretti cool So now what we can do sinc thi is still a Python list right thi is still a Python list I know the token can alreadi output pytorch tensor but kind of cheat So we want to us thi librari right here So we want the train data set there is a method call set format right here And you sai type equal torch And what that doe and I think you need to sai which column you want So we want column Mayb we should get all column can we output the text So you can select from the sampl which of the column you want And check it out again For now as long as just debug here I like to do a debug flag So thi is usual on of the first flag I do defin Boolean debug And what thi doe is whenev thi is activ I try to be as fast as possibl So there in thi pytorch lightn trainer actual thi fast dev run argument which doe the same thing but I can push it a bit harder with thi debug here So let me sai thi is like thi is sorri thi is like on Yeah We just load like batch size sampl If we ar in debug mode Batch size we actual have a batch size argument yet do we If flagsdebug els Okai So we have batch size yet sure gonna need that at some point So go with a batch size of Just becaus we can So now if we run thi in debug if we run thi in debug we should Okai ye thi need to be a string Shagaboom Cool So it sai the fast dev run And if we run it in debug it just load veri few data point So thi map function here take thi whole while Mayb a wai you can stream that I know For now thi is pretti good So if we look at the train data set again you can see that it ha the same entri So thi is still a list of But if you index it right now if you go to the zero of data point okai then it crash Becaus it tri to convert these two PyTorch tensor and it convert the string So have to sai we just want the column input ID and we want the label Label spell Okai try it again So right here you see that what we get out is actual a PyTorch tensor for thi and not kind of Python list anymor So thi is now pretti thi is on to on So with duck type mayb even subclass Thi is a PyTorch data set right which we can load into a data loader So thi is a perfectli fine data set So we can now sai self train data set is thi train data set Now we want to do thi for the test as well But in order to do that we would have to write all of thi code again which not realli in the mood So just loop it creat a function prepar data set And take in the split name right like thi And just go with the split name here That should do it And we just call it data set Data set shook a boom shook a bang shook a boom and return that So now we can sai train data set self dot test data set is prepar data set for train and test Okai Excellent So now we have a train data set and a test data set So here in the train data loader we need to take the train data set and construct a data loader from it Thi is super super easi So what do is do it in on line Data loader So what doe the data loader the data loader need a data set So the prepar data is call at the begin So we have thi data set right here And I think we can go with a batch size right here And we alreadi have a flag for that And I think there is like a drop last Ye So the drop last will go for true We onli want full batch dure train and also shuffl Okai And the same goe for we need a valid data loader for our valid set So in pytorch-leightn you have train valid and test and test Test is realli onli for like the final final test If the test data set we have here is the would be call the valid data set in pytorch-leightn So we fals here fals we want to shuffl particularli Okai so we have a train data loader and a valid data loader Now what do we need We have optim Veri good Now what do we need All we need to do is to actual pass our data through the BERT model So thi forward thing here just go to leav not implement Mayb we can implement it Okai so we do need a model As you can see right here thi batch sai thi batch is go to go right here Right So if you know if you sometim know what to do you just go to where you should be Okai Ultimat empti paramet We have paramet yet All right So what do we do We go up here and we make a model We need to actual make the BERT model right here So from transform we can us the BERT model Now thei have a lot of BERT model and go back right here to the BERT model becaus thei as you know BERT is just an encod So we need to build a classifi on top of BERT but thei alreadi have done thi So thei have a bunch of BERT differ configur And the on look for here would be thi BERT for sequenc classif Right Thi is BERT model transform with a sequenc classif or regress head on top Right So thi is exactli what we need Classifi on top of BERT And we can I think we can also load thi with thi from pre-train and just put in the same name So we can thi BERT for sequenc classif and load up the same model that we had Okai So thi is our model Easi as that So what do we do with thi BERT If we put in data what happen For that we quickli go back again So in the forward method we can input the input ID Right Which is batch size sequenc length tensor We can input the attent mask that basic tell you where pad and where there Mask to avoid perform attent on pad token mask valu select in for token that ar not mask for token that ar mask Then we can input the token type ID which we have here We just have on sentenc But usual in BERT you have the capabl of input two differ type like a question and a paragraph or a first sentenc and the second sentenc Posit ID ar option Blah blah blah Blah blah Blah blah None of that Okai We could also input the label These ar option and it would alreadi comput a loss for us Which we almost cheat So just focu on put in the input ID and I think go to be enough Sinc we basic truncat our long text to token we need to worri about mask right here Otherwis you would input a mask for actual we can do it We can do it Okai So what you could input a mask for basic where your token ar not pad token and the pad token in BERT ar So basic your mask should just be non-zero But mayb also your model learn to ignor the pad token I might be wrong here and it doe it automat Right So in your forward pass what do you do Actualli go to the train step put someth here You can see it So if you have BERT it would actual BERT you up It would download BERT right here But sinc I have it you can see here thi is the smaller BERT model PyTorch Lightn I have enough space in my consol right here but it would give you a nice overview over your model How mani paramet it ha what kind of layer it ha and so on So we also need a valid step if we have a valid data loader Valid step and we need the valid epoch and function So usual in train you realli care about epoch too much becaus you just have mini batch after mini batch But in valid what you want is kind of on singl metric across your entir test data set or valid data set And therefor you sort of in the valid step just kind of output thing You output local thing per batch and then in the epoch and function you aggreg them into on big number So just put thing into each thing thing thing So pretti sure go to end up in the valid step first becaus if especi if we do thi debug run it tri to run a valid first at the veri start of train So we can look at a batch right here So a batch The batch seem to be a dictionari If you look at it kei we have a label and input ID OK So pretti cool So if we go for the input ID that give us a tensor and the tensor of shape eight which is our batch size and which is our sequenc length And we should be abl to pretti much input that into the BERT model that we creat Boom OK And what do we get out We get out a tupl and the first entri is go to be thi look like logit OK check the shape And thi is eight So thi is our batch size and two is the logit So on for the neg class and on for the posit class Thi is thi we can basic input into a cross entropi loss given our label So we also have our label here and their label is all on Nice Is thi mayb sort Is the data set sort into good and bad thing Becaus that would be bad In ani case So what do we have to do So in the forward method we get the input ID sai we get the input ID and we run thi through our model And we can actual construct a mask here and the mask is go to be wherev the input ID ar not zero And that as a what doe it need to be So these mask thi attent mask is go to be a float tensor OK So put it as a float tensor Cool Right like thi So our logit ar go to be that and yeah tupl with on entri So the comma here is import go to return the logit So thi is our forward function So in the valid and the train step the first thing we got to do is we got to call thi forward function with the input ID and these of cours ar in our batch like thi So these ar go to be our logit And then in the valid what we want to do is we first of all want to comput our loss Right So we have to construct thi up here in the init We can actual fold thi prepar data Loss is go to be a cross entropi loss Ye that exist with read reduct I like to put reduct none I think there is like a deprec reduc and there is like a reduct where you can put mean or someth I like to not reduc the loss at first becaus then I can aggreg I can us the same thing for valid and train So in the valid step I just want to comput my loss right here with self So loss loss and have to cheat a bit So look up the cross entropi loss and where is the cross entropi loss Cross entropi loss it take ye a reduct And so the input to the function that we construct is go to be first n by c first the input and then the target So first the logit and then the target right Criterion that combin log of max and NLL loss over a singl class Nice nice nice OK OK cool So first logit and then label label OK our loss So if we check out what our loss is go to be probabl go to be an vector of size becaus we have reduct none Loss ye C vector of size veri nice So we can just basic return sai we can return thi loss just as is And then in the valid epoch end the output here is go to be a list of and everi entri in the list is go to be on of these valid step for on batch so we can aggreg those So loss is will concaten them sinc go to be chunk of output at the dimens zero and then we can calcul the mean right So we can do that and then we can Oh no we need to do more We also want to know the accuraci right So the accuraci is go to be whether or not the logit dot arg max is equal to the label So the accuraci for each sampl is go to be that either go to be or and we want that as a float So here output a dictionari with loss and accuraci All right Excellent So here then we can aggreg so the loss is go to be and I like to have like a construct here that aggreg thi still So we go out loss for O in output So these ar now go to be entri Each on is go to be a dictionari Right So our loss loss we have concaten to the mean OK Our accuraci is go to be the same thing for the accuraci Nice So our output here is go to be a dictionari And I think in PyTorch Lightn there if you put valid accuraci like valac it select the model accord to thi but not sure So also in PyTorch Lightn I can now output thi here but also if you have a log entri it will forward thi to the logger which we can actual do and make a TensorBoard logger out of thi So what have we done We have first of all set up the valid step So the PyTorch Lightn is go to run through the data loader for each batch do thi So we forward it through the BERT model to get our logit and then we comput our loss by the cross entropi loss of the logit and the label We also comput our accuraci by see how much the logit agre with the label or the maximum logit And then we aggreg all of thi over the entir epoch and output that Now set up a logger So for the logger we can put thi I think in the trainer here PyTorch Lightn logger dot and I think there is a TensorBoard logger Pretti sure PyTorch Lightn is there TensorBoard no PyTorch Lightn logger pretti sure that exist not the newest version I hate these old doc So latest Come on Thi wa call log logger Logger Port logger Right here Nice So our save here is go to be call log and then what do we want We want the name IMDB and also thi version thing where if you put version zero it will just make a new kind of folder each time I guess we delet the log anywai We delet the log folder at the begin so we have thi problem But I gener like to overwrit my log and not make new run But if you like someth differ fine All right So run thi again And cool Thi is the BERT configur that we load And then we have no attribut logger PyTorch Lightn logger Logger Okai Again Load the weight Veri cool Blah blah blah Blah blah blah Blah blah blah And in an IPython shell And do we have an IPython shell remain Only in the train step Okai So at the train step right here And we can actual we can check whether or not ah now we have lightn log and log might just ah Okai So thi appear to be our TensorBoard log So we ar mayb abl to run the TensorBoard here later run it Log We have TensorBoard Okai Oh yeah uninstal it becaus I wa angri at it Oh come on go on I have TensorBoard I should have TensorBoard somewher like in local bin or someth Local bin No not in local bin find it figur it out How to get a TensorBoard Mayb we need to instal TensorFlow Well gonna take a while Okai So back to the train step In the train step we basic need to do the same as in the valid step So need to forward our batch through the model But here we need to comput an accuraci but we do need to comput a actual batch loss that we can back propag on Now in the train step you can either specifi how you back propag per se or what you can do is you can just output thi log loss attribut and then PyTorch Lightn will basic do the back propag for you We have the TensorBoard now pleas All right there we go And we can put thi into a differ thing right here Git NLP demo Ye Okai So thi is run And if everyth goe correctli Six Shaboom We have a TensorBoard Okai So we need to forward our train step and we need to calcul a loss for the batch So these loss here we do the same thing but here we call mean on it So thi is the mean loss from thi batch and we can now return the loss right here And we can also in the train step you can also output a log dictionari and output the loss again here in order to thi is go to be our train loss that we output right here call it train loss And thi also will go into the TensorBoard So if we run thi right now we have an IPython shell Simpli by output thi loss attribut we alreadi instruct PyTorch Lightn to now run backprop on thi loss us the optim that we have defin Okai And by output the log we instruct to put thi into the TensorBoard So now we have a scalar entri and you can see thi it onli contain the valid node It contain everyth Veri cool Veri veri cool So remov the debug flag and just see what happen So to recap right to recap we have oh now I go see epoch on epoch two go go go go go Ah veri cool What done is set up thi PyTorch Lightn modul It need a bunch of function but in the init basic just set up our BERT model from the Hug Face Transform librari load in a pre-train BERT model that go to fine tune The main thing that the PyTorch Lightn modul need is a train step function where you defin what it should do with the data and thi data loader function So in the data loader function load up a data set and we basic specifi the batch size Thi is veri easi Where doe the data set come from We do it here in prepar data Thi is a special function in PyTorch Lightn basic call after the init but befor anyth els run And here we ar load thi data set from the NLP librari and thi is kind of the magic part We specifi the split and the size that we want insid of the string and you can do thi in percent or in a number of sampl that you want sort of sure you can do more thing but I explor that Then we run map on the data set in order to token it and right here and we us a token again from the PyTorch Lightn and just run thi encod function Thi is veri simpl How complic wa thi just like a year ago Crazi Then we need to put set format and set format tell the data set how it need to output it sampl and we tell it pleas output torch tensor and we want these column right here And we make a train and a test data set from the train and test split accordingli So we have thi Thi goe into a data loader PyTorch Lightn will take the data loader and run train on it us thi train step function In thi train step function we get a batch In the batch there ar these two column that we specifi previous input id and label The input id will put through the forward function of the model itself Thi is the forward function We construct a mask and run it through the model We actual need to construct a mask but okai And we get back the logit of the classif And then we run thi through a cross entropi loss Get the mean of the batch and there we go In the valid step we do the same thing but also calcul the accuraci but calcul the mean We want to keep it per sampl and onli at the end we want to concaten everyth and calcul the mean If done everyth correctli you see right here our train loss goe down down down until it is almost zero becaus just and the valid accuraci is super high Is thi becaus all the label ar equal Like for real Okai so do someth els make an integ with percent and thi wa five right So we load five percent of the data set but load some more And thi might take longer but load percent of the data set and just see what happen No Present I call it present Pretti good So load up percent of the data set and do the same thing and we can track in real time what happen in TensorBoard And unrecogn instruct format Okai Can we make a format string in a format string Thi is nasti Doe it work Pleas work We can make a format string in a format string Absolut bonker Okai so it take a littl bit longer and you could actual I think you can speed thi up thi map of the data set Mayb you can stream it pretti sure you can batch it You can do batch process of thi but for our case right here we think enough So it wa like what if we had five percent So now it should be someth like So continu with the recap of what we did here We have the train data set the valid data set and on ye so we have everyth like thi The configur optim You can put an optim You can also put a learn rate schedul if you want to And then in the main function we load thi PyTorch lightn modul and we specifi a trainer and the trainer we tell it you know the max epoch and so on And we set up the logger and we just run fit on thi model And thi run epoch of the model and after each epoch it doe a valid epoch and minim our loss Veri cool Veri effect So now if if pleas if you would All right here we go Thi is my laptop train BERT All right Okai We seem to make too much progress check the tensor board Train loss goe down Train loss goe to zero I have the sneak suspicion that thi is not entir shuffl So is there like a shuffl like a shuffl thing Becaus thi seem a bit thi seem a bit bit fishi Thi IMDB dataset right here Just seem like you know we could us a bit of shuffl becaus all the label yeah the train loss instantli goe to zero So mayb mayb not We can we shuffl here look at the load dataset function Load dataset batch No Keep in memori No None of that Thi doe not seem to go to continu Right here Dataset NLP Dataset I hope here we know We should find thi load dataset somewher Builder featur load Load dataset Split Can we not shuffl anywher search Shuffl Builder Okai So gener call Thi function preprocess exampl Kei will be hash Okai we ar not abl to shuffl thi Just like that But I hope thi at least give you an impress I guess if you were to take the full dataset and map it then it would actual work just try again with of the data just to see it go down TensorBoard See thi is now good becaus we alwai delet the log folder We have ani remnant old TensorFlow log All right Come on Come on So should be about thi About thi Okai Train loss Look good Look good So far Look at these model How larg is that How larg is the BERT base case Hug face pre-train model Pre-train model BERT base on case the on we have layer million paramet Easi Easi No too larg Train loss goe to zero again Okai so determin that thi dataset veri probabl entir shuffl It might just have all the good label first and all the bad label last And yeah just to confirm confirm thi right here go with But put an IPython shell down just befor we map the dataset So we have to go through the whole map procedur Actualli that would be here right Ye Can we not map thi asynchron Map I might be do someth realli wrong with thi librari but I think how it should go So map def map right here We can do batch We could do batch And then I think hug face ha a function to encod batch encod Encode batch No go to the token Shabada bum Build input creat token type ID get special token mask save Where is encod Right here Can we have batch encod Build input No Thi might be it Batch encod Ye there is a batch encod where you have batch of these thing So okai what if we do the neg on See the label zero pretti sure pretti sure Batch true do that And in our function here sai batch encod So see how fast thi is with a hundr percent Where token ha no we just had batch encod Oh but thi might be we have batch encod plu Batch encod plu Or text pair Okai We need thi batch encod plu But then that give us a dictionari right Thi give us a dictionari with the field input ID right here So like thi how about that And then we still might want to limit the actual data set onc we have map it becaus we need to train on it as well I just want to see how fast thi batch encod is Ye Okai Reason fast but it take like three minut Yeah So we go on here I will put thi as is on GitHub and I hope you can profit from that in ani wai you want The Hug Face site ha a tutori on squad where thei also us the metric So thei have basic these predefin metric like blue or roug I think And you can just us them as you us these data set So veri veri veri conveni to work with these thing in NLP So NLP ha come a long wai Absolut invit you to check out the transform and token and NLP repo And with that it for me I think I hope you enjoi thi Again leav a comment if you see improv or if I mayb should edit thi a bit more I thought the entir process of just go 
Language Models are Open Knowledge Graphs (Paper Explained)
Hi there todai look at languag model or open knowledg graph by Cheng Wang Wang Xiao Liu and Don Song Thi paper on a high level propos to construct knowledg graph which is a structur object usual built by human by expert either fulli manual or semi manual with heavi human involv It propos to construct knowledg graph automat by simpli us a pre train languag model togeth with a corpu to extract the knowledg graph from The cool thing about thi paper is that there is no train involv So there is no model that learn how to construct a knowledg graph The entir knowledg is simpli extract from run the corpu onc so on forward pass through the corpu through the pre train languag model and that construct the knowledg graph So kind of the the core messag of thi paper Thei sai thi paper show how to construct knowledg graph from pre train languag model without human supervis And it turn out the wai thei do it it work pretti well on kind of standard knowledg graph construct benchmark So the the paper in a nutshel will go through all of thi includ I have a bunch of critic but it is a pre print Rememb thi And yeah so usual sai at thi point if you like thi content hesit to share it out and so on Todai go to try someth differ in Stop sponsor time Thi video is sponsor by tab nine tab nine us deep learn to help you write code faster What could possibl go wrong if you do that No joke joke Take a look at thi piec of code here I wa try to refresh some elast indic And as you can see here all I said wa could and tab nine complet it to could not refresh Becaus abov I wa try to call a refresh method Thi is someth that I seen ani other complet engin do yet compar to a regular code engin tab nine is train on lot of open sourc project And it combin thi with your code and it predict what you want to do compar to predict possibl which is what a classic engin doe Tab nine it us a GPT base model And it download that model onto your machin So the code never leav your machin There is an opt in featur where you can run that in the cloud And that will just give you a bit of a better beam search and better qualiti predict And it save you a bit of RAM As you can see I myself us tab nine I just have it on by default And pretti happi with it I us it through CFC integr into my neo vim But you can also get it in sublim atom IntelliJ VS code even like Jupit notebook and you can us it togeth with classic complet engin So you can realli get the best of both world So whenev you see me code in a code video look out for thi TN marker next to the complet the complet by tab nine it onli work for Python it actual work for pretti much ani program languag that complet obscur If you go to thi link within hour of when thi video is releas get three month of tab nine profession for free The profession version remov the project size limit of the free version And it also give you access to that sweet sweet cloud infer After the three month automat kick out of the pro version no auto sign up realli noth to lose I mean the onli bad thing here is that tab nine itself is written in rust If the worst thing about an offer a pretti good deal Again I us thi myself and pretti happi with it So again if you sign up at tab ninecom slash promot slash Yannick cultur within hour of when thi video is releas get a free three month of tab nine Pro no string attach and now enjoi the video Thank All right I hope that wa fun get back to the paper get into the paper So first of all what is my first critic of thi paper Ye the titl there ar some disturb trend in the last few year in in in machin learn paper and the disturb trend can be mayb encapsul with the phrase is all you need So peopl have sort of sinc attent is all you need sinc thi paper peopl have discov that if thei just append thi to whatev their paper is about then the paper will get much more notorieti And the same thing I think is a bit at plai here with thi with the R becaus in recent time kind of seen a bunch of paper that show equival between model such as famou exampl is that the transform ar hopfield network in some kind of in some regard And it these paper ar pretti cool right Even if the two thing ar not exactli equal all the time if you can sai look there is a set there ar you know under these assumpt under these set in thi situat these two model actual ar the same a pretti cool recognit pretti cool thing to show And veri us for academia and and practic I believ Howev I believ the R keyword the is keyword should be sort of reserv for when two thing ar equival Wherea here in the veri first at least honest right In the veri first sentenc thei show thei sai well we show how to construct knowledg graph from pre train languag model So essenti go to us a languag model to approxim construct a knowledg graph And also go to us a bunch of other auxiliari model that come all pre train but still thei do not show an equival of languag model and knowledg graph in thi paper not at all So I would sort of I see that you can get somewher with these titl But yeah mayb peopl will be disappoint kind of if thei read the paper which it is actual a cool paper believ me All right So as I said what we have usual is a corpu Okai a corpu is simpli a bunch of text piec you can think of mayb just the text in Wikipedia Okai Here you know the thi Wikipedia page about Bob Dylan Bob Dylan is a songwrit wa award the Nobel Prize sign Albo Grossman These ar easi sentenc right there there can be sentenc ar usual larger and longer and so on And what you want to do is you want to extract a knowledg graph So the knowledg graph ha two distinct thing it ha entiti and on entiti here would be kind of Bob Dylan songwrit is an entiti Nobel Prize and it is an entiti you can sort of think of them as noun okai And then the second part in knowledg graph ar the relat here occup sign award receiv and so on So the relat connect to entiti alwai call a head of an entiti of a tripl So ahead of a fact which in thi case is Bob Dylan three time then there is a tail which is sort of like the object of the verb And then there is the relat which is describ by the verb So here you can see there ar two stage of construct such a knowledg graph ani system that doe thi probabl goe through these two stage So first you extract a set of candid which not the knowledg graph yet becaus these ar still string right you extract direct a bunch of string triplet as you can see here And as we said as the sentenc get more complic it get more and more difficult to extract these kind of tripl and then the second part is that you need to map it to a to a scheme to a to a schema And these schema ar usual defin by human So here still go to reli on human to defin the schema So there is on list that sai entiti And the entiti there ar just the entiti ar list okai by the human And at some point it sai Bob Dylan Bob Dylan and it ha a bunch of mention of Bob Dylan associ with it And it ha a clear ID In thi case you see the ID is q in that knowledg graph And the system not onli need to extract these fact but then also map these fact to the correct entiti Sorri map these fact to the correct schema entri Thi second stage right here is a bunch of standard task So especi map someth like the the word Dylan in it context to thi entiti Bob Dylan which you can think of it as like the Wikipedia page of Bob Dylan right how the system usual work That is a task call entiti link okai entiti link and similar task exist for like for sign like the relat award map thi to award receiv to thi So mayb some kind of a dictionari entri award receiv and what it mean and a bunch of exampl And suppos to map thi to that These ar standard task And the system that we ar go to look at right here is not not much concern with these task It simpli us pre exist method to do these thing So the system look at todai doe thi first part right here It take text okai thi is text And it come up with these candid fact about the text whether how thi is then map to the schema that is a differ question And so there there ar pretti cool thing in thi paper about thi step But first go to look at the first step and then at the second step All right So how doe thi system do thi And how doe it do it that there have been machin learn model befor But be machin learn thei all have like some sort of a train corpu where you have kind of the fact as a train set And then you have a separ set of fact as a test set And you try to learn from the conjunct of the text and the train fact how to extract fact not thi system Thi system simpli us a pre train languag model So the reason The reason is the follow We us to think that we could do NLP probabl best with have a knowledg graph right with have thi set of veri structur data we can answer someth like the the ag of Barack wife and then you could go to the entiti of Barack Obama you could look at the relat spous you could go to Michel Obama you could look up her birthdat which would all be structur inform in thi graph So you could sort of answer question like thi and search engin like Googl and so on that thei have thi built in So there is kind of a knowledg graph entri sometim when you search an entiti in Googl that pop up And these have been veri us to answer question like thi Howev in recent year languag model have becom better and better thing like BERT or have becom better than these expert system call them at answer question By the wai if you want to if you want to hear a veri veri cool and solid argument of where these kind of expert system where thi kind of structur human annot or mayb extract inform can still come in in natur languag understand I would recommend the machin learn street talk episod we had with Walli Saba extrem interest person And I just I can recommend listen to that thi should be out ani dai now if it is not alreadi So the languag model have becom better and better at these task without have thi structur inform So the hypothesi is mayb these languag model can alreadi contain the inform necessari to construct the structur fact becaus the structur fact is what we you know sai should us to answer these question becaus we feel that structur inform is better than unstructur The languag model ar pretti good at these task So mayb we can get the structur inform out of the languag model So what thei do Thei sai the contribut ar as follow We show how to construct knowledg graph from pre train languag model The knowledg graph ar construct with a singl forward pass of the pre train languag model without fine tune over the textual corpora I think thi is the thi is kind of a veri strong point about thi paper And also show that if some PhD student somewher and you necessarili have the resourc to train the next model or even fine tune it there is still research to be done Simpli if you have enough resourc to forward pass your data which is often much fewer than to train on you can still do veri cool research I think thi paper show thi explicitli Okai Thi help research explicitli understand what the languag model learn bridg the deep languag model and the knowledg graph commun through enhanc model transpar Okai thei sai we propos an unsupervis two stage approach MAMA M-A-M-A which stand for match and map to first match the candid fact in the corpora with the knowledg store in languag model the first step we look at then map the match candid fact to both fix and open schema to produc a knowledg graph And then thei sai thei produc a new type of knowledg graph which simpli is the fact Sometim the fact thei extract thei realli map to a schema entri And go to look at that becaus I think a bit critic of thi name the open knowledg graph consist of map fact in the fix schema of exist knowledg graph annot by human and the unmap fact in the open schema that ar new in the refer knowledg graph schema So what thei claim here is that their system is find these new relat that even exist in the schema and is abl to uncov kind of build new addit schema entri And thei call thi the open knowledg graph a bit skeptic of thi as go to see So the first step how do you come up if you have a sentenc And thi is a veri poor exampl I feel honestli to do thi I get it must be short but a poor exampl But stai with me So you have the sentenc Dylan is a songwrit and you would like to extract a fact from thi The paper is not realli written clearli on how I mean it is I could you can pars it out But the descript is kind of distribut So step on step on is run spaCi run spaCi Thi is a standard kind of librari for NLP to extract noun phrase or thei call them noun chunk Okai So step on is not noth to do with the languag model it is simpli you want to find the noun phrase in here The noun phrase ar Dylan and songwrit Now these noun phrase now defin your head and your tail of the fact So you alreadi have two thing right So the the entir task of what of their method propos is so the step on is run spaCi to find the head and the tail of fact Step two is question mark for now Step three is go to be us the entiti link system and the relat link system to construct the knowledg graph Okai so step on is steal underp And then step three is profit So step two step two is obvious step two is where their system come in Step two is here is the head and here is the tail in the text some hat where in between there might be a relat and we need to figur out where that is So how doe thi method figur it out You alreadi see the assumpt here ar veri veri restrict right So you us spaCi to extract basic noun phrase which mean you probabl alreadi go to miss a lot of thing that ar not recogn as noun phrase And thei all thei also sai that that spaCi annot ar sometim error prone And why thei miss a lot of thing And then secondli the assumpt that the relat must be in between the two thing textual Now you can run the algorithm forward and backward but still it must be in between And it must sort of be encod sai as a semi accur string in there I guess then up to the relat linker But alreadi these assumpt ar super constrain in the the kind of thing you can find And see in the experi that their biggest flaw is that thei have a veri veri low recal I mean so do all the system on the task appar but thei still have a veri low recal And becaus thei constrain their problem so much go to guess if thei constrain their problem so much then thei would have mayb a better recal but their precis would just plummet becaus these these thing if you let them run wild thei just over extract So basic everi everi set everi verb in everi sentenc is go to be a relat right So like I at a banana I at banana would be a tripl not necessarili a realli valuabl entri in a sentenc In ani knowledg graph though banana ha a lot of carb So I would want to know about that Okai so you see that the task is now reduc from build knowledg graph to simpli given a head head annot had piec in the string span and a tail span Extract ani span in between the head and the tail that describ the relat between the head and the tail So the wai thi algorithm doe it where it us the languag model Okai So here go to do someth that is go to be similar to dynam program If seen kind of the dynam program and search algorithm sai you know string match algorithm and so on Thi is go to be sort of similar in that what go to do go to start from here from the head in the string there could be text befor it right simpli go to locat the head Dylan right here and go to start then go to look at it attent matrix Now the attent matrix is go to cross out here the attent matrix if you have done mani mani video on attent the attent matrix basic in a sequenc mean how much each token attend to each other token right How much inform is kind of sent from each other token to thi token right here So thi up here would be the queri and these would be the kei the attent matrix specifi that So sinc we locat thing between the head and the tail what we want to do is we want to cross out we want to disregard everyth kind of behind the queri and onli look ahead in the sentenc Okai so why the sum of the attent matrix here is cross out As you can see these ar the Thi is exactli becaus we onli search in on direct So from each from the token Dylan we can look at three thing we can look at is a or songwrit And thi question is simpli where do we go next with thi algorithm right no interpret yet simpli where do we go next And the where do we go next is simpli answer by just take the highest score thing in that column of the attent matrix look at the attent column where of the token Dylan I take the highest score on point three here is higher Okai then I go to point three and that mean is get into my candid fact Okai And onc I put is into my candid fact I then go to is so the next thing I do is I go to is and then I again look in the correspond attent column And I see now the biggest entri here And the biggest entri is point four which is songwrit And you can see here now we skip the A how we leav out some text okai by skip it basic So you can see that thi thi can creat artifact right Thi can creat like kind of hole in the middl and so on But we skip a we go directli to the point four and then we discov up the point four that is our tail So now we put our tail into here And sinc our tail is the last word we can stop the algorithm I ye so there is no need to go on even if there were text behind the tail As soon as we ar at the tail which we alreadi know right given the head and the tail we stop Alright so the we simpli go forward with alwai the biggest entri in the attent matrix until we reach the tail the algorithm Thi thi there describ here But kind of describ in thi in thi wai where it ha these action like start yield and like thi mayb not understand someth but it seem complet unnecessari to kind of describ these action And it basic start the search from the head the head is ad as the initi candid and so on then in yield it sometim sai with the largest score from the attent matrix is append to the end to yield the new candid and so on But still and then stop we stop And the algorithm descript here it basic just sai while not done if if not the stop action we continu sort of it tell you anyth like thi is thi is a super unclear descript of thi algorithm Basic the whole logic that you would want to know about is here in thi action manag right So the action manag that give you the action is do the actual logic of figur out which token you know you should do next and where you should go next and so on Thi is nowher in the algorithm the algorithm just describ beam search So you can do thi a littl yeah the littl more sophist that come in is that you do thi determinist but you actual do it via beam search Okai But you can you can just gener thi All right So the descript is a bit floppi with the whole action and action manag and whatnot And not describ the onli thing that describ formal is how actual to select the next token which is basic the entir kind of meat of the algorithm In ani case you might thi is someth that confus me right here So fair enough you know thei sai here we take the attent matrix and we cross out these All right But thei sai thei can take thing up here right thei can take thing like Bert and you know as I said fair Bert ha a full attent matrix everyth attend to everyth But thei can also take thing like Now is an autoregress languag model That mean that in if you look at it then you produc each token on after anoth which mean that when you produc so each token when you train or when you evalu even each token can onli attend to the thing in front of it right You see the problem with what thi thing requir Thi is also the same Okai do that You see the problem with thi method Thi method is the exact opposit Each token attent matrix is delet such that onli the entri ahead of it ar in the attent matrix You actual get to give you an attent matrix that look ahead becaus it onli ever look behind So mayb mayb happen is that the queri and kei matric ar switch up in some wai In that case when we want to interpret the algorithm the wai thei write it down is if I am at a particular part of what I think is the relat between the two entiti how am I go to find whether or not there is more to the relat right There could be a multi-word relat like ha a child with or I know think of ani multi-word relat or whether we kind of ar done with the relat and go to the tail What thi thing is sai is that we should look at the languag model So if thi is realli how it is here and you ar at the word is what you want to know if thi is BERT if thi is a BERT languag model what you want to know is if I were to cross out is if I were to delet thi word which other word in the sentenc right here that ar ahead of me ar veri veri inform to predict thi particular word kind of the queri style And if the answer turn out to be songwrit is quit import for that Mayb Dylan is too but we onli look ahead If it turn out A the word A is not as import as the word songwrit right Becaus songwrit yeah it give an indic that there should be is becaus songwrit is kind of a profess and a person in front of it We look at that but the attent matrix would have that in mind valid right So how thi construct is made Howev if thi is the kei we have to think of the other wai around If we ar at is we look ahead and sai if I were to delet the word A how well could I reconstruct it from thi word is or if I delet songwrit how well could I reconstruct that from the word is I think both ar you know there is interpret probabl for both of these method But what I want kind of to convei is that none of these thing ar realli amen to construct a knowledg graph quit interest that thi stuff actual work becaus all it ask is how well doe on word inform about the presenc or how well can on word predict anoth word And from that inform we construct thi knowledg graph which probabl is a testament to the fact that knowledg graph mayb so much about knowledg If you extract them from a corpu but more about grammar I would think the thing that goe on here becaus these languag model ar a lot about grammar right A lot about how differ word appear togeth frequent So given that songwrit is kind of a mix between grammar and basic word knowledg given that songwrit is kind of an object here the word is be the verb is probabl quit import for it And exactli these tripl thei alwai appear a bit like compress sentenc which ar veri grammat relev So not bui thi hypothesi that there is much knowledg in these languag model and why thi work What I much rather think is that thei ar realli realli realli good at kind of grammar and statist associ between word across the languag and why thei can extract these candid fact so well So what I think about the algorithm Thei do constrain it some more as if it alreadi have enough constraint but thei all make sens So thei sai the match degre which is simpli the sum of all these attent matrix entri that encount dure our search So all the on we skip or to count it togeth ar the match degre of thi tripl The match degre must be abov some threshold the first constraint Becaus so thei give an exampl right here for the sentenc Roll Stone wrote no other pop song ha so far realli challeng artist convent And the extract candid fact is Roll Stone wrote pop song Again you can kind of see here mostli go into grammar-ish So spaCi extract Roll Stone and pop song and the languag model here extract like the onli verb in between wrote So yeah to limit to kind of limit the to limit the match degre to sai it must be at minimum kind of some number it make a lot of sens Becaus if the match degre is high that mean if we go by thi attent matrix it mean that these word that ar in the candid fact thei kind of as themselv thei follow from each other So the languag model think that wrote is a veri good follow to Roll Stone and pop song is a veri good follow for wrote or the other wai around depend on which wai the attent matrix is But kind of the languag model think that these word togeth make sens in the context of the sentenc of cours like in the context of thi entir sentenc So as I said sort of think of it as a bit of a summar paper but with more constraint Candid number two is that the frequenc of R is abov a threshold So the relat itself be too specif it actual should appear a bunch of time in the corpu So what you do is you know you go through the corpu onc extract all the fact my pen just drop you extract all the fact or the all these candid and then you kind of count them and go through the candid fact again and delet all the on that ar below a certain thing peopl usual do thi with thing like stop word or rare word and so on pretti standard make a lot of sens And constraint number three relat R is a contigu sequenc in the sentenc Okai so we have an exampl here from the same Roll Stone wrote challeng convent which the languag model would like to extract becaus again these in the context of that sentenc these word sort of you know thei jump to each other in the attent matrix becaus you can predict them from each other veri well But thei sai thi must be a contigu sequenc So what I said befor I said thi could happen with thi constraint thei exclud Okai so for the second part where thei actual have to map a candid fact to a fact in the schema as I said thei us kind of pre pre made solut entiti link and relat map with the schema I go into thi except to sai that whenev thei find a match thei sai that thi is a map fact whenev thei find a match thei sai Oh thi is an unmap fact Okai an unmap candid mean that at least on of HR and T is not map to the schema There ar two type partial unmap fact is where some ar map and complet unmap fact indic that all HR and T ar not map to the schema For exampl Jacob wa a regist Mennonit Now here thei so thei thei sai thei have these differ fact And you know a cool thing if a model like thi can actual come up with new fact not so not onli new map fact which is someth you would expect right If human provid some kind of a schema then build a knowledg graph thi is never complet So if you can automat kind of fill in miss fact veri veri cool Though I would sai human if you construct knowledg graph human should probabl also build kind of like neg connect sai like ye it is conceiv that Elvi wa a vegan becaus a lot of text talk about it but in fact it is explicitli not I think what we have in the knowledg graph so far But it would be cool if thi model could fill in new fact Ye to the schema it would also be cool if it could uncov complet new relat that been been consid by the human maker of the knowledg graph like if the knowledg graph itself is incomplet the schema is a man you know same argument the schema is probabl also incomplet Thi paper is sort of try to sell their system as someth that can do that And I believ that to a degre but also also Jacob wa a regist Mennonit Okai now mayb complet wrong from the sentenc Jacob wa a regist Mennonit in Amsterdam I might be complet wrong but Mennonit is a religion I think And veri veri sure that ani of these knowledg graph with the schema that thei have have be in a religion or be of a certain faith in their relat tabl somewher And also pretti sure that Mennonit larg enough that that would actual appear as an entiti mayb Jacob not right Mayb Jacob is an unknown Jacob We know who Jacob is But thi seem more like a failur of the entiti linker and relat linker then an uncov new relat or an uncov new entiti So yeah take thi stuff with a grin Now thei thei ar veri honest about thi But just to sai that probabl what happen most often So here you can see the graph for Bob Dylan construct from the Wikipedia page that ar kind of thei sai around the page of Bob Dylan So I guess on or two or three hop awai someth like thi And you can see the blue stuff is stuff that we alreadi knew so that the human human also found when look at thi Then yellow stuff I believ is either new relat So whenev thing ar annot a new relat in the schema So you can see thi is an entiti in the schema becaus annot Thi is a relat in the schema but the arrow is new So the human yet extract the fact that Bob Dylan wa or wa a member of artist unit against apartheid Then the yellow also sometim mean that there is a new thing So here tour with is a relat extract that is not in the knowledg graph yet Also thi on And you can pretti pretti cool right that you can extract these thing automat a lot of yellow stuff here which mean there is not a lot of new inform that thi extract And a lot of thi new inform is actual map to the schema right Bob Dylan resid in Duluth I know how to pronounc that by the wai Ye So fairli fairli cool Thei do some of these task of these knowledg base task So in these task what have I believ what have is alwai have like a head and a relat given So you have a document and you ar given a head and a relat and ask the tale of thi And then you ask the system and the system will tell you So you have these baselin and these baselin I believ thei ar specif made to extract these knowledg represent Thei might even be train I know that but you can see that the MAMA even the smallest on here beat those by quit a bit Now you can see that the recal is significantli lower than the precis which is a direct result of how mani constraint on the system there ar and tell you sort of what the go forward what the improv can be So thei analyz a lot of thi And yeah so a first recognit is that larger and deeper languag model produc knowledg graph of higher qualiti Third languag model outperform languag model under similar model size which is interest is scalabl to larger corpora which again as we said you need to train it and larger corpora emb more complet knowledg graph which is someth we would expect The other interest part is the unmap fact So the number you can actual comput onli for the map fact right Becaus where you have data human produc the knowledg graph from thi what you can compar with Now the unmap fact thei sai thei analyz we turn to studi the qualiti of the candid fact that ar not map to the abov refer knowledg graph schema but ar in the open schema gener by MAMA We manual judg such unmap fact gener by our best method from sampl document in Wikidata and TAC KBP respect So thei go as research thei look at these thing and thei judg them whether or not true given these document in Wikipedia Thei sai the qualiti of unmap fact is veri so the claim is that look at them and thei ar good We find that of the unmap fact ar true on Wikidata We find that of those true fact ar partial unmap fact For exampl Bob Dylan tour with the Grate Dead And yeah here is an if thi realli in the schema right Thi is a nice relat that you might think human would miss becaus tour with someon is not the first thing that would come to mind if you had to come up with a bunch of relat between entiti but it is someth that is regularli us regularli us for musician So that is an applic where certainli an autom system can even extend the schema right Whose relat is not within the scheme of Wikidata Well both head and tail ar in the schema The regist the remain true fact ar complet unmap fact For exampl thi red Jacob wa a regist Mennonit And thei also sai accur entiti detect is desir where thei sai a lot of the error ar due to spacei detect wrong incorrect entiti or due to incorrect or miss entiti link by the by that those system The rest error made by mama ar incorrect relat phrase such as uninform relat phrase For exampl Bob Dylan made and hi breakthrough What can you do What other what other on what other verb would you put there Yeah but okai go to look at a few last thing right here Thei have a bunch of a bunch of experi right here which where thei show you know the beam size ha an influenc thi constraint number on and number two that we look at ha an influenc right So you can tune these thing a bit What is interest here is that thei try thei try to look at either the attent matrix of the last or of all the layer And interestingli the system perform better if you onli look at the attent matrix in the last layer Now thei reduc that attent layer becaus there ar multipl head us max or mean You can see thei perform similarli but it is interest that onli the last and thei argu thei argu in the text that we know that the last layer kind of have higher level featur than the lower layer But I recal there ar multipl paper like done video about them what doe BERT learn and so on I think even someth in constraint in conjunct with lotteri ticket and so on that show that in a transform at least I think it is the middl layer that encod the most kind of semant knowledg becaus the lower on ye thei ar for kind of low level featur but the upper on thei ar again for low level featur becaus the task right here at the end is to predict an individu word or token right So expect that the featur in the attent matrix there go back to kind of sort of more grammat featur and so on and that the highest level featur ar actual somewher in the middl I know if thei test if thei onli test like all versu last in which case yeah I believ that But if thei test each on individu and it still turn out that last is the best that would kind of add to my hypothesi that what happen here is more kind of a grammat effect of extract thi correct candid verb in between the head and the tail So that kind of give more weight to my hypothesi So to repeat my hypothesi is that kind of a grammat thing go on here becaus the onli task of thi model is basic to find the correct string span for the relat between head and tail becaus alreadi given head and tail And there from the text their hypothesi is more like the languag model have a lot of knowledg built into them and we can extract that knowledg kind of thei make it sound like the languag model ha thi semant knowledg in them Okai okai So look at a bunch of map fact right here You can okai you can mayb check out a lot of them yourself but just look at like on in each categori Blah blah blah mail yada yada yada yada is in wors shape howev Klau told press confer at the Western citi of Essen where yada yada yada And it extract thi compani and it map it to the citi of headquart Mayb thei leav out some text here What I want to get to is the unmap fact Where ar the unmap fact to just kind of show you map fact unmap fact okai so the unmap fact what I feel and you can judg for yourself pleas what I feel just to pre-bia you befor we look at them is that a lot of time simpli it extract thing that ar that ar it extract thing that ar not it simpli assign thing right a failur to sign not a new thing becaus in these schema like you seen the schema but you kind of get a feel the last which is the last tabl you kind of get a feel of what contain in it so mayb get a feel for for what okai Ernst Haeckel wa born of Februari in Potsdam okai so the extract thing is Haeckel wa born on of Februari in Potsdam okai so that it map to thi is in the knowledg base a schema thi is in the schema but wa born on of Februari in is simpli a failur of the relat linker okai he wa also a pacifist until the First World War yada yada yada Ernst Haeckel and then wa on a and a pacifist ar both not in the schema now mayb pacif in the schema mayb mayb though I would guess pacif ha a Wikipedia page so it must be in the schema becaus a wiki data but wa as you know the relat here with someth be like a polit lean or someth like thi which is certainli certainli in the knowledg base right then you have thing like Haeckel wa award the titl of excel so you have correctli Haeckel again recogn award receiv is in the schema nice excel as a tail and excel you know what what do you want like thi is thi is a thi is not a fact right thi is the award or the titl of excel would be kind of the thing so thi is a failur of spacei so again I have seen littl fact here that would actual be of genuin a genuin addit to the schema that should be consid and I absolut believ that the schema is incomplet get me wrong I like a the schema is probabl less than of what it should be right if we did a thorough job I just think that thi system here is a good like I think that the thing that thi system come up with mostli ar simpli failur of it subsystem rather than genuin new entri to the schema differ from when it genuin discov when it discov a new map between alreadi establish thing for exampl Paulin Bain educ at thi colleg right so these ar new fact all fit in the schema and the system might be veri veri nice for that all right so that wa my kind of estim of thi paper I hope I rag on it too much as I said veri cool work actual I look at thi appendix is giant go look at it check it out pleas tell me what you think about it in the comment ani feedback is 
Neural Networks from Scratch - P.4 Batches, Layers, and Objects
go on everybodi welcom to part of the neural network from scratch video In thi video what we ar go to be do is take our current a singl sampl of input and convert that to be a batch of input explain why and how momentarili Also we ar current model a singl layer of neuron and what we would like to do is model two layer of neuron which then we could extrapol out to howev mani layer of neuron we want at which point it will becom pretti appar that we would benefit pretti greatli from convert thi to from thi code now to an object instead So be get into do object orient program creat our first layer object With that in mind as a remind thi seri is base on the neural network from scratch book which you can get at nnfsio at the time of my record thi thi book is in draft form but if you pre-ord ani version of the book you get within hour access to the ebook draft version which is current in a googl doc so you can go in there highlight ask question comment in line with the actual text also the draft at some point it be a draft anymor obvious the book is current schedul to ship out Septemb and but right now the draft version of the book is quit a bit further ahead than we ar right now cover everyth from obvious creat the the neuron in the neural network creat a full neural network do a full forward pass obvious with also activ function calcul loss do the partial deriv back propag with variou optim also regular techniqu like dropout and regular so quit a bit further ahead so if you want to get a jump on the materi or you just want to review the materi befor or after the video a great wai to do it and again access to the ebook version is given within hour still do that manual so just if around give access so as far as I know no on ha wait more than a dai for the access so anywai get into the code now so the first thing that go to cover is batch first of all why batch so kind of two major reason for batch the first pretti obviou and simpl to understand we can can we can calcul these thing in parallel so the first thing that allow that batch allow us to do and realli a cherri on top not sure I think thi is just kind of a nice thing that happen I realli think thi is the primari reason I would sai batch but becaus calcul thing in parallel the bigger the batch the more parallel oper that we can run now thi is also why we tend to do neural network train on GPU rather than do them on CPU so I know what the averag CPU core account is these dai some ar probabl between four and eight not veri mani core compar to your typic GPU which is go to have hundr or thousand of these core that we can actual run calcul on so why so much faster on a GPU so and and realli the CPU core right those ar meant for much harder calcul than what do here which is typic like matrix multipl and pretti simpl calcul so so why we us GPU rather than CPU now a benefit of do thing with batch but the other reason why we do thing in batch is becaus it help with gener so if you just think for a moment so as a remind what ar what ar input right now a singl sampl and that sampl current contain four featur so given our continu the exampl of hei try to predict whether or not a server is go to fail obvious made up all of these valu but just pretend for a moment in that exampl mayb each of these featur is a read from a sensor so thi could be your temperatur sensor thi is your humid sensor valu and so on so these ar these four valu ar valu from uniqu sensor at a specif point in time so a singl sampl now we could convert that to be a batch of sampl and have mani read of of of sensor at ani specif point in time and then go to attempt to fit those to someth now right now we have the intend output or anyth like that not calcul loss yet or anyth like that so for now just make sure you have a good idea of what what ar these input right now thei ar featur from a singl sampl and so thi thi describ the current statu of thi server at a singl point in time right now and what we want to be abl to do is pass a batch of these sampl instead so so the second reason why thi is help is it help us to gener becaus rather than show the machin the algorithm on exampl at a time and then basic go to try to fit to that sampl and then the next sampl and the next sampl we can instead show it multipl at a time that wai much more like to gener and I think the best wai to understand that concept is to just visual it so check that out so to visual the concept of batch and gener what go to do is go to take a theoret sampl set of sampl and then go to ask a singl neuron to attempt to fit to that sampl now go on here is just fit on sampl at a time to thi neuron and you can just see that thi neuron is just bounc around with that fitment line so that line there is the fitment and the dot that see these ar individu sampl so just ask it to fit to thi on then thi on then thi on thi on veri confus and just imagin yourself if you were shown just on sampl at a time and you were ask draw the fitment line to repres the entir set of sampl you would have a probabl a pretti hard time do it right wherea when we show multipl sampl or you can see all of the sampl kind of fade into the background here you can see okai I could probabl what like if you just had a better represent of all the sampl you could probabl do a better job of fit to those sampl so if we increas the batch size to we can see that it doe a better job a lot less wiggl around now we expect that when just show a small number of sampl at a time that the fitment line is go to move a littl bit we expect that but you can alreadi see how much less movement see so alreadi becom much easier for thi neuron to fit thi exact same sampl set when we increas the batch size to if we continu to increas to we see even less movement for the most part right thi line is pretti much in the same kind of standard deviat rang onc it catch up via the learn rate you need to understand what the learn rate is necessarili do here just know the learn rate is kind of the wai of kind of move forward so how much of the previou knowledg do we want to keep versu the new knowledg or someth like that get to learn rate soon but I think if you just imagin again yourself like if you were try to creat a fitment line yourself imagin you ar the neuron if just show on sampl at a time veri difficult wherea if show at a time suddenli it becom a littl easier now you might wonder why not all the sampl at a time thi can gener the problem with thi is it can caus over fitment so if you show all the sampl at onc now tell the neural network hei just fit to all of these and the neural network is go to try to fit as good as possibl to all the sampl at onc which is actual like to hurt your gener becaus now it ha knowledg of all of the sampl and go to actual fit your in sampl data it will fit your in sampl data like pretti good and then do pretti bad on your out of sampl data so you realli want to pass all of your data at onc and you necessarili want a huge batch size either gener someth between mayb as high as pretti rare a batch size of is actual a pretti common batch size you know from what seen differ problem will vari with batch size for sure but that is the concept of batch size alright so now that all convinc of the wonder of batch convert our singl sampl of input to a batch of input so right now a list that mean go to convert it to be a list of list and then go to match the valu from the book so the second sampl here is and then neg the third sampl is neg and neg the next question you should ask yourself is there anyth that we need to chang about weight and bias do we need to add weight or ani bias or anyth like that just as a gener review of how thing ar work so the answer to that is of cours no becaus weight and bias ar associ with individu kind of uniqu neuron ar we ad ani neuron to like thi in theori is our layer ar we ad ani neuron to our layer by ad more of a batch of input ha anyth chang there no so we need to chang anyth in our weight and our bias get to the output is there anyth that we need to chang in thi dot product or ad the bias or anyth like that so now that both weight and input ar go to be a matrix talk briefli about the matrix product and how that work essenti go to be veri similar except for the fact that now that the input ar go to be a matrix talk about how that how we do a matrix dot anoth matrix so to do thi what we do is we take that first row vector from the first input matrix so in thi case it wa matrix a and then go to do the dot product of everi singl column vector from matrix b thi result in the scalar which becom the first valu in that first row vector for the output matrix so again still the first row vector in that a matrix go to do the dot product with the second column then the third column fourth column and fifth column of matrix b and these individu scalar valu as the result of that dot product becom the first row then we continu on now at that second row that second row vector from the first matrix and so on until popul the entir output of thi matrix product so rememb in the previou tutori that we hit an error becaus becaus we were pass input by weight right and recal that the size at index at the index of on of the first element in the dot product need to match the size at index zero of the second thing that we pass so in thi case at index zero so weight is what a three by four so that mean input need to be the four by none realli or four by on that it wa so now is that true doe that still is that still the case well no right becaus actual input and weight ar current the same shape so if we go to run thi go to see uh-oh right we get the shape error right and again just like I said befor thi is like a perfect shape error that you probabl would see fairli frequent in other deep learn framework and then you would probabl you know googl or duck duck go search to figur out what is the answer to thi problem becaus it make sens to you but pretti clear if you now that you understand how thi work that tell you right now you know got a dim on here of size four right so weight tell you right now at thi at the first of index so the zero is thi stuff so you have three set of weight at the first of index well how mani weight do we have in each weight set four so tell you right now you got four here but unfortun at dim zero of input right you have uh three right you got three in thi batch so got three sampl thi is a problem and it get ani better by chang thing so if we refer back to input by weight for exampl we get anoth uh shape error becaus it still chang we have to uh we have to do someth els so what do we do becaus so go back to what ar we try to do what we ar try to do is input time the weight so happen right now is the dot product is attempt to take thi valu multipli by thi valu then thi valu multipli by thi valu thi valu multipli by thi valu and then we get to and the dot product is like what what do i do it have a valu instead what try to do is thi valu time thi valu thi valu time thi valu and so on all the wai through four and then suddenli uh the shape ar correct so how do we do thi so what we realli would want to do is take thi weight matrix and we want to switch row and column right what we how we would solv thi problem so luckili for us call transpos so again the concept of a transpos is just to swap row and column so all go to do is take the first row that becom a column go to take the second row that becom a column and so on throughout thi entir matrix and all there is to a transpos of a matrix all right so go ahead and transpos the weight now to do that actual go to convert weight to a numpi arrai now why do we need to do that so right now input and weight ar list of list in python when we pass those to dot product function here happen is in the back end these input in thi weight these ar actual get convert to numpi arrai but sinc we want to us numpi to transpos the weight we actual ar go to convert weight now to numpi arrai weight and then to transpos that actual just dot capit T boom transpos go ahead and run that and what we get is an actual batch of output so we should be abl to recogn thi on is thi is the same on that been see uh so far which is the output from the three neuron from uh thi as your input and then these ar some new output from those same three neuron base on the the other input that we just just start ad so okai pretti cool now the next thing that go to talk about is ad on more layer here but befor we do that make sure with us as far as what happen why did we have to transpos here so befor we move along to ad anoth layer review happen so far so initi we were abl to do the input and the weight without a transpos becaus the weight wa a matrix but input wa just a vector so in thi case the shape line up we could do a dot product as long as the weight came first and everyth wa fine but when input becam a batch of input the shape did not work anymor becaus what we need is index on of that first element need to match index zero of that second element so onc we transpos weight suddenli it wa abl to work again so review the actual calcul that have taken place in our exampl the first thing we do is transpos our weight so that we could even do the matrix product then we take the matrix of our input dot the weight which mean go to take each individu row vector from the input matrix and do the dot product with each column from the transpos weight matrix do thi for each of the row vector in the input matrix which give us our final output for thi matrix product okai so our final calcul is to add thi bias row vector to our matrix product output from the input in the transpos weight to add a row vector to a matrix all we need to do is we ar go to just add thi bias row vector to each of the row from that matrix product output to do that all do is ad each element togeth so at the same index so plu plu and neg point plu point okai so now what go to do is add anoth layer and to do that we just need anoth set of weight and bias and then basic the size of weight how mani neuron go to have the length or size of bias again how mani neuron go to have so for now just go with the exact same size layer just for sake and just so you understand how thi is kind of work and then what do is convert thi to an object so again like befor I am go to copi the valu from the book so go to sai neg for that first set of weight the second set of weight will be neg neg and that final set of weight is go to be neg and that neg and then final the bias that will go with neg and neg so that is for our second layer now what go to do is rather than just call thi straight up output go to call thi layer layer underscor output so thi is now the output of layer which then becom the input for layer so all go to sai is what go to do is go to take thi line copi we past thi line and then go to rather than input here thi becom layer input call thi layer output and then so the input chang now to be the input from layer now what we want to do is take weight bias and then what we can do is we can print you know layer output at thi point go ahead and run that and just to confirm that is the same from the book ye it is cool so okai so as easi as ad anoth layer now of cours you know you might want to be abl to realli quickli chang the size of that layer like how mani neuron ar you go to have and ad more layer and of cours at thi point we should hopefulli recogn that thi is veri quickli go to be quit quit unruli so instead of do thing thi wai what go to do is convert thi to be an object right so to convert the concept of layer up to thi point what go to do is keep input and these will be thi will serv as actual our sampl data and instead what go to do is go to delet basic all of thi and we will start code the other thing I will do is go to go to chang the name of input to X as a capit X kind of a standard with machin learn that your your input featur set is go to be denot by a capit X so have X and mayb have X train and X test and so on but for now just make it a capit X so that signal that thi is our actual train data set now so assum thi is the input data to the neural network and then now what go to defin is the two hidden layer and again call hidden onli becaus we as the programm realli specifi how that layer not in charg of how that layer chang I suppos like we kind of defin you know how we defin the optim we defin variou hyper paramet for the optim and stuff like that but but the neural network is respons for tweak that wherea the input data you know a lot of time what go to do as a programm is go to scale that mayb between and or neg and posit and so on for now just leav it as is but that is go to be our input data so in thi case sai got three sampl so so that the next thing that go to do is just go ahead and class and go to call thi layer underscor dens no perform requir and then go to defin our init method in thi case just pass self for now and then go to hit pass and then also go to defin the forward method and again pass self and then also just sai pass for now and so talk about how we initi a layer so in the case of neural network kind of two wai go to initi a layer the first is got a train model that you save and you actual want to load in that model so the what happen when you save a model what all do is actual just save the weight and the bias so when you load the model all do is set the weight and bias as whatev thei were in that save model in thi case though sai make a new neural network what how do we initi well got weight and bias that need to be initi so first go to initi weight the wai that we tend to initi weight is as random valu between in the rang of neg on and posit on but the smaller that rang or the more tight that rang is probabl the better we want gener with neural network we want small valu so the reason why we want small valu is we hope that thing continu to tend in the rang of neg on to posit on now just as a singl exampl of why we do that is if you have input data and the weight is sai or or or some larger valu what do is data pass through thi network is make it bigger and then for each of those connect if each of those weight is abov on make it bigger than bigger than bigger than bigger and then you get thi like explos so on of the thing that we try to do is keep includ with our input data so in thi case we have a few valu that ar that exce the rang so on of the thing you might do is first normal thi data data set and then scale thi data set and by scale we gener mean take it and chang the valu so that like kind of the mean stai the same but the scale ha chang in such a wai that these valu ar all between neg on and posit on well we will talk about that later but in thi case when we initi our layer we want to kind of adher to thi principl of keep valu small rather than let them even at the outset just explod so for our weight you know we you know mayb someth between the valu of neg and posit mayb a good start point for weight then for bias we tend to initi those as just zero now time where you might not want to do that becaus if for whatev reason your neuron fire initi so if thei thei go through the neural network input time the weight not big enough to actual produc an output your bia be zero mean that neuron is go to output a zero so then what happen well that output zero get multipli by whatev the weight is for the next you know the next connect zero time ani weight is still go to be zero if that neuron bia is also zero go to output zero and then boom just propag through thi entir network all zero and the network is dead so to speak so if that if have a problem like that just know okai mayb we need to start the bias off as a nonzero number like I said in gener that be an issu but if that is an issu that is on solut and again why we learn about neural network from scratch so that when we hit issu like thi we know ah probabl a dead network okai how do I solv that I mayb want to initi bias of someth other than zero so with that in mind do it so for the weight we want go to us random and numpi and just so everybodi follow along hopefulli get the same valu go to set nprandomse go to set thi to zero so for initi we need weight and bias so go to sai self dot weight and thi is go to be equal to nprandomrandm and then we need the shape that we want to pass so in thi case we need to know kind of two thing from the programm when thi layer is creat which is the size of the input come in and then what is the size how mani neuron do we want to have so in thi case what is the size of the input come into thi layer it will be the size of a singl sampl so in thi case go to pass a batch but in thi case four right so you would sai you would sai four but instead what go to sai is n input to the network and so what we want from numpi is to creat a weight that will be the size of n input multipli by how mani neuron do we want to have so sai n neuron and then what go to do is in the paramet that go to add ar go to be n input and neuron so when we creat thi dens layer specifi these valu so that is nprandomrandm n input and neuron now just to show what thi doe becaus we ar actual go to go to make thi even smaller go ahead and print and past thi in and just sai the n input is four and go to have three neuron we can save and run that and you can see here is our weight so what randm doe is just a Gaussian distribut bound around zero but as you can see some of these valu ar actual bigger than on so what we want to do is instead just sai time and then run it again I know why why is that not run there we go okai and then thi look a littl bit more like what we hope our weight to look like so what go to do here so just go to sai time and that will be our weight and thi and again thi is just the shape that it make for us and then go to sai selfbias and go to sai that is equal to npzero and in thi case our bias the shape is on by howev mani neuron that we have so actual two thing to point out befor we move on to thi forward method first of all the input to npzero to pass the shape actual the first paramet is the shape so we need to actual pass the actual shape as a paramet so be just a tupl of the shape versu nprandomrandn where the paramet ar the shape so just kind of note those differ but actual both of these function return a matrix of the shape of you know thi and thi and or the shape of thi and thi so kind of confus I almost mess that up so make sure you so the other thing I want to point out is the shape of weight so how did we shape weight we ar sai thi matrix is n input by the number of neuron so view part three what wa the shape of weight the shape of weight in thi case wa actual the number of neuron by the number of input so the opposit so why did we do that so the reason why do that is when we go to do our forward pass if we have shape our weight and so in thi wai alreadi when we do the forward pass we have to do a transpos everi time we do a forward pass just a silli oper to be do everi time when now got full control right here in the initi so what go to do is actual just not need the transpos anymor it just make a lot of sens so take note of that as well so for the forward method the forward method is go to just take input so whatev input ar thi could be realli on of two thing either go to be if thi is the first hidden layer the input ar go to be your actual your actual train data or the input will be the self dot output from the previou layer now the self dot output thi is go to be mp dot dot and in thi case go to be your whatev the input ar multipli by the self dot weight and then of cours plu self dot bias and that is your forward method so make us of thi and then be done with thi tutori so the first thing go to do is go to make our two layer so go to sai layer on equal a layer dens object and we specifi basic here the size of the input and then the how mani neuron do we want to have so the size of the input we alreadi know that it is so the input basic how mani featur in each sampl so in thi case we have on two three four so we know the input size is go to be four and then for number of neuron anyth you want you can make thi ani number you want go to for now just sai five so we can look at it on the output but you could make thi five twelv it doe it whatev you want so and why we want to make thi an object becaus go to it just make so much more sens so much more dynam so so our layer on and then we can go ahead and just specifi our layer two now the onli requir for layer two is that the output from layer on is go to be the input to layer two therefor if thi is five on the output layer two ha to be of shape five for the input but again the output can be ani shape or ani size that you want so sai two okai again it could be anyth you want go to choos two so now we have these two layer object now we can pass data through these layer object and when we do that we get a self dot output for that layer so first start with layer on so go to sai layer on dot and do a forward pass or the forward method and pass in the x data when we do that that mean we will now have a layer on dot output valu so go ahead and print that so print layer on dot output and we can see here that sure enough becaus or how big wa the data that we pass we had three sampl so we get three output basic of thi batch and each on ha how mani valu well five becaus how mani neuron we said thi layer ha and again you could have pick ani number here so now layer on output becom the input to layer two so we will sai layer two and then we do dot forward and we will sai layer on dot output and then what we can do is print layer two dot output and let me just silenc thi on so super clear go ahead and run that and what we get is again of size three becaus that wa the batch of the input data that we pass we now get three exampl of these two neuron output base on the data that we pass in so pretti cool thi ha been a pretti long time to get to such a basic concept but as you hopefulli can see so much that even in thi super basic concept of input time the weight plu bia so much ha gone into that just alon that hopefulli at thi point I think pretti cool to take your input data and pass it through thi network and get you know whatev shape you could possibl desir I just think that concept alon is actual pretti cool so anywai that is our dens layer object at least to the point of do a forward pass of some data now at thi point the next kind of major thing that miss is in our activ function so right now just kind of pass data we do have these weight and these bias but the next kind of major element that we tend to tack on here is go to be an activ function so go to be the subject of the next video and then of cours as we continu to build after that we get to final try to figur out okai well the loss of thi of thi network and then we start to determin okai well how can we optim these weight and these bias to decreas that loss and get the valu that we want so pretti cool stuff to look forward to begin to get into the more sexi part of deep learn hopefulli you gui ar enjoi again you can check out the book at nnfsio where we ar alreadi at the sexi part of deep learn also if you have question comment concern whatev 
How I Read a Paper: Facebook&#39;s DETR (Video Tutorial)
Hi there peopl So a lot of you have ask me how I read paper And honestli I think there is ani super special method to it But you know I thought becaus peopl have ask me to make a video on it so make a video on it And try to share my method of read paper And hopefulli thi is go to be somewhat of a mini seri or a seri where I everi now and then discuss how I read on of the paper that I make video about And try to select them such that differ thing ar highlight Now select thi on right here for realli for no particular reason other than I sort of rememb it And go to try to go with you through how I read thi and how I encount thi and kind of try to honestli share what I thought at the first time when I read it And I hope thi help some of you If it doe help you and if you like content like thi of cours feel free to share thi out and subscrib If you seen my origin video on thi paper it might be worth to go watch it link it And with that dive in So again thi might be not realli someth new but just go through it Okai So first thing I do is of cours read the titl So the titl ha three part end to end object detect with transform So what I notic that I do myself is I like through read a paper like read the paper with an open mind I do that I almost immedi form an opinion and a hypothesi of go on Like so I see transform So I know what transform ar if you made a video of my lot of video on transform attent is all you need is the base paper for that So I know what a transform is okai And I know that transform ar usual in NLP ar usual us in NLP though there ar thing like you know other thing with transform but usual an NLP model Then I read object detect And I know object detect is a comput vision task So immedi thi here is sort of a a differ And I immedi try to assess the new idea in thi paper And in thi case it might be okai it might be appli transform to object detect But then I also see end to end And the onli reason to put that in a titl is becaus the novelti becaus usual in deep learn sort of us to system be end to end And even if there if most system end to end a lot of peopl is like end to end imag classif on ImageNet like thank So I wa guess that the reason thei put in end to end into the titl wa becaus actual someth special about the model So now I have like two compet hypothes of why thi paper matter first of all becaus it doe it with transform And second becaus it doe it end to end And the of cours the truth here is that the combin of end to end transform all of that is what make thi model And I alreadi form like a hypothesi of whether I like thi or not like I have to be I have to be honest I have veri quick judgment of paper of whether I like them or not And then I sort of catch myself each time And I still try to so there ar for most paper actual that I have sort of a neg opinion at the begin where I will neg There ar paper where I think like there is no wai thi is go to you know work or someth like thi actual posit convinc throughout the paper So for most for most paper where I that I read try to find the posit thing in there But I do form an opinion pretti quickli usual Alright so the second thing thi part right here I like I even see thi is like advertis on on like Twitter I like you just I have alwai had issu with author name like peopl will come to me and be like Oh have you seen the new vinyl paper And I have no clue And then when thei sai like oh where thei us thi charact level model to do that And like oh that paper So I like do not care who the author ar of a paper Like I I rememb the paper by their author name gotten better at it I have to sai but alwai had troubl with thi Now not to sai that a name pop out to me Like if thi would be like like Yoshua Benjo or some someon like realli famou then of cours that would catch my ey And but I also know that you know Yoshua paper Yoshua lab is huge So just becaus a big name is on the paper mean that the paper is go to be of ani good or bad qualiti Sometim the author give you an indic of what kind of research is there Like if you see Jeff Klune or Ken Kenneth O Stanlei you know that go to be thi certain type of you know learn to explor and kind of a bit more out of the box think in their paper which I realli like but it immedi give you a clue Mayb if you go by first author much more indic if you have alreadi read some of their paper but most often I just ignor author and go on The affili sometim matter in in that a bit of a viciou cycl If a big name affili like Facebook AI Googl AI and so on These paper also thei get more exposur in like the press and so on So whenev Googl AI publish the paper all of these all these pop site magazin like The Verg and thi and Lifehack and Hacker New and whatnot thei like like write a blurb about it So often thei get much more scrutin for these paper thei get much more thei get much more the public attent but thei also get much more scrutini which in turn mean that there is a bit more pressur on them to do good experi So that bias me like a littl bit into the direct of believ their experiment evid more Now usual thi is also back up by the fact that I am actual convinc by their experi usual So these these big name paper often I find myself even without or disregard the affili to be convinc more than of like regular paper my most often issu with paper is that I believ the experi And I make no differ Like even if Facebook I still my prior is the experi ar crap and I believ them And thei have to convinc me of the opposit But some like I sai that it affect me that like a big name affili Okai so then the second thing is I sometim I see the paper on archiv and I skim the abstract Sometim the abstract is inform and sometim not So here like blah blah blah a new method that view object detect as a direct set predict problem And like Oh yeah okai So streamlin the detect effect remov the need for mani hand design compon like non maximum suppress yada yada yada The main ingredi call detect transform a set base global loss that forc uniqu predict via bipartit match and the transform encod decod architectur So thei make it clear here why it matter And what I what I want to get at is sort of the new thing in thi paper most paper ar even though all veri long and have lot of math and so on Thei often have like on or mayb two new core thing that thei realli tell you sometim zero But a lot of time like on thing that thei realli do And you you sort of have to but try to cloak it often becaus thei need to make their research as impact as possibl right But you need to sort of figur out what it is do here thei make it fairli easi for us in that thei sai okai thei remov the need for mani hand design compon like non maximum suppress which tell me that build someth easier than what came befor them And that alreadi tell me not necessarili go to be better Their argument is more that go to be easier right There ar sort of two kind of experiment result the on where you try to beat what came befor you And the on where try to sai look our thing work just as well as thi other thing while be more advantag in some other metric So I would place thi alreadi in the sort of second categori And then thei sai what ar the actual ingredi a set base global loss that forc uniqu predict via bipartit match Now I at thi point I know what these term mean But at thi point I actual have to know what the term mean What I need to recogn is that I simpli have to go later in the model later and figur out what that is and a transform base encod decod architectur Okai So there ar two thing right here that I rememb I need to pai attent to later thi loss which seem to be special And there is the transform architectur which seem which thei sai okai that the model basic consist of those two thing And then thei have a descript of what it doe given a fix small set of learn object queri that your reason about the relat of the object and the global imag context to directli output the final set of predict in parallel that almost tell me noth of Yeah okai the model reason mayb thi in parallel is someth but the model is conceptu simpl and doe not requir a special librari Unlike mani other modern detector thi sort of repeat thi enforc my hypothesi that go with the Hei thi is a much easier wai of do thing approach Deter demonstr accuraci and runtim perform on par with well establish that further confirm my hypothesi that thi is on par right the runtim perform on par with the current state of the art And at the end thei sai moreov Deter can easili be gener proce to produc panopt segment in a unifi manner We show that it significantli outperform competit baselin train code and preterm model ar avail Okai Now thi last part when I first read it like okai can easili be gener to produc thi panopt segment Thi is I know yet whether thi is like a central claim of their paper that it can do thi segment or whether thi is like an ad benefit to their paper becaus you can read it in both wai And just readi to find thi out in the paper Now after read the abstract and sort of alreadi form the hypothesi of go on So here I alreadi in my mind I alreadi sort of have a model of how would I do that Right How would I how would I do that And then what would I do So right now what I might be think is if I have a transform over imag that directli output the predict in parallel imagin like an imag and the imag somehow need to go into a transform So mayb like an encod like a CNN encod that give me imag featur And then so mayb you sampl thi down thi imag Thi is just me hypothes what could be go on right And then I might be unrol that right thi imag into a vector of these lower pixel And then so in my mind what I would do right here without know anyth more would be to do someth like BERT span predict So I would have BERT right here And I so for I would input the sequenc right here And then to detect an object I would sort of think that mayb the BERT you know BERT ha an output that is the same length as the input right So veri good at sequenc tag and thing like thi So mayb how it detect an object is go to be that it sort of like tag the tag the center locat in the pixel of an object right here or it tag somehow the corner of the of the bound box But then I know how thi is go to be in parallel mayb BERT output like a score for each locat and then you do some kind of match right here So thi is my initi hypothesi of go on And then I scroll through And honestli the first thing I do is I go and find the pictur And no no differ in all like sinc sinc your first book you read what you do I go and find the pictur becaus usual if someon propos anyth new that go to try to make a pictur of it Luckili I do like super theoret what not your Bayesian gener bound and I know So most often paper I read have some sort of pictur And veri help to me I know I know I know But yeah so I find thi pictur And here I see okai you have imag you have CNN okai give you set of imag featur ar so far so good then transform encod decod then set of box predict So all of them come out here and alreadi read there in parallel and then bipartit match loss So here thei I can see thei color these in differ wai And these color appear to match with these color right here right in the green here And these thei thei also thi is a veri good graphic right from thi I can alreadi read that these here go to the no object A lot of time the graphic veri good So thi thi is what not sai in everi paper you can learn by look at the graphic like sometim the graphic ar terribl And like go on here I like I thi thi make no sens Thi happen a lot in thi paper right here Thi happen to be veri veri good explanatori graphic So take advantag of that And I do the same thing in the other paper right But then later when it match what I read in the text have to you know updat my belief and so on But here I see that these go to no object And thi goe to no object So I know yet that thi is the test set At the point where I read thi I wa sort of confus by thi But I recogn that each of these box right here is go to be either result in a bound box or in the no object predict So from that I could conclud that these thing here ar mayb some sort of a fix set right But I still thought that you know these that thi would actual be the output of these imag featur So that in thi case have like six set of imag featur And then have like Bert here Even though not an encod decod I still thi wa still my run hypothesi that somehow map these imag featur to these box right here So and I know what to what to make of thi thi thing right here So then I went through some more and look for more pictur And not sometim I also kind of glanc at the formula But okai when I ever see thi thi is just I mean thi is kind of useless like okai cool you minim the loss thank Thi okai realli pai attent to that Ah new pictur Cool So thi pictur is much more inform than the other pictur I believ with the other pictur thei were try to showcas thi loss how thei do the match And even though I could read a lot from that pictur I did not get that part And therefor I felt when I saw thi and I just glanc at it like wait differ than up here It seem like the same But okai look at thi So again we see okai you have set of imag featur that come out of the CNN So that conform with my belief But then thi here goe into a transform encod And thi come out So immedi I see oh thi is not the same as these box here right That wa my hypothesi that these thing here would be the color box So I I sai okai obvious not what happen Thi thing here seem to be sort of the encod imag inform then somehow fed into here And that then there ar these object queri thing and thei seem to correspond to thi So a bit more confus right now what I can see is that these then will result in these in these box Okai So be confus by that I look for more pictur So I go look for more pictur Thi here seem to be like a visual a lot of these paper have some sort of ablat experi or so and so on Thi I just find realli cool pictur for now I know yet what it mean thi I know yet what it mean And I go down skip all of thi And then back here in the appendix I find thi here which I immedi map to the previou where thi is the end And thi is a decod And alreadi read the attent is all you need paper And at that point it click in me is like ah thi is not a BERT transform Thi is on of these transform that ha an encod in the decod even though thei told me like billion time Alreadi I wa too stupid until thi point So now I know okai okai I see go on So the imag goe through here And then thi goe as a side input like as an attent from the decod to the encod like I know in NLP right So in NLP thi here would be a sourc sequenc like mayb if you do translat and thi here would be a target sequenc So now whenev I see a now whenev I see a transform like thi and it output someth at thi I I look at it as okai thi here is sort of the input that goe as like a side input over here And usual here you have the target sequenc but not the case right here right You have these these object queri So thi is how far I get from the pictur Now I go up so I have a sort of I have question now I have question And when I start read the paper Only now do I start read the paper after look through all the imag form the hypothesi and sort of have question on how thi work And go a bit faster from now on to just not bore you with all the thing So the introduct is often veri import even though call introduct And mayb you know if you read a book like if like introduct or prologu or someth like thi often kind of pointless Introduct in these research paper is on of the most import point becaus all of these paper thei try basic all of them try to convinc a review to accept them And in order to do that thei will set up their main point and their main stori immedi in the introduct So what usual have is a problem statement which is here like why wrong right now And then you have like a stori of how their paper address the issu Okai and here We streamlin the train pipelin by view object predict yada yada yada Thi is often formul in word what the paper is about and what contribut the paper make right Thi is like a thi is like a longer abstract the abstract is often veri veri cryptic veri dens Thi here is often much more inform of what the paper doe So for understand the paper and a high level the introduct is the best place But given that alreadi look at the imag and so on I actual draw mani new much new inform from thi thing Then relat work And honestli I skip it Like unless the actual review of a paper like when the review of a paper I read the relat work But often relat work is a veri veri complex work But often relat work is just like you first of all you cite a bunch of your friend and then you cite the mandatori paper and then you cite everi singl person that you think could be a review becaus or actual been reject from a confer with a review claim that you compar or you cite that or that paper you can pretti much be sure that the if if not a glare omiss if like a nich paper and you cite it then like okai gonna cite it just becaus the next confer you could be my review again So not not sure that these relat work section necessari Like if someon want to write their thesi and thei go and read thi paper and thei want refer Oftentim thi is a good place but a lot of it is just blah blah blah Okai I know I know disagre with me if you want Oh yeah to mayb to read qualiti So I tend to at thi point I tend to not skim So at first I skim But at thi point I tend to read everi sentenc and read it close and understand it And when I realiz like tire or someth I just skim the paper tri to skim paper and it work Try to read everi sentenc understand everi sentenc And okai if you understand it stop read becaus of that But try to not skim and be like Oh yeah yeah okai I gotta go to go to go to not help Except relat work skip complet cool Then a lot of time in thi paper now is the model And thi is the section actual interest in right So I read veri veri close here And then I find out what their their loss is all about And again I stress read these thing and understand them right Sometim hard But if if confus that mean you either done a bad job or thei made a mistak or that you understood someth If you understand the sentenc try to read on mayb clarifi later and then you know go back But again do not do not like just start a lot of time when I read paper previous like I understand someth quit well yet And then I would be like Oh yeah yeah yeah And then I notic that I start skip and skim more and more becaus that would you know pop up again and again and I understand it again and again And then at the end I would just be kind of glanc at the paper And I want to do that right here So I want to read everi sentenc and understand it Okai so here then I find out about the loss And then I if I know someth here then go and look it up on mayb on Wikipedia or someth like thi Now I need to actual I need to understand everi singl part of it right mayb I should correct myself So for exampl thi bound box loss here thei talk about the second part of the max and quat and Hungarian puzzl is thi box loss that score bound box Unlike mani detector that do box predict with some initi the yada yada yada thei sai the most commonli us l on loss will have differ scale for a small So here thei basic talk about how thei mix the loss thei sai overal our box loss is that defin as thi and thi Now I I know what these loss ar I just assum some bound box loss So when I not true when I sai understand everyth Understand the thing that ar integr to the stori of the paper right How exactli thei comput bound box loss at thi point I care I just assum that some loss that I can back propag right I what is import is that thei do thi Hungarian match thing right As soon as I get that like oh that wa thi you know thi thi thing no thi thing up here Thi thing thi with the match thing Now I get it Now I know there ar alwai the same amount of box here And there ar alwai the same amount of label here And all we need to do is somehow match them And I immedi think why is that relev Oh becaus when someth is alreadi match to an object some other thing cannot be match to the same object And how we you know prevent the fact that all the thing predict the same thing right And so that immedi becom clear And as I said there is usual like on or two idea in a paper I assum or I care what their exact loss function is becaus sort of gotten the idea up here of what the loss is about All right so I hope clear on the veri close read the thing and understand the thing that ar necessari for the stori If you find if you think not necessari for the stori and then later end up not understand that mayb come back and you know read it again In ani case I would I would rather I would rather skip someth and assum not necessari if I think so and then come back then try to understand everi everyth But the thing I do read I try to understand thoroughli Okai Then the architectur okai And that again I read close and get backbon okai transform encod okai And now I understand much more close decod okai And here I get now final I get what thi is about decod and object in parallel yada yada yada These input embed ar learn posit encod that we refer to as object queri And similarli to the encod we add them to the input at each attent layer So now thei name alreadi seen these object queri here And the onli word I actual need from thi sentenc ar learn The fact that posit encod I just kind of ignor As soon as thei sai learn I know aha these thing here ar learn thei have actual alwai the same for each of the imag just overal learn Okai so now I feel I understand the entir model And yeah so thei then thei sai auxiliari decod loss And thi sometim you have to pai attent to like auxiliari auxiliari thing like like auxiliari auxiliari thing becaus those ar the the thing that here thei sai explicitli we found help to us auxiliari loss Sometim thei sai why thei did it just sai our loss consist of three thing And you know if you look at the three thing onli on of the thing is realli a part of their stori so far And that you should immedi conclud that put in the other thing becaus thei tri it and it work right So you can also kind of get an estim of the brittl and so on of the system in that you see how mani unnecessari thing ar there or how mani thing ar not straightforward how mani thing ar the easiest thing that you would do when you would go about and do what thei did Okai So then you thi conclud thi model or method Usualli thi section is call like method or model or someth like thi And you go to experi Now the main question I have so far or I have mayb I have some more question about the model itself that I been abl to pick up from thi section which is not the case here But I simpli keep those question in mind and see whether thei ar resolv later right So I keep an awar of what I understand But from here on my main issu is ar thei demonstr that their stori work right So here propos a loss and a model And in my mind thei now need to convinc me that that work And not as easi as simpli to show me some number that thei ar good at some benchmark thei need to show me that thei get those number becaus of what thei claim So here thei claim well okai thei propos a new thei propos a new architectur So what thei need to convinc me of is that the architectur itself make sens right But in other paper when when you propos like and when you sai like oh we for exampl in an LSTM when you build in an attent mechan and you claim oh we you know the attent mechan can look back at the sourc sequenc in on step then you need to convinc me that that actual happen right So you need to not onli do you need to perform well you need to convinc me that you perform well becaus of what you claim your model doe right So and often difficult And I specif look out in the experi for usual the question is like where ar thei try to bullshit me Right Where ar thei try to our or ar thei try to bullshit me Are thei try to cover up the fact that someth work Now all the experi ar alwai in the best light possibl of cours and you have to keep that in mind But a lot of time you can also alreadi see from the experi that okai ar thei do someth weird Are thei not show me some obviou experi Or And a lot of time becaus is there an easier explan for why thei get the result that thei get other than their explan right And it is it is their job to convinc you that their explan is the correct on for these number And especi if there is an easier on that thei exclud and then I believ the experi if the case right If there is an easier explan for the effect veri skeptic But some paper have an easier job here than other paper So in thi paper thei basic show result on a on a on a task And sinc their paper is about hei our pipelin is just easier than other pipelin what thei first of all need to do is thei just need to like match the number of other pipelin And here I see that okai in these result often you have mayb a tabl or someth Here you see like thi their model other model and their model is the best model in a lot of case Now if the best thing is of cours if their model throughout is the best the worst thing is if like scatter like thi even if their model is the best but in everi singl benchmark a differ configur of their model is the best sort of a bad sign unless thei can explicitli explain why that is And also not that good of a sign if these thing ar spread out like thi like sometim the baselin is good sometim their model is better and so on So pai attent to that Now in thi paper it matter so much actual fine becaus what try to show is that their model is on par and wai easier And alreadi made the case in what wai it is easier easier in term of architectur If thei were to sai much faster then after that I would expect you know an experi in speed while these number ar match But sinc thei sai easier alreadi seen the architectur convinc of that Now that thei show okai our number match or actual surpris thei even outperform a lot of time then quit happi with these experi So also look for differ between number and the spread of number Now not easi to sai what if like point on is a big or a small differ that depend on the task But if you know pai attent to these thing pai attent to the fact that these result ar noisi And oftentim there is a lot more hyper paramet tune go into the model of the paper then into the baselin model So I do want to make your look your stuff look as good as possibl And here is a littl bit where the institut credibl of someon like Facebook come in in that I tend to believ their result a bit more than other result not mega but a bit more Yeah also look at pattern that thei point out in the text So if there is like a pattern if you see like an interact between the number of paramet and the score or someth like thi just try to be on the lookout of that and see if you can spot someth that you think or think about whether that make sens or not in what your hypothesi would be So here we go on and okai then thei go into ablat and a lot of a lot of these paper do ablat and I gener appreci that So here thei visual that the attent mechan in their model actual refer to differ instanc right encod self attent for a set of refer point the encod is abl to separ individu instanc and you can see that pretti clearli right here where and even here with the overlap cow And thi is the sort of experi that I would expect that actual convinc me that their architectur doe what it sai that it doe right And someth like thi where you see like total overlap thing with the attent of the individu thing visual So tell me like especi thi on right here the the foot of the back eleph actual be focus by the attent of the bound box of the back eleph the sort of experi that convinc me that their claim like that their number realli come from what thei claim it come from Okai so at the end of the experiment section you should alwai ask yourself have thei realli convinc me that their stori is true right That the improv or when whenev thei get an improv or whatev thei get whether is is due to the stori that thei want to sell me Or could there be like an easier explan Or doe someth not fit is like ar there ar the experi differ than from what you would expect here Okai so these ar these ar my main question Are thei ar thei convinc me of their stori not Do thei have state of the art number I care I care Even though like sometim So there is a bit of a catch I I care about state of the art number Now sai you have a tabl like thi And you have a comput vision model And on of the model is like on the CIFAR data set Now if your baselin model ha like a accuraci on CIFAR when I know the state of the art is I care right I know like done CIFAR I know with like I know five six layer CNN you can reach these accuraci And to get to the actual be like in the region of a wide re net and whatnot So I I know that even though a few point behind state of the art I know you know thi thi is valid still so I care But if you were to be like at accuraci on CIFAR then I then I get a bit like hmm I like pretti easi to get to plu with like a standard CNN So there I immedi start to wonder why is there an explan Now thi could be like a theoret paper that sai Oh we investig MLP And why we onli get that number So that would be fine But if someth is out of the ordinari like thi then I pai attent but never becaus someth like the latest and greatest state of the art just dumb Okai And also if onli evalu what the paper claim it doe right If the paper sai we want to show that we ar on par with current model then be mad if the paper outperform these model Thei claim that right So yeah So after these ablat actual pretti happi right here with the result And thi right here when I saw thi I I expect that But I read the experi descript that these ar these differ learn object queri and what thei do And that gave me an increas understand of how these object queri actual work Right So at that point I still had like a vagu I knew that these ar learn But read thi and sort of look at it studi it a bit I wa like oh okai then I understood even better what thei ar So again when I sai understand everyth in the method section you can still have question And but you just have to keep it in mind for later And then here I go on And thi detr for panopt segment And thei here thei propos like a new model So I first look at it and like okai thei propos a new model thei can do stuff like thi Now thi is not object detect And again not sure is thi like a is thi like a an add on to the method Or is wa wa thi up here just an intermedi step to thi And honestli after read that I still sure It seem like someth in between of cours the paper is also a bit longer than other paper It just it seem too long for just be a side note But too short for be it own thing So that wa just a bit weird And I treat it as as just like a oh we can also do thi with our model But I pai like too much attent to that Okai So at the end I you know look at conclus Now the conclus of a paper ar much much often thei ar not nearli as inform as the introduct the conclus thei all often tend to be veri gener and kind of hedg a bit against critic sai what would be up for futur work which is again hedg against critic Becaus you simpli sai well we do thi futur work Yeah so again I read it but I realli pai attent to it And then I gloss over the abstract I just would kind of scroll through the abstract if someth that catch my ey I would look at it And if not then not And then I basic go to the start And whenev I understand someth I go back I look at it again And I try to think ar all my question answer And have thei suffici convinc me that their stori is the thing that realli ha the effect right here And then if I now were to make a video of thi often found it us to just put the paper awai for a while And I usual get the best result when I read the paper the dai befor and then make a video the dai after Or if not just you know put it awai do someth els do some email respond program go outsid eat lunch just some kind of a break between first read or between first coupl of read And just I even think about the paper I just kind of just in the subconsci it kind of brew right And I happen to think about the paper everi now and then But I make a consciou effort to be like Oh how am I go to explain thi and so on But I just found the worst video ar the on where I immedi make the video after read a paper And just discov that if I kind of take a break and then I look at it again right I look I read it fulli again But I if I have if I have the feel understood it I read it fulli again But I just kind of look at it and go again through the stori And I think even if you you know want to if you want to talk about a paper in a read group or tell you know explain it to your friend or whatnot Thi is often veri us just put it awai for a while let it mellow And I find that help a lot Okai that wa my process of read thi particular paper Now we again thi thi is a high qualiti paper So I find a pretti easi read in that I simpli need to understand what thei did And pretti happi with their experi I mayb next time I can find an experi or a paper where initi more skeptic about what thei did Where initi more skeptic and not as happi with what I find But yeah let me know if you enjoi thi or if you would like to see ani other explan I exactli know if thi is what you expect from a video like thi So let me know Mayb misunderstood you complet or wai too long wai too detail or wai too undetail Yeah leav me a comment and see 
A. I. Learns to Play Starcraft 2 (Reinforcement Learning)
StarCraft II is a multiplay game where the object is simpli to elimin the other player You do so with an armi of attack unit We like thi We do not like thi Those attack unit cost resourc to make so you probabl need to set up a base build variou build collect resourc and then you can build that armi All of thi involv seemingli endless strateg and knowledg of the game along with year of tune muscl memori to achiev high action per minut But programm and we can just us librari that let us us Python code to plai the game for us Even thi howev take a while to master all of the strategi But also machin learn practition Could we just simpli import some reinforc learn from Stabl Baselin and plai the game I guess go to be a littl harder than that But sai that we realli do want to us deep reinforc learn to plai some StarCraft II How might we actual do that Like most deep learn problem at the macro level we have to consid what our input and what our output ar go to be as these ar the thing that we actual have control of and we have to determin the characterist of these ahead of time For the input what inform do we think the neural network will actual need In thi case it could quit liter be the game visual like the frame from the game It could be some other graphic represent or it could just be a vector of valu like number of unit and miner and locat of thing and so on Then for the output of thi model we need to consid what thi neural network can actual decid Essential with deep reinforc learn in thi case thi is go to be the action that the model can actual take like whether to expand and build out our base to attack the enemi and so on For the input into the model I think someth like the mini-map is actual a realli good candid for model input Show the actual game would mean that the agent would have to come up with some logic to either move the camera around or it would probabl wind up mostli pai attent to the mini-map anywai So I think build our own version of the mini-map with the data that we think is import I think the best place to start from For the input map go to start with a blank canva that is just simpli the size of the game map Then first go to draw the miner Not onli ar we go to draw them but also make them more bright the more miner there ar left us the same concept for miner as well as the ga but also a similar idea for build and unit health the result of thi so far We can onli gather inform on miner that we can see with unit which is why some of them ar just veri dim and then some ar veri bright The dim on we just we seen so we have no idea how mani miner ar there Next mark the enemi start locat with a red dot Then go to us a similar strategi for enemi unit in anoth red and then enemi structur as well in a red color Then go to mark our own nexu build in a light blue color and then all the other structur a greenish color go to mark Vespen as a pink do thi after our structur becaus if we just slap a structur on top of the Vespen we actual know what the amount of Vespen wa right You can onli have on thing on a squar So I just went with draw the build first becaus we end up build a build on top of the ga to collect the ga So draw the build first then draw the Vespen Then final draw all our unit which will be green by default but go to color the void rai unit which is our attack unit specif a blue I think thi give us a pretti good final result where fairli easi to see go on in the game I did most of my R&D here in Linux which allow me to run the graphic represent of the game and just by watch game with thi represent I think realli easi to see go on how do and all that I think you could definit make good decis with thi inform So I think thi will be a pretti good start point for input to our model Will it work Mayb Rememb the best thing that we can do is keep thing simpl to start One issu I can alreadi imagin here is that thi is just a ton of mostli nois per input The imag is mostli empti so go to be an area that we could probabl improv upon later But now that we have network input we need to handl for the output logic which will be the macro action that go to allow the reinforc learn agent to take For action zero refer to it as the expand action Thi is the code for it Essential for expand we want to make sure we have enough suppli to grow and build as mani void rai as desir If we have that then check to make sure we have enough probe These ar essenti the worker unit If not build more of those If we need those then build assimil to collect the vespin ga Final if we have all of thi stuff then make anoth base in anoth resourc area For action on all about build stargat and to build stargat we need gatewai and cybernet core to also exist If those exist build them Otherwis build a stargat Right now the code is limit on stargat per base but thi could be chang to allow the AI to make endless stargat theoret speed up the abil for you to build void rai faster For action number two if we can afford and we can build a void rai build on Action number three thi is to scout Just not too often Essential scout is send a poor probe to a certain death and just do thi to see what the enemi is up to Initial go to start out complet random So theoret on out of everi six frame we could be send a probe and thi would be faster than we could possibl build probe So just to limit thi ad a onc everi frame when you can send anoth scout Action four is to attack First go to look for ani close unit then ani close build then ani known unit then ani known build per void rai If all els fail then go to send the void rai to the enemi start locat We could probabl get a littl more fanci here sinc total possibl that we erad the enemi from it start locat and we have no idea where it is from thi point but somewher build it base and collect resourc and all that So we might want to have some sort of abil to hunt or seek out the enemi in other locat Final action five is mostli tuck tail and flee The void rai should also be capabl of just simpli return to the base after a fight So not just to flee also just to bring our void rai back home The void rai unit ar realli our onli base defens here So if not busi elsewher we should bring them back sometim At thi point the last thing that we need to do is calcul a reward for the agent to learn from But first is thi even a worthi experi at all I can alreadi hear some peopl complain that the agent control everi singl action in the game and we must ask ourselv how will we know if thi agent is good or not We need a benchmark to compar to So befor we wast our time on a reward mechan and all the R&D requir there which honestli is often the hardest part of ani reinforc learn develop we realli do need to test random which is realli just a question of how good doe an agent do if it is just randomli pick action We have a lot of helper code here some fail safe it realli just possibl that randomli pick action will also win game or at least be remot compar to ani reinforc learn agent Well we can test thi realli easili After run game run random action we have not won a singl game Okai I think we have someth we can certainli work for All right now talk reward In order for the agent to learn anyth at all we need to reward it for do quot unquot good Reward agent in reinforc learn is often the hardest part of all Take our problem for exampl the actual goal here is to win game pretti simpl If we win we could give the agent a reward and if we lose a big neg reward The issu is as seen alreadi the agent start off basic random and the number of observ and action per game can be to or even more sometim If we go through action and lose the game as we pretti much know we initi will everi singl time how will the agent know which action were the on that lost the game and more importantli which action would be good to take With thi mani action to sift through befor get an actual reward far too challeng for a model to discern which on wa the good action and which on were the bad action So instead go to need some sort of intermediari reward to help boost the activ that we think will be us for victori To do thi you can just give a veri small reward per step for thing that you think might be help and then you can still give on larg reward at the end for actual win the game So what might we reward to help win We could reward resourc gather but thi would probabl just result in an agent priorit lengthen the game so it can collect as mani resourc as possibl and it probabl want to build mani attack unit sinc these cost resourc We could go off of total unit and build as well as a sort of reward for the overal size of our militari but again thi would disincentiv the agent from actual win and complet the game Instead it would just want to build lot and stai aliv for as long as possibl So for me I person settl on someth extrem simpl A reward for attack For everi void rai which is our attack unit that we have in our armi if that unit is activ in combat then thei earn the agent a small reward per step Thi incentiv actual elimin or at least attack the enemi It will still possibl be the case that keep the enemi just bare aliv to make more unit and build for us to destroi will happen but my hope is that a larg reward for actual finish the game will overcom thi Okai so got some great idea here but we still have to actual connect Stabl Baselin in thi Python StarCraft environ and to do that we also need to convert the Python environ to an OpenAI Gym environ in order to work with Stabl Baselin Due to the wai everyth run and can or cannot commun while it run thi present a far more difficult challeng than I expect If look to learn more about Stabl Baselin and custom environ put a link in the descript for a tutori seri I did a lot more simpl and easier to understand if new to it compar to what go to be do here So for everyth to commun here I end up go with a state file Thi file contain the state which is go to be our mini map the reward action and whether or not the game is done And thi share file is how commun between the system here Is there a better wai Absolut Will thi work though I hope so So thi is definit go to be veri confus with everyth that ha to connect and wait on each other and all thi but just go in logic order So to actual run everyth just go to have a train script trainpi and thi is go to actual run and start up basic everyth So for trainpi realli noth fanci here a typic Stabl Baselin train script The script run the file which I shown yet for our custom environ So now peek at thi file So what thi is is a custom OpenAI gym environ Essential we have an init step and a reset method Again to learn more about thi check out the tutori in the descript be much more easi to understand than thi but basic all thi doe is initi the action and observ space for the agent immedi Then befor the environ actual run it is reset and here is where the magic or hacki code happen In the reset method we empti the map and prepar our state pickl file Note that the action in thi pickl dictionari is none to start After creat and save that pickl we open up anoth process to actual run the StarCraft game So thi is actual run the game with the logic that kind of describ up into thi point as well as through the Blizzard API and actual plai StarCraft Final the reset method actual is just return an observ which is that map that blank map that creat So at thi point theoret the StarCraft game is just kind of hang out wait for us to issu command to it So return thi observ from the reset method which Stabl Baselin via thi trainpi file pass through the reinforc learn model In thi case a ppo model and that model return an action Stabl Baselin then pass that action back to our step method In the step method the first thing we wait for is an action to be pick by the model Once we have that action we add it to the state dictionari Keep in mind again up to thi point the action ha been none So actual wait for an action Our actual StarCraft game and the script that commun with it which is Incredibotpi Once that script detect a new action it perform the action actual in the game It redraw that map that creat It calcul ani reward then replac the previou state dictionari with a new on where yet again the action is none But return the reward and a new observ also handl at the end for if the game actual end From here which script is wait on an action the script and the cycl goe on until the game end which will issu a done and then the environ reset and we continu on indefinit So yeah thi is definit the most spaghettifi code for reinforc learn that written but doe it work Of cours it Around thi time I spent quit a while tinker with variou reward and logic but the on that work best wa just a simpl reward for attack and of cours win or lose While the model were train I wa obvious track thing like overal reward The reward mechan itself chang quit a few time so each variant necessarili directli compar to the other on as well as I have person found that best to try multipl time with the same exact reinforc learn algorithm as it seem the random initi start point of all model can actual plai quit a signific role with reinforc learn and that actual model learn over time As you can probabl tell I made mani attempt befor settl on that veri simpl reward mechan of just simpli reward for combat and then at the end a reward or punish for a winner loss The best model land on a reward of about at it peak That said what doe that actual translat to As model train I also kept a log of win and loss sinc actual what truli after so I want to measur the win rate In the case of thi model at the same time as it reward peak we can see game victori at around against the hard comput bot which too bad especi consid random wa liter we were never win Despit the amount of effort put in so far I would still consid thi just the begin of ani reinforc learn with StarCraft and have to think more on how I might expand thi to do better at both macro task like done here as well as some sort of micro reinforc learn algorithm probabl in the form of two separ algorithm run at the same time where on algorithm is focus on macro like what we have here and make larg overarch decis like build build and expand or attack and so on and then anoth on that focus sole on micro task for exampl like what coordin to go to or build on and so on but overal sai thi is a pretti good start I did also attempt to build some sort of hunt capabl for the void rai to find enemi at more locat than just their base or if thei just happen to show up sinc it seem like mani game were lost becaus the enemi just hid when we had the advantag but later came with a bigger armi and defeat us I end up allow void rai to look at random coordin if no other enemi were present but my result here were actual wors than when we look at all I still think someth here to be fix logic probabl fairli simpli but again I wonder about mayb instead make a reinforc learn algorithm that choos what coordin to fly to rather than me hand code even more logic I think make a micro strategi reinforc learn algorithm to pick coordin for travel is veri like the next step to try That howev is a project for anoth dai host all the code as well as a decent model that you can plai with if like you can find that in the descript of thi video If interest in learn more about neural network and how thei work then you might want to check out Neural Network from Scratch a book by myself and Daniel Kukiewa insid of which learn how to code neuron activ function how to calcul loss do optim and back propag and of cours appli everyth that just learn to a problem The book itself is in full color for graph and code syntax highlight on high qualiti page with full font and a comfort line space to make read easi Final everi concept includ the math and code is broken down into the truli atom part and then put back togeth step by step to give a full understand of neural network and how thei work includ all of the math behind them By the end of the book you will have creat your own neural network framework from scratch Thi is the book that I wish exist when I wa learn deep learn and extrem happi to be abl to offer it to you If thi sound interest to you then you can head over to nnfsio to learn more and bui yourself a copi 
SynFlow: Pruning neural networks without any data by iteratively conserving synaptic flow
Hi there todai look at prune neural network without ani data by iter conserv synapt flow by Hidenori Tanaka Daniel Kunin Daniel LK Yamin and Surya Ganguli So thi paper on a high level doe what the lotteri ticket hypothesi doe but doe so without ani data it prune a neural network at the begin And it doe so abl to do that becaus it claim that it algorithm avoid thi problem call layer collaps and then is base on conserv a quantiti thei call the the synapt flow And go to look at thi And pretti cool algorithm it seem to work pretti well As alwai if you want to help out you can share thi video And let me know in the comment what you think of it I do read the comment And I would love to hear from you Alright dive in So sai prune the paramet of deep neural network ha gener intens interest due to potenti save in time memori and energi both dure train and at test time Recent work have identifi through an expens sequenc of train and prune cycl the exist of win lotteri ticket or spars trainabl sub network at initi So what is thi paper talk about If you if you know much about prune here is kind of a basic overview So if you have a neural network that consist of mani mani layer of neuron what you can do that on wai of prune that what what the goal is is to end up with a small neural network that perform well But for now we have a big neural network that perform well it been train yet right So what you can do is you can first train the neural network And then you have a big neural network that perform well and then you can prune it Now a lot of time a lot of the time thi thi ha been seen as sort of the prune wai you would train the big neural network and then you would prune it becaus the other wai wa not feasibl First prune and then train wa not feasibl You might you might ask okai we might just want to start with a small on And yeah correct So what doe thi first wai by you thi first wai is bui you mainli two thing So imagin thi network right here is much smaller than the origin network So it is less it us less storag So you can potenti if you want to ship it to like a custom over the internet you mayb instead of a gigabyt you onli have to transfer a few megabyt And pretti cool The second thing if you prune in the correct wai you can also make it faster becaus now less weight to multipli with you can actual make it go faster So prune is a now thi thi combin with techniqu call distil and so on is our wai to make the network smaller and faster So if your custom ar for exampl on on mobil phone then you can ship you can train a big network to a good perform on your big GPU server and then ship it out to a mobil phone onc small and it will perform fairli well on that mobil phone without GPU So what about thi other wai Now in order to do the other wai we would sort of have to have an idea which on of these big network which sub part of these layer ar the good on right in order for us to do thi first prune and then train The interest thing is that the paper the lotteri ticket hypothesi done a video on thi and also interview the author on our ML street talk podcast Thi paper ha shown that thi is in fact possibl A long time peopl have thought we need the big network in order to train right we sort of the big of the network the full connected of the network is requir for the train dynam But thi paper ha shown thi is not possibl you can prune at the veri begin Now what doe it do It first train in neural network like in the in the olden dai then it prune the neural network and then it rememb which connect of the train neural network it ha prune And then it simpli goe back to the begin of train right here up here and sai I now know which connect ar import And simpli go to prune all the other connect other than these on And then interestingli if you prune first and then train that work just as well and can actual work even better The interest thing here is that I mean thi is a big big cycl But the interest thing that the paper demonstr is that thi is even possibl right Peopl thought it possibl And thi paper demonstr if you onli knew if you onli knew which on you must retain you can prune at the begin of train The lotteri ticket hypothesi paper though still requir to actual train the full network and then do the prune like in a classic wai in order to find out which on you need to prune and which on you Thi paper right here take that idea and sai can we find Can we find a prune algorithm that prune at the begin of train yet doe not have to train the full network in fact look at ani of the data Okai and thi is go to be our start point So their stori is go to be quit an involv stori And I think the overview is import as we go through the paper So first thei name thi problem call layer collaps Now layer collaps is go to be whenev a prune algorithm remov the entireti of a neural network layer which mean that no inform can flow anymor and therefor the network train And thei claim that thi is the main problem why these current prune algorithm cannot achiev veri high prune ratio So can like veri high compress ratio is becaus thei do prematur layer collaps Thei then formul thi maxim critic compress axiom that ha sort of a guid principl to build prune algorithm Second thei show that thi quantiti call synapt salienc a gener class of gradient base score for prune is conserv at everi hidden unit layer of a neural network So thei show that these ar conserv And thei show thi becaus thi is their their argument is go to be first the argument is layer collaps is a problem The second argument is these thing ar conserv and these the conserv of the synapt salienc lead to the layer collaps And go to see how that happen And then third thei sai the solut to that is iter prune So thei show thi at the at the exampl of iter magnitud prune which we know avoid layer collaps So iter magnitud prune is someth that happen in thi lotteri ticket wai of of do it Thi lotteri ticket wai you can actual do it not in on step but it tend to work better when you want to go from of your weight to just of your weight it tend to work better if you do it in stage So first you go to to to and so on down to your desir thing And thi iter procedur thei claim is what is what circumv thi problem of layer collaps And then at last thei sai we prove that a prune algorithm avoid layer collaps entir and satisfi blah blah blah if it us iter posit synapt salienc score So thei bring it all togeth and sai if an algorithm satisfi our axiom and if the algorithm is an algorithm that us these salienc score like thi on here and if the algorithm is iter then it is not go to be subject to layer collaps and therefor it is go to be abl to compress to a veri high compress ratio And then thei actual do suggest an algorithm thi synapt sorri iter synapt flow prune SynFlow that doe all of thi and never look at ani data Alright thi is quit a stori But rememb what do first layer collaps a problem Second why is layer collaps a problem becaus of thi synapt salienc conserv Third we can avoid it by do iter prune and lastli thi algorithm doe it without look at data Okai so layer layer collaps layer collaps is a pretti simpl phenomenon alreadi said it if you have a neural network and it ha a bunch of layer and draw a coupl of neuron here and the neuron ar connect to each other via connect connect connect connect And you have a prune algorithm Now the prune algorithm thei consid here ar so call singl shot prune algorithm What thei do is thei look at the neural network and thi can be befor train or after train But thei at some point thei look at the neural network and thei each thei assign a score to each of these weight like sai on a five a nine and so on And then thei simpli prune awai the lowest score Okai And you tell the network what compress ratio you want to you tell the network for exampl pleas prune awai of the connect So these algorithm would look would assign the score onc and then remov the bottom of weight Okai like thi So those ar the singl shot prune algorithm Now what is layer collaps Layer collaps is whenev an algorithm remov all of on layer becaus mayb so here wa a nine mayb you have like here okai so and then in in thi situat right here And the algorithm is pretti pretti dumb simpli remov the bottom of the connect And here it figur I need to remov on more to meet that goal I remov the on with the lowest score go to remov thi on And pretti obviou that now no more inform can flow from the begin to the end of the network becaus well where is it go to float to a bit more complex than that like you can just retain a layer like a connect For exampl if thi were a connect there would also be no inform flow becaus have no outgo connect here But ultim layer collaps is whenev an entir layer is remov Okai And thei thei do sai somewher that the case I think layer collaps here layer collaps occur when an algorithm prune all paramet in a singl weight layer even when prunabl paramet remain elsewher in the network So not as such not sure that thi is like a giant problem It get to be a problem but it could be circumv fairli easili right by simpli sai if about to prune a connect integr to the inform flow from the start to the end prune that connect prune some other connect right And then you could simpli avoid that And be interest in how that work out So but in thi case for purpos of thi paper thei simpli consid algorithm that assign a score and then prune the bottom coupl of percent okai for so we want ani like handcraft rule in here or someth So thei look at thi quantiti call the max compress The max compress is a quantiti basic the maximum achiev compress whilst still avoid layer collaps And thei sai for exampl for a network with l layer and n paramet the max compress is n over l which is basic mean everi layer onli ha on paramet remain and therefor if the correct on therefor inform can flow from start to the end All right so thi is the maximum achiev compress anyth beyond that would automat induc layer collaps Now anyth befor that could induc layer collaps but there is a wai to compress the network to the same level without induc layer collaps And their point is basic that these other compress algorithm that thei compar with thei alwai thei alwai induc layer collaps befor thei actual have to becaus thei cut off a connect that lead to layer collaps befor thei like there would be anoth connect that thei could cut off that would not lead to layer collaps And of cours if you ar layer if you have done a layer collaps then you accuraci immedi drop to zero or two random becaus no more inform flow So thei look at these thing here random prune is where you simpli assign a random score to each connect Magnitud prune is what the lotteri ticket hypothesi doe but just thei look here at a singl shot So you simpli look at the magnitud of the weight and thi can be befor after train I think thei do it after train here which is classic done you look at the magnitud of the weight and you prune the bottom awai There ar also two more advanc method these SNIP and the GRASP so SNIP and GRASP which look at the gradient of the train loss in the network and thei decid accord to that gradient which which thing to cut and which thing not to cut awai The GRASP even involv the Hessian right here So fairli you know complex method that have some thought behind them about why thei do what thei do yet thei all induc layer collaps befor thei actual have to So thei defin thi thing here call the critic compress The critic the critic compress is the maxim compress ratio a given algorithm can achiev without induc layer collaps So the critic compress here is basic whenev that algorithm goe to zero the critic compress kind of the farthest you can push the algorithm without him without sorri German speaker without it induc without it induc layer collaps Okai so you can see that for these baselin algorithm the layer collaps occur wai below the theoret possibl max compress And go to see that in their algorithm thi sin flow that thi max compress is achiev And actual achiev without ani of these handcraft rule that I mention it is the algorithm by design alreadi achiev thi maximum compress ratio So thei formul thi here as a guid principl Thei formul is an axiom I would I would rather sai like thi kind of a kind of a guid principl of build these algorithm that ani algorithm you build should have So the critic compress ratio of a prune algorithm appli to a network should alwai equal the max compress of that network It basic mean when you build a prune algorithm if you push that prune algorithm to it limit it should not do layer collaps unless it absolut need to Okai Again the extent of thi problem I I know but thei do demonstr that that thei can push their algorithm a fair bit further Now without induc layer collaps you alreadi see that these other algorithm like in thi regim appar layer collaps happen yet becaus thei still have sizabl accuraci But still a you know there is a reason differ here between those and the sin flow algorithm So not too convinc yet that layer collaps as such is the problem becaus thei have a differ befor their layer collaps as you can see right here And I have a feel that thi differ here is due to thi iter procedur and not actual due to the phenomenon of layer collaps But yeah so if it were onli layer collaps what see is that thei do the same the same the same and then at some point is like boom now I have layer collaps Okai Yeah so the layer collaps stori not sure but part of the stori So go with that The second part which is kind of disconnect So thei establish two thing thei establish a layer collaps problem And now thei establish the synapt salienc which then later go to connect to the layer collaps So the synapt salienc thei sai is a score is ani score metric that can be express as the Hadamard product of thi thing with the paramet Okai So each paramet is go to be multipli by the gradient of some function with respect to that paramet Thei sai where R is a scalar loss function of the output of a feed forward neural network parameter by theta Okai so mani of these prune algorithm can be formul in thi framework right here And their their algorithm can also be formul in thi framework So you can see the score that the algorithm assign to a weight can be defin as such And as I said mani fall into thi categori or ar similar to thi especi for exampl thei sai when R is the train loss L so thi is the simplest case you take you put data through the network and then you take the train loss of that data and you sort of back propag it And now go to prune these connect accord to how big the gradient is If you sai the gradient is veri big that must mean the connect is veri import Becaus lot of inform flow through it So if the train loss L the result synapt salienc metric is equival to the score metric us in skeleton on of the first network prune algorithm The result metric metric is also close relat to thi right here Now thi you can see not exactli the same but close relat to the on us in thi snip baselin and also close relat to thi thing right here us in grasp where not just the gradient actual the gradient multipli by the hessian to account for curvatur Okai so go to investig thi synapt salienc in neural network Thei formul two theorem right here about the conserv of synapt salienc Rememb synapt salienc is ani score that respect thi that is built like thi ani score s The conserv of synapt salienc all synapt salienc metric respect two surpris conserv law that hold at ani initi and step in train So these ar not usual like in distribut or someth like thi with high probabl These thing hold at ani point in the neural network First is the neuron wise conserv of synapt salienc For a feed forward neural network with homogen activ function and a homogen activ function is an activ function that is can be express like thi for exampl ReLU fall into that categori the sum of the synapt salienc for the incom paramet is to a hidden neuron is equal to the sum of the synapt salienc for the outgo paramet from the hidden neuron So what doe it mean actual pretti simpl If you have a hidden neuron and you look at all the incom weight and you look at their synapt salienc which is thi s score of each of these weight like what would the prune algorithm assign to that and you look at the outgo on then the sum of all the incom on is go to be equal to the sum of all the outgo on So pretti interest And thei extend that to layer to the entir network So an extens of that network wise conserv of synapt salienc the sum of the synapt salienc across ani set of paramet that exactli separ the input neuron from the output neuron of a feed forward neural network with homogen activ function equal that So it basic sai it remain equal So what doe it mean It mean to exactli separ the input from the output basic the definit of a layer in a neural network So what sai is that you have a bunch of layer And if you look at a particular layer like thi on here and you look at the incom connect and you sum up all of their synapt salienc go to be equal to the sum of all the synapt salienc of the outgo connect of that layer And it can also appli to like a group of layer and so on But the synapt salienc is conserv in that wai Now why is that import And here is where we make the connect with the layer wa it later drop layer whatev okai the fact that the fact that these algorithm tend to drop entir layer befor thei have to if you have in your network layer that ar of differ size so you have larg layer and then smaller layer and smaller layer what will happen is that sinc the synapt salienc is conserv the sum is conserv if you have more connect in on layer so lot of connect lot of connect and in the small layer you have as mani connect the sum is equal So that mean each individu on here is much much smaller So the S is veri small for each individu on here and the S is veri larg in there That mean the prune algorithm is go to realli realli kill off thi these connect in the big layer right And actual go to kill them off to a point where it probabl is go to elimin that layer befor it even prune mani of the connect of the small layer just becaus of that conserv fact And thei do experi like thi I think an experi up here where I like thi on down here better where thei basic show that you have invers layer size on the bottom and you have the averag score that the prune algorithm assign to ani connect Now these as seen not exactli assign the score of thi salienc but veri close to it The Synflow algorithm doe exactli assign the synapt salienc as the score for the prune Now basic seen that thi lead to a bad result but the synapt flow is go to compens for that But in essenc as you can see as the layer get so invers layer size grow which mean that layer size shrink as the layer size get smaller the averag score of the connect in the layer is higher and higher which basic mean that the prune algorithm if you just let it go by itself go to kill off the smaller sorri the larger layer first becaus thei have the smaller score And you can see that even though the other algorithm conform exactli to that thei conform to thi approxim So these here becaus their score is close relat to what the Synflow doe and the magnitud prune becaus mostli becaus now not sure if at the end of the train at the begin of the train if you just initi then the score is go to be proport to go to be proport to their magnitud and their magnitud is determin by the initi scheme And the initi scheme is most of the time like modern initi scheme compens for the fact that you have differ number of incom and outgo connect and therefor thei would automat assign higher number sorri a higher initi constant to layer that have the lower number of paramet So even the magnitud prune will conform to thi Now it might be absolut reason to sai that also the case at the end of train becaus most paramet go to move super much dure train So thi still approxim hold as you can see here Of cours the random on do that Yet becaus you prune randomli still absolut subject to thi layer collaps In fact in the random on the smallest layer would be the on to go awai first becaus just more probabl Okai So discov that if you do someth like salienc score or someth correl to it then go to remov the biggest layer first And a problem And what thei sai thi con thi fact of thi conserv law and the singl shot natur of these algorithm that thei onli assign score onc and then thei prune awai whatev the bottom such and such percent ar lead to layer collaps Right I think establish thi now that the combin of the two thing lead to layer collaps Now thei make a littl bit of an excurs And thei sai there is actual someth that run into layer collaps And iter prune algorithm So specif thei look at magnitud prune Thei sai magnitud prune which rememb is also if you do it singl shot it also run into layer collaps magnitud prune avoid layer collaps with conserv and iter So becaus it iter it avoid that And what these lotteri ticket hypothesi paper doe It doe it iter remov a coupl of connect then it retrain the network basic recomput the magnitud and therefor recomput the score and then it prune again and then it recomput and compend prune again And by recomput you can basic these some of the connect that import befor but just surviv the prune thei can now be like wait I have now wai more respons as a connect and thei will shoot up in import to avoid be prune So you can see if you push your network to a sorri to a high compress ratio then if you just do thi singl shot prune you run into thi layer collaps at some compress ratio you simpli crash to random perform or zero perform Yet if you do multipl iter you can see here alreadi two iter then much longer befor you run right here into layer collaps And if you do three iter you do much more Now thi the three iter mean you prune more like at thi at thi point right here to tend to the on All of these thing prune nine out of everi connect just the thing that ha three iter prune mayb first three and then again three out of the Wherea the on iter would prune all of the nine at in on go And thei give a reason for thi thei give thei sai that the fact that gradient descent encourag conserv So thei give a littl toi exampl here thei sai to better understand the dynam of the IMP algorithm dure train a littl smaller we will consid the a differenti score thi on So thi is not exactli magnitud prune but it is veri close right The squar just the squar of the paramet instead of the absolut valu of the paramet Thei sai algorithm equival to magnitud score Consid these score throughout train with gradient descent on a loss function us an infinitesim step In thi set the tempor deriv of the paramet is equival to that and thu the tempor deriv of the score is thi So now go to look at how doe the score evolv when thei train the network And the score evolv exactli as the neg to the salienc Surprisingli thi is a form of synapt salienc and thu the neuron wise and layer wise conserv law from section four appli In particular thi impli that for ani two layer of a simpl fulli connect network then thi quantiti hold So that thi is not new But what it basic sai is that through train these connect equal the salienc again So if you have a veri big layer and here a veri veri small layer and becaus a big layer these score ar veri much lower right just littl s and here big S per layer But then if you prune awai and you run gradient descent on thi these score will tend to becom bigger And in thi case these weight will tend to grow in magnitud becaus prune awai the other thei now have more signal probabl flow to them and more gradient flow to them And therefor go to grow in size And therefor their score is go to be bigger So thi gradient descent of thi iter procedur make the score better for that So basic counteract the layer collaps So thei put all of thi togeth and sai theorem three iter posit conserv score achiev maxim critic compress If a prune algorithm with global mask and global mask mean that you rank all of the connect and then prune from all of the connect a differ to layerwis mask where you sai I want to remov of each layer which sound like it would avoid layer collaps but also it work a lot wors than the global on the global strategi assign posit score that respect layerwis conserv And if the algorithm so respect layerwis conserv it basic mean you your score should be or if your score is a salienc score then the case And if the algorithm reevalu the score everi time a paramet is prune then the algorithm satisfi the maxim critic compress axiom So basic sai that if you have ani algorithm that prune with a salienc score like their is go to do is go to be abl to be push to the limit until the maxim capac is reach if you reevalu the score everi time a paramet is prune So thi is basic sai that whatev the lotteri ticket hypothesi paper did with magnitud prune if you do it with salienc base prune guarante to achiev the maximum possibl compress if you if you push it But of cours we know that whatev the whatev the lotteri ticket hypothesi paper did is impract becaus it need to retrain the network everi singl time it want to prune right If go to do thi after everi paramet go to be a long time go to be impract We ideal want to prune the network befor we even look at ani data And go to do exactli that with the Synflow algorithm Thei sai theorem three directli motiv the design of our novel prune algorithm Synflow that provabl reach maxim critic score Oh no Okai thi wa bad Malari critic compress First the necess for iter sorri first the necess for iter score evalu discourag algorithm that involv backpropag on batch of data and instead motiv the develop of an effici data independ score procedur Second posit and conserv motiv motiv probabl motiv the construct of a loss function that yield posit synapt salienc score We combin these insight and introduc a new loss function where the on is the all on vector Okai so thi is the loss function of their salienc score And thi might seem like so what do we have We have the paramet of layer L the absolut product sorri the absolut valu of those paramet And then you simpli multipli all of the layer togeth And you have thi product here with the on on the side So thi is a quadrat form sort of Okai thi might seem a bit weird But but in practic and thi is also what happen in their code you can do someth pretti easi So first you have to transform all your weight to their absolut valu Now in their code you can look at it thei thei do rememb the sign for later So but first you convert all of them to their absolut valu then second you simpli take a data point that is fill with on that liter the number on So if your if your input is an imag you just put a on at each pixel you feed it through the network with all of these posit weight and you get out some output you get some output vector okai then you simpli you need to do thi Thi inner product with the on vector which is simpli a sum right I I get why thei a bit of a funki wai of write a sum right You simpli sum that up to get a to get a singl number And thi singl number now is your is your pseudo loss function simpli the loss function that an all on data point get when the when the loss function is just the sum of the output it And then you back propag that loss to you back propag that loss to the layer Right So thi is our rememb thi is not the score itself but our score is go to be the deriv of our with respect to a weight time that weight Okai so you want to back propag and then you multipli each of these weight by the back propag signal And go to be your score for each paramet Now thi seem too hard right You just need you even need a batch you need a singl data point on back propag and then you get your score Okai you need expens train or anyth like thi Thi seem pretti cool And thei give an exampl here For exampl for a simpl come on for a simpl fulli connect network ie thi so thei consid here a linear network right just so we can look at exactli what happen for linear network you can often comput quantiti exactli So if we look at just a linear network without non linear we can factor the synapt flow score for for ani paramet as such So the score thi is now not the R thi is go to be the score is go to be thi thing right here So you can see that the paramet is multipli by thi thing and by thi thing And other than for exampl magnitud prune thi actual take into account all the input flow becaus it goe from thi on sorri goe from thi goe from thi on it goe through all the network right everi path that arriv at thi particular weight is go to be consid And everi path that goe out from thi particular weight is go to be consid And the salienc score is go to depend on all of these path all of these all of the inform flow from input to output that goe through that weight And if you do thi then you get a realli good prune algorithm So yeah the algorithm is is alreadi describ it And in their experi as you can see right now thei have a bunch of network these VGG network or like wide ResNet thei have a bunch of data set like tini imag net or where thei experi with these differ baselin And you can see that the baselin often run into thi layer collaps problem Sorri often run into thi where all of a sudden actual look at look at thi ResNet right here Mayb you can find a connect between mayb differ size layer in ResNet And why the collaps happen even earlier But you can see right here a collaps if you do magnitud prune even also if you do random prune it fall down pretti hard after a while the baselin thei hold up better but you can see in differ model and differ data set that the baselin crash at some point as well Now alreadi said the comparison here it seem a littl bit unfair I might I might have over read someth but pretti sure that the baselin remain singl shot While the sin flow algorithm here is now of cours no longer singl shot actual multi shot and made the exact argument that the singl shot is the problem And therefor their algorithm is multi multi shot And it seem like thei should give the other algorithm the opportun to also do multi shot just to compar them fairli As I said mayb do that but I read ani anyth So it you know it just seem like the comparison is a bit unfair if you identifi the problem and then just leav the other algorithm with the problem Sin flow is still differ from these other algorithm even if thei had the multipl step Now the counter argument to thi of cours is that these other algorithm all requir the train data thei requir actual pass the data or train the network in the case of magnitud prune and so on So pretti expens wherea sin flow you simpli pass forward on data point and it a good argument But it seem like the effect of the synapt salienc score and the effect of the multipl step realli disentangl in these experi right here It simpli show that it consist outperform other prune method And what like to see is realli where that outperform come from Okai so what I think of thi And that wa the paper basic even even if not convinc quit yet thi is pretti cool right And I think thi will if not be if not us itself it will inspir kind of a line of work into prune at the begin of train without look at data And mayb you know mayb we can even think of build network like instead of just prune them we can think of construct build network that observ these properti And therefor we can just construct initi network alreadi with good properti such that we even have to go to a bigger network and then prune it down It seem wast It seem like we should just be abl to deriv principl of what we want in the how the weight ar structur and then construct network that ar accord to that And I guess go to happen in a few paper that ar come Alright again if you like thi video consid subscrib give it a like comment and let me know what you think 
I COOKED A RECIPE MADE BY A.I. | Cooking with GPT-3 (Don&#39;t try this at home)
Jona is just look up adject for bad food I think gonna need them Look at thi stuff gonna go to the store bui some random stuff put it all into an AI that gener recip and commit right now to cook Can you just move your hand in a kind of random manner And eat whatev it output All right everyon thi is Jona He is an expert in non-convex optim and also a veri veri good cook Mamma go to be extra spici for him todai when he ha to follow instruct by not-so-good cook which is the languag model Yeah do it Awesom So the plan gonna go to the store and each of us is just gonna bui some random item We know what the other person is bui All right real realli weird And come back and whatev we have put into and ask us to gener a recip for it And try to follow that recip as close as possibl As close as possibl As close as possibl And then whatev come out gonna eat it And if it turn out great gonna give it a try as well No just kid both gonna eat it commit now do thi Absolut So a coupl of rule Rule number on Jona is a vegan which mean that todai go full neutral absolut organ healthi cow-friendli ethic perfect vegan Yeah Just yeah Rule number two gonna follow the recip as close as possibl If it suggest an ingredi that we happen to have go to put it in If we need to wait for a coupl of hour come on got time But other than that do whatev it sai lot of video on how to do bike Probabl done it yet on Minc Meat And rule number three we must finish our porridg Are you readi Total do it do it To the kitchen To the kitchen All right we ar back from the store and we got ourselv a whole bunch of food wai too much Jona how wa the experi It wa love So we went shop and we found lot of tasti healthi vegan food item I am veri sorri about that but that wa my restrict sorri Janik So todai go to be a vegan dai All right We have pretti normal stuff Thi is an avocado not just an avocado organ avocado Well I have to check the imprint Nice nice actual imprint never seen that You should start do that We got some vegan plant-bas butter How ugli is that Have you tri thi befor Yeah pretti good actual God Tofu the classic The stapl We also have vegan plant-bas What is thi made from minc meat made of no cow and no pork made of pea Pea Yeah Probabl other good stuff Probabl tast like pea too All right What els we got We got pomegran chocol garlic sweet potato mushroom What els man Kale We have the superfood here Jesu Christ all in here If not go to be so hipster after thi we got kale Kale We have these tasti Gwurzburgruten How is thi ever How is the chocol ever go to be not ani chocol a cook chocol Of cours And we have soi Soi Whip cream Soi whip cream Okai beauti All right Orang soi cream go to put all thi into and whatev it spit out go to cook it And go to eat it go to eat it train at OpenAI is a giant neural network call a transform with over billion paramet It is train as a languag model which mean that if you give it a piec of text it can predict what the text will look like that follow it It can do so with remark accuraci and just like a human would can do it in multipl wai So you can sampl mani time given the same start text and you will receiv mani differ answer can do thi becaus it ha been train on a scrape of the entir internet In a wai it is the collect knowledg of humankind at least what ha been written down in the internet So see if we can make that collect knowledg work to gener on recip Now rememb that I said that you can sampl from the model and get multipl answer We were a bit disingenu here in that we sampl a few time to make sure that the recip wa reason long and contain at least some funni part though we genuin were readi to accept whatev came out as long as we could do it in a few hour So what we did here is we input our list of ingredi and then let the model gener the recip The model is usual pretti consist and output actual regular recip though I think the fact that we sampl a few time plu the fact that we gave it such a weird combin of ingredi made it a littl bit thrown off Okai reduc the size of your prompt Damn You have too mani ingredi man Thi must be like dirti We have salt and pepper Thi is wai too littl Wait it Thi is too littl The other instruct ar not long enough I guess Yeah serv the bread with mustard and pomegran on top Shred the carrot and grate the chees What chees Still not as good Not as good Not as good So at the end we got a recip that we were reason satisfi with and we went ahead and cook The recip start out with us boil the potato and carrot which wa definit a good surpris for me becaus I wa worri as unboil potato realli someth nice to consum So at least had the foresight to boil potato Then step two In the meantim prepar the vegan minc meat or us precook soi meat Jona also enhanc our meat with some veri skill shamanist procedur No vike no hipster man The recip went on ask us to fry the butter add the garlic Comput scienc peopl how you do garlic How do you do garlic Smash it You can just peel off the Add the mushroom Oh total gonna kill us And stir for two minut So far so good gonna add soi cream stir and cook for three minut Okai Thi is the soi cream Add it add it add it come on All the wai yeah Three minut cool Next time set Tell all your vegan friend to subscrib to channel Thi is come along nice Step five Add the pickl tomato and bean Stir and simmer for anoth five minut So the pickl ar in there and look tasti Thi recip so bad until now We actual we have pepper Thi is alreadi burn go absolut great Next come the bread Cut the bread in small squar and fry in the vegan butter until golden brown A chunk of butter that gonna put into the pan We decid to take a new pan for thi instead of ad the bread to whatev we had alreadi See thi Thi is the last thing your arteri see befor thei go Okai we have to put the bread now You readi Sure put the bread Next cut the lime into cube and squeez the juic into the bean mixtur Easier said than done Step eight Add the soi sauc parslei salt pepper cumin cilantro Where did it come up with that All right go to leav that awai as per our rule if we have it Do you have cumin No I know Good And dri fig In the meantim the do great Also the potato look super healthi And the carrot Should we ever stop boil the potato though It sai so I think at some point we should stop Mayb later We exactli have all of that but we made some substitut I have ketchup I mean we can total get ketchup just go to replac the cumin and the cilantro with the coriand Yeah look better and better actual We total need to figur out a name for thi recip The GPT toast or someth like that Add the kale Kale cannot be unhealthi Step nine Pour the bean mix into a blender The blender Thi is where the recip start to turn a bit Blend the bean mix wa definit a first for me but it wa a lot of fun I have to admit One And whatev go to come togeth all in your stomach anywai So who care Step Bake for five minut in the oven at degre Celsiu Celsiu Celsiu for you American Oh beauti I think three blue on brown had a nice mnemon where you distribut degre Celsiu onto like a semi-circl So here you have thi you have a semi-circl and then here is like degre Celsiu and here is degre Celsiu and here is zero And so if I want to like degre Celsiu then thi angl right here just take thi which is like degre So thi is like degre I add and that give me like the So degre Celsiu is like Fahrenheit Is that correct I know It fit Mayb you should first take it out but ChibiDrew sai so It seem a bit pointless to bake someth for five minut but we trust the recip Step cut the sweet potato in cube and add to a pot with the remain butter What more butter Come on go to have to do workout to compens for thi What am I suppos to do with the carrot sai Oh the carrot So the carrot never ever enter the recip With the remain butter add the red bean mixtur Yeah So the carrot just out of the game now Add the red bean The most surpris part about thi is that thi wa probabl the exact point when the potato were cook the best So prop to for time us so perfectli We then had to cut the bell pepper into cube add to the pot and add the vegan minc meat You can actual eat thi raw right You can but not do it Right thi is kind of sticki Minc meat is there What is thi Thi is the rest of the minc meat Yeah we have enough butter becaus you put all the butter in the pot Look the carrot ar still aliv Come on carrot part of the game part of the team We need you And cook everyth in the oven at degre for minut more Once that came out we ad the avocado chickpea Okai skip the chickpea skip the chickpea The chocol and serv on bread with mustard and pomegran on top It might not be the most obviou choic but thi wa the ingredi that we gave to so we had to do someth with them And kudo to the model that it wait until the veri last second until it ad the ingredi that it realli want to add and I realli want to eat togeth At the end we got a nice warm meal and we were absolut thrill to see what it would tast like Are you readi What part ar you go to start with We commit The sandwich with the chocol and the mustard on top I think get myself a nice piec of chocol bean lime avocado carrot Wait definit definit make sure to add some of the pickl Fatti butteri brat Nice Mustard and pomegran Uncook kale No not yet I need some of the minc meat Okai Minc meat and the chocol You have the chocol piec too I have the chocol do the chocol Come on chocol Formid Chin chin my friend Thank you Yeah Enjoi I like the chocol part all togeth sweet and salti and bitter and sour and butteri Oh my God The sweet potato I like the sour part of it There must be the lemon We have wai too much lemon in there Like two entir lemon Well thei told us to And the pickl I mean come on Have you ever cook like fri a pickl befor just actual surpris the sweet potato ar cook through We had them in the pot for like an hour almost Yeah So why not for that almost done Janik Oh my God The carrot It be the same without the Did thi grow No I know All right Thi is the last piec of not fulli chop garlic How do you like it Excellent So thi is just the bread go to eat some but I feel Yeah I think more like a low carb gui I feel fulfil our duti just the bread remain The rest is done Awesom Excellent Excellent Well thank everyon for watch If you have recip idea pleas send them to us Yeah Subscrib Check out Googl Scholar Review hi paper Accept them Strong accept Strong accept Smash accept And yeah Bye bye Stai healthi 
Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents (+Author)
Hello there todai look at languag model as zero shot planner extract action knowledg for embodi agent And go to interview the first author Wenlong Huang in a few minut So first an explan of the paper minut or so gonna try to keep to it And then we jump into the interview where we can discuss thi paper at length On a high level thi paper ask can we us the knowledg that is inher in larg languag model like or surprisingli open codec in order to do plan in what thei call embodi agent Ultimat go to be thi environ right here the I even know what the virtual home environ And about a virtual home you have to fulfil some task like brush your teeth then the model ha to come up with a sequenc of step that ar admiss by the environ So a level of admiss of action predefin action that ar admiss the model ha to come up with these action in order to fulfil the task the model is then rate base on execut and correct of their plan And it turn out that the larger the model get as you can see right here the less execut the plan becom which mean that the action thei gener admiss by the environ probabl becaus the model ar more sai power thei can express themselv in more wai thei have differ idea of how to reach goal Howev the correct thi is human evalu of these model rise as thei grow larger So thi give you an indic that the larg model seem to have quit a lot of knowledg And we have to sai these ar not train the entir paper just work except for on baselin evalu just work with pre train model thei ar not fine tune at all on thi environ right here So what thi paper doe is it sai well given that the larger the model get the more correct their plan ar can we do someth to fix the issu with the execut to that thei develop these translat procedur right here these ar three specif improv thei do to the model in order to get their execut up you can see thei sacrific like a littl bit of the correct but thei do make the plan larg execut in the environ And therefor procedur like thi could be appli in mani differ wai not onli about the virtual home environ and so on essenti anywher where you bring togeth the knowledg that is inher in larg languag model with some sort of a domain specif languag or a grammar or ani anyth like thi like where you have to transfer that knowledg into a new domain but you want to train a model to do so So go to see how thei do it realli briefli First of all the environ itself as I alreadi said is thi now thi is visual although thei never work you know actual in just a small correct here becaus I mess thi up There ar actual two version of the virtual home environ One is a Python version that focus on the textual interact with the environ The other on is implement in Uniti and actual doe work in The develop of the environ mostli focu on the Uniti environ becaus more real But as of yet that ha a subset of the action avail that the Python environ ha And the author of the paper us the Python environ and the data set that come along with that go to go into thi more in the interview Stai tune Thei simpli grab the data set of possibl task some task you can see right here a task could be throwawai paper anoth task could be brush teeth and there be a sequenc of step Thi environ is made by human So the task ar made by human and then other human have to come up with the step that ar admiss admiss action in thi environ There ar I believ a number of object that ar defin predefin Yeah so there ar a number of object for exampl live room televis sofa and so on And there ar a number of verb So walk find switch on and so on And not everi verb object combin is possibl Some verb have two object and so on But essenti you combin the predefin verb and the predefin object and then the state of the world chang So the world keep track of state there ar certain precondit For exampl you can probabl onli sit on the sofa if you ar in the vicin of it So you need to first find the sofa you can onli switch on the televis Similarli if you have first found the televis or walk to the televis or someth like thi if the televis is in the live room you first need to go to the live room and so on So a hidden kind of a state But all of thi is construct And we talk about thi in the interview like the appropri granular of action like thi And thi a major issu but it is made all with the human in the loop So the data set is suppos to be kind of the most natur express of these task as split into step that a human would come up with So thi is the grammar of the environ And the languag model thei thei know about thi grammar just languag model So what thei do is thei take someth like and thei make a prompt Now the prompt as you might know in you have to give a prompt So the prompt could just be like the task you know blah blah blah brush your teeth then step on right And then will probabl it will probabl even gener step two and three and four but it will probabl not be accord to the these action in these templat you can help thi a littl bit by put a prompt up here So the prompt thei us is on I believ on specif plan So thei have alreadi like task up here some task and then some number of step so that the model kind of know what is expect We also talk about thi in the interview and thi could potenti be improv by multipl multipl prompt and so on But in the baselin thei have on particular prompt and then on of the improv is actual to select a more optim prompt Thi is the basic setup you have a goal in thi environ with a fix grammar and you task you input thi right here to your languag model And the languag model will spit out the plan Now what do you do with the plan the plan you score like how good is the plan and thei have two differ score avail One is execut And execut is just like essenti parsabl by the environ So in execut you ask yourself can it be correctli pars which mean that is the syntax accord to the syntax of the environ and thei do have a littl translat procedur like a littl heurist translat procedur for the baselin in place so that the languag model probabl get it exactli right But thei do sort of translat to the closest action there But also on of the improv is relat to thi And then also doe it satisfi the common sens constraint of the environ And these would be program in like for exampl you can onli pour yourself a glass of milk if you first open the fridg and grab the milk thi can be measur directli what cannot be measur that well is correct So these model thei would come up with plan and independ of whether execut or not thei could be correct right And where thei ask human So thei us human evalu thei conduct human evalu in order to score the correct of whatev these model output So thei give it to human ask the human doe thi look like a sensibl plan in order to brush your teeth and the human would either sai ye or no when thei do like ablat and so on Thei also us like longest common sub sequenc between two program and so on in order to not spend ginorm amount of monei on human But essenti the correct metric is a human metric also interest becaus you thought you could just execut like the plan in the environ and that give you like doe it succe or not but thei sai correctli that for a task like make breakfast not realli a defin end condit that you could program into the environ to give a reward So more accur to ask human whether a plan is correct As you might have guess thi environ is veri human centric made by human with human in the loop and so on suppos to realli be sort of a represent of human task and human plan to human task Alright so now go into the improv there ar three distinct improv thei make So if thei just do thi if thei just do what describ so far then the graph up here result exclud the two model on the right you can see the larger the model get the higher their correct but the wors their execut So now the thought is can we chang that Can we rais the execut And so thi is the baselin right here zero shot plan via causal larg languag model you put in a task as a prompt and along with like the format you expect which is thi on right here which is some other task from the data set then you us the pre train languag model like GPT three or someth And that will give you a plan And it So the next thing thei do is thei do what thei call a translat model So thei introduc a second model which is also pre train And thi is not train on translat just train on mask larg languag model So think of thi like thi is just BERT In fact I believ thei us sentenc BERT just pre train on English languag And what thei do is thei make a big vocabulari of all the admiss action So all the admiss action would just be like ani combin between ani verb and ani object that would actual go with that that is admiss to thi verb So from thi thei make like a giant list of all of the admiss action And then thei emb that giant list So thei put thi into some embed space us the sentenc BERT model pre train right And then whenev the larg languag model output someth thei implement it into the plan directli thei first emb whatev the model output put thi over here thei emb it sai that becom thi right here then thei see the nearest neighbor of my admiss action to thi thing And then thei simpli replac whatev the model output with the nearest neighbor And thei call that thei call that translat So essenti it translat from gener natur languag space into the space of the admiss action or the grammar of the model Now thi ha some problem on it own For exampl if the model output the compound action So if it sai for exampl squeez out the glob of lotion and put it in your mouth or so or on your face I guess then well appli lotion anywher squeez out the glob of lotion and put it on your skin that would be still on action Now which on would be the closest right here go to be somewher like squeez out a bit of lotion and the other on is go to be like put the lotion on your skin Yet you onli have on action like on line So on action it just contain like an end Now the end might be easi to recogn but there ar other there ar go to be other like compound action And thi is go to be a problem here becaus you just map on action to on admiss action But in ani case do thi alreadi help a lot even though there ar still some problem To allevi the rest of the problem thei have two more improv The first improv thei do is thei sai well if a compound action we can still kind of allevi that a littl bit So in the origin method what thei did is thei simpli took thi through the through the languag model and thei got out just a list of step right Here is step on here is step two here is step three and so on That is just a list of step And thei would translat even when thei us the translat model thei would translat each of them to a admiss action translat thi on to an admiss action Well now you have no idea of whether that sequenc of admiss action even make sens right For exampl on could be a compound action and it just get translat to on of the two action and then the next action have a precondit So what thei do is thei interleav the two step right thei interleav thi translat with the gener So thei would onli gener on step at a time like step on then thei were translat and then thei would us the translat version and put it back into the languag model to get step two That wai the languag model alwai is condit on admiss action instead of just be free form and then translat after the fact So thi is autoregress gener The last improv thei make which is I guess more of a minor improv why not in thi diagram Howev what thei do is instead of have a gener prompt what thei do is thei take the task thei emb it us the same sentenc bird embed and thei compar it to embed of all of the task that thei have in the data set And thei just pick the closest task in the data set to act as a prompt which could still transfer some in context knowledg in you know for the current task So that is essenti the method thei investig thi thei have a algorithm right here thei also like thei I formul it in a rather easi wai but thei do not onli consid the closest action thei consid actual a weight of in so in the translat thei consid a weight between how close it is it to an admiss action and how like is that action that thei that thei output So thei would gener not onli on action and then translat it thei would actual gener a bunch of variant And thei consid each on of them like how close is it to an admiss action and also how like is it and then thei take the best combin of the two that is obvious modul by a hyper paramet Thei have earli stop and all of thi kind of stuff And thi result in thi result in a neat in a neat algorithm that and go to talk about these thing in a bit and also the also the result right here I just I want to highlight that if you look at for exampl vanilla ha a realli low execut it doe have a high correct Howev if you look at the translat version which is after their improv you can see the execut ha risen dramat while the correct is a bit lower like you get a bit lower in correct becaus of the whole translat procedur and so on mock with the output human mai not like it as much Thi is all stuff go to touch on in the interview Just interestingli highlight that codec like the codec model seem to be score quit well on these task So also the translat codec is much smaller Howev it score high realli high So paramet for paramet the codec model is actual pretti pretti good at thi which wa a surpris to me So I think thi is an excit paper It it except as I said for a fine tune baselin it turn out to work with complet without ani train just evalu so to sai And I like it And I think thi doe have applic like get the knowledg out of these larg languag model is someth we should you know be get better at do Otherwis I think we make full us of them Alright so now I want to jump into the interview with Wenlong I hope you enjoi that as well Tell me how you like these these video with the interview without the interview anyth you want in the comment see you Bye bye Welcom everyon Todai with me here is Wenlong Huang who is the first author of the paper about languag model as zero shot planner and veri veri happi to have you here Welcom Wenlong Thank you Yane Yeah super super happi to be here And thi is alreadi told you but thi paper is differ And I like differ paper And differ in a wai that mayb expect Everi it seem like everi dai we find a new applic for these larg languag model and yet anoth thing that thei can do here And when I when I saw thi I wa remind of a friend of mine who had like similar idea but it never realli materi I tri some of thi stuff as well combin larg languag model with plan with tell me what to do in the real world I even made a video where GPT three told me a recip and then I cook the rest like me and my friend we cook the recip and so on But it seem like alwai a bit a bit out of place a bit a bit off just to give you detail instruct And when I saw a paper that wa realli try to make thi work in a real environ I wa I wa veri happi to see that And yeah that is that is thi paper And also to be said you have a stellar board of of co collabor right here How did thi come about Like how did you even get to the idea Hei I could us these languag model to do plan Wa it like did it immedi come to you Did it sort of build up from some basic idea or what wa the process So yeah thank for the brief So I think actual came out to be realli surpris to us as well So first we were just have when we just plai around with the largest languag model on the mani of the web interfac we found that like actual someth there like you said if you ask it for a recip or we actual origin studi like whether you can offer the staff for make coffe etc So we found that like when the most get larg enough actual someth there And thi is the sign of life I think for us to kind of go on and investig how we can make that actual us for agent So we kind of just start from there And actual it came out to be pretti surpris Origin we felt like mayb we need some train data set to mayb like train someth a translat or someth to actual make it us But it turn out like but we realli try to constrain ourselv in the meantim becaus we want it to be tailor to a specif environ So we would just want to see like just the languag model itself like how well can do how far it can go So thi is what got us in the end We just like explor for like two month and then found like you can actual do thi without ani ani train And it Yeah actual truli surpris And actual actual a realli fun process for me as well It sound like fun Yeah just try to see whether you can offer someth like realli realist and realli fun Yeah Yeah So you came you came across thi thi environ right here thi virtual home environ Wa thi alwai the plan Or why did you choos like there ar a million environ open AI gym and and move you know these Mujoko kind of robot simul Why wa thi on particularli us Did you immedi think of thi on Or how did thi came about Thank Yeah So actual I do too much research in thi embodi agent area especi for thi like realli high level task And then I actual went went to the like Googl Scholar and then search for appropri environ for thi And we found thi virtual home environ And we realli like it becaus it actual can model ani ani task if you can express them in term of thi like textual languag plan like a like just just like textual plan So and actual there ar mani mani other environ as well But some of them ar limit by I think a lot of peopl also us Alfred environ a realli good environ too And I think a bit more structur there But the task ar often come from like a templat So usual like pick someth pull someth But actual there ar a lot of challeng there I think a differ set of challeng And we found like what the virtual home tackl is exactli what we look for becaus it can model like ani task express in freeform languag especi those like realli challeng task like peopl do actual everi dai like make breakfast make tea make coffe and then a particular care about the common sens constraint in them So specif thi environ ha a set of like precondit and post condit for each action So for exampl if you want to grab a glass of milk from from the fridg you can just like sai go to the fridg and grab glass of milk becaus you first got to open the fridg first And then like prefer you want to close the fridg afterward So realli thi like thi constraint I think ar realli us and realli interest to studi whether languag model can handl thi And investig sever differ languag model And just to be clear thi environ it ha thi kind of syntax it ha veri defin thing you can do And somewher I think you sai about action that ar ultim possibl kind of a combin of a bunch of verb which ar grab open go to and lift or thing like thi and a bunch of object like kitchen fridg and so on So ani plan would consist of a sequenc of verb object verb object like here walk to kitchen open fridg grab milk So the the the ani planner in thi environ would have to output thi syntax directli Now you had a plan of not train anyth right You want to train anyth you simpli want to investig what knowledg is alreadi there in the languag model And you came up with kind of a wai to translat that you want to mayb elabor how do you how do you queri these languag model And how do you make them actual conform to the to the the the syntax here Of cours yeah So the the wai that virtual home express thi action ar via like thi specif format where you put a squar bracket like for the action atom action like grab food open and then you put I think a parenthesi or Yeah someth for for the argument And but the problem is like we just like expect languag model to handl thi Becaus I mean even if we put an exampl in front mayb thei can do it But definit not the wai that usual human produc languag So and after all thi languag model ar train on human text So we decid like mayb not the the right wai to queri thi model Mayb we just want to have you ever tri Have you tri let them output directli the syntax Or wa it just like Yeah not gonna work anywai I tri briefli but definit not thoroughli investig And yeah like it like intuit wise I think definit sure to to to us like natur languag but we did adopt for the the most basic approach that we can we can think of which is like just defin a straight up like templat for each atom action And actual becaus thi atom action ar simpl enough like just walk grab and those thing So thi atom action I mean the templat the templat we actual came up with our I think actual just natur wai like peopl peopl sai thing So like turn off someth turn off someth and then add some some word in between like in on on top of etc And then and then you just queri these model And you have multipl wai of evalu thi right You care about two thing you care about correct and you care about execut And in at least so you also make us of human like how did you how did you design Like what wa your think behind design the evalu Yeah so actual it came up to be realli challeng to evalu these thing Like I said so like thi thi task ar becaus express free form languag So that mean realli open end So it might be determinist whether like if you want to grab a glass of milk you just want to look in the end whether you have a glass of milk But if you realli think about it if we want to constrain anyth in the task that we want to want to do like make breakfast like what is the correct wai to make breakfast everyon ha differ prefer So hard for us actual I think still a challeng In thi sort of task is like realli determin the correct sorri the success rate for each task So you realli tell if a task is realli success depend on how open end it is So we decid that okai so if hard to computation produc a metric for success rate like but as human we can definit tell if make someth semant meaning So thi us part of like human evalu to do thi but we want to entir reli on human Becaus as you can tell for the for the text that like for the action plan that real languag model gener so realist that like thei can even fool mani human that like ar too realist So you can just entir reli on human to sai if success So we also us thi metric execut which is also us in in past paper from in like that us virtual home So we just us thi metric as well to basic determin whether the plan satisfi the common sens constraint in thi environ name just like whether you like open make sure to open the fridg befor grab someth from it Yeah like thi interest becaus when the human rate it the human would also skip a bunch of step right If you said if you tell a human go to the fridg and grab a glass of milk the human will go like Oh yeah of cours All right which is which is on of my mayb thi is jump ahead a littl bit But on of the question I had most when I read thi wa just there is a level of specif that is requir right here which is kind of ambigu right You have a high level descript which is like make breakfast right And then you have a bunch of step which you need to follow And sure these step correspond to action in the environ So kind of given by that But the languag model know that right The languag model just know I need to produc a plan So how is the languag model You know why do why do we expect the languag model to figur out that it need to like that it that it need to sai open the fridg befor you get a glass But it for exampl it need to sai put on foot in front of the other foot you know in order to walk So you know did you have ani insight or concern with like there seem to be like a veri specif level of specif of these plan Yeah so a realli good question Actualli thi granular actual come from the data set or the virtual environ itself becaus the wai becaus we essenti follow the format of virtual environ and also thi data set thei collect from human of how to do thi realli like human activ task So the wai thei collect thei build thi environ is the first ask mani human to come up with a set of task that thei do in everydai household And then thei ask a differ group of human to come up with a detail plan that can drive a robot to do to perform thi task And and after that thei build thi environ base on the verb us by those human So you can think of like thi environ is realli built on top of what human sai Now now the develop who would just sai like okai we want thi granular you want thi like walk grab and those etc So thei actual ask thi human to give those word and verb and then build those action accord to those verb And thei did make sure to for each of the verb to develop a set of common sens constraint which complet make sens And I think actual like reason exhaust for those action So if you want to grab someth you definit need to make sure the thing you grab is not within a close contain for exampl So in thi case the fridg is a contain and it ha thi attribut of be open or be close So thei intern keep track of the attribut for each of the object And then to make sure that like if you do someth like thi you violat the common sens constraint So to answer your question so like thi granular realli depend on the human And like I think thi is where languag model realli shine becaus it essenti languag model is train on human produc text So my hypothesi although thi is definit not someth onli test by my hypothesi is that becaus train on human produc text and human after all produc these action So if you do it care enough and then us some techniqu to properli translat them or do someth els you can essenti get back someth similar to what human produc in the begin Yeah I mean you would imagin that sort of the huma of how the environ wa built would also be present a littl bit in these languag model which make sens I have a better idea like of how to build an environ like thi So yeah I think pretti reason Yeah actual not to be realli like interest to me becaus like just super hard for me if I were to develop thi environ like how would you even anim like all thi like realli like human task even just in a household set super difficult And I think thei did a realli good job here And then I think thi is also what make like languag model particularli us for thi task becaus these ar basic just human task And languag model ar realli good at like mimick human Yeah Yeah So on the on the left here we see a bunch of model that evalu right here So again execut is sort of how like if it if it match the syntax of the environ if I can map it to that and also I guess if it if it violat ani of these common sens constraint So just like how execut is the plan in the environ no matter whether the wrong thing right And that come in in a minut second and correct is a thing that is rate by human annot thei look at the plan that wa produc and thei just from their own intuit ar like Well is thi a good plan to make breakfast Ye or no And we clearli see like there is thi downward trend if we exclud the model on the right there is thi trend line here where the larger model thei seem to produc more correct plan which mean plan that the human like more but thei ar less less execut Wherea the smaller model thei ar less correct which you know we can best correct I would have expect that but more execut Yeah And notic in the paper that veri often thei just produc plan that have noth to do with the task descript thei will just produc like a plan And that is accord to the syntax of the exampl that you give in the prompt right But how can you explain that Like even on the top here like the larg model even better than human at correct So human rate other human think that produc more correct correct plan Why is it so bad at execut Yeah So there ar actual two question that like I think you eras on is why thi like smaller model like like when I sai smaller actual still pretti larg the larg model So why do thei produc like more execut plan And the second question is why the the larg model is actual better than human So to answer the first question I think becaus we did find some failur mode here for smaller model I think the two most promin on ar first it frequent tri to like repeat the given exampl For exampl you give it like how to brows internet instead like go out to the comput and us type on the keyboard etc And then you ask it to brush teeth it still goe like goe to the comput and then type out on the keyboard So total noth like sensibl here And the second sourc of error is sometim it just output realli short plan If you sai like sleep task go to sleep just like go to go to the bedroom and just stop So thi right here brush teeth just like go to bathroom Yeah exactli So when these plan ar short enough even though it can be execut like if you just sai like walk to bathroom and walk to the bedroom just on singl action like for walk not much like common sens constraint there So like you can total imagin like super execut But if you present them to human of cours like human will spot thi and then sai Okai thi is not correct Becaus we when we do human evalu we were try to make it simpl so that the the error here is not too big Becaus we ask like hundr of human to evalu thi We onli ask got to ask evalu in thi case So so why like did thi smaller model ar now realli good at execut And and the second question that you ask is why thi like larger model ar actual better than human So we actual thi is not a complet fair comparison if you just look at on axi So all the result here will be look at from two axi that we care about So on is the semant correct which is evalu by human And the second is the execut So thi human plan that we us ar from thi data set that virtual home develop like crowdsourc from from from Amazon Turker So thi plan thei make sure that like these ar execut plan So which mean that thei have on like here Yeah be over here Yeah but but we want to put a spot right there Yeah on the right becaus hard to see becaus human ar a big baselin and refer here not the baselin that try to beat of cours like GPD three is out there yet in term of like at the same time output correct action plan and semant semant correct action plan and also be abl to realli ground them in the environ But us thi to access we can realli see for exampl which is the which axi is the place that as a commun that we mai want to work more on to get it better to to get the human level and with thi paper that we kind of find thi result actual a bit interest to us is that like for thi larger model like in term of semant correct you need to worri too much about it kind of alreadi there if you if you if you do it extract them But the real question is how do we make them execut for agent that we that we care about And thi is exactli what you do right in the in like the meat of the paper and the result or these these translat model right here that you know notabl thei do drop a littl bit in term of their correct as rate by human but thei gain massiv in execut And thi is the result of a bunch of differ ingredi like three main ingredi as far as I could tell you quickli want to go like tell what like what the ingredi ar to make whatev these model output into what someth that I mean you know the virtual home is mayb a test bed right not I see thi paper be about virtual home more like here is a model that output someth yet I need the output in some other form right in in thi is veri gener problem ha mani applic And if we could solv that bridg that technic is you know is a big game exactli what you do So how did you go about thi Yeah so I sai I just want to make sure that actual thi paper just present a realli like preliminari staff I think anyth particularli I mean it doe like if thi problem but a big step I believ like I mean you the execut rais pretti pretti high I I want to oversel you but not not undersel you certainli Yeah So um but to answer the question so so so we actual found like actual as you said there ar three ingredi but central to thi is on simpl realli simpl techniqu that we found the most us which is action translat So becaus in thi virtual home environ the action that it support ar ar a limit set I mean not small but someth that we can definit enumer with our comput hardwar and in like in a realli quick manner So like just like on of a second or someth someth like that So sai if we can enumer all the action that ar support by by the environ then the question now becom how do we translat the thi realli sensibl action plan gener by languag model for but not realli execut plan How can we translat that into those action support by environ or if you want to deploi someth deploi someth in the in the real world sai your robot onli support action How do you map those task into the action that the robot support So what we found is that you first need to enumer all the action And then we found that you can again leverag the the world knowledg in thi languag model by us anoth languag model Name here we reus Roberta which is a languag model realli similar to BERT And a differ languag model becaus it essenti a mass languag model So realli good at output a us embed like in term of about the semant mean for that sentenc So what we do is that we take the sentenc output by GPT three or codec and then we just compar that against all the possibl admiss action allow action by the environ And then we found the most similar on in term of like thi distanc in the embed space Yeah we actual us just cosin distanc and found that to work decent well Yeah I have like like an entir space somewher and you just place all the action I guess you can even pre comput those right you can pre comput the embed of all possibl action there And onc my languag model output anyth at all all I need to do is ship it through the Roberta model get it embed put it somewher get the nearest neighbor And my kind of translat action So here we have an exampl that would that would translat like squeez out a glob of lotion into poor lotion into right hand Yeah thi it would map Yeah action into and poor poor it would be the verb lotion kind of the object and right hand also on of the object So mayb like two argument to pour Yeah I mean thi make it is it seem veri simpl But I wa at a talk by the peopl who made the first version of the you know in Gmail you you have these alwai like three option to respond to like the quick quick option to respond right Yeah And and I think the first not sure how it is done now But the first version of thi we were like wow thi is you know you know cool It actual take you know it take into account the the email messag that wa there we alwai thought it wa kind of like a languag model gener model somewher So I went to talk and thei were just like No we just have a big list of respons we just classifi right Whatev We just take your messag right And we just put it through a model And then we just classifi into thi big big bucket of possibl answer So I mean thi is even though it is it is simpl veri power power method And that be said you even train thi you take an off the shelf embed model and you comput nearest neighbor And it doe turn out quit well you do Howev you talk about thi in the paper there is a bunch of problem And on of the problem I see is whenev a step contain like multipl step right Is that like is that a big if you found thi to be a big problem becaus thi just map on action to on other action But if like you know open the fridg and take a glass of milk then I have essenti no wai of translat that into an admiss sequenc Yeah a a good question And I think on of the main error that like thi thi oper model that we us actual a sentenc oper model becaus train with a differ object such that it can realli you can actual calcul thi cosin distanc between the embed thei gener So a like we found like pretti difficult to map a compound action like you said like like two action in on sentenc into on admiss action But thi is partli mitig by how you tune the temperatur set the sampl paramet just the temperatur for the or codex model Becaus we found that if you do increas the temperatur then it tend to output someth more verbal express answer for each step So that mean harder to translat And we if you if you try like all thi like differ set we did we in the end we found like usual you want to us like lower temperatur than than what peopl mostli us for languag gener for exampl Yeah so that like each action is like small enough and succinct enough And then and then after we translat thi action so that easier for thi bird model Roberta model to translat And yeah someth I forgot to mention like after we got thi translat action we found that too us to put that back to the origin prompt for the translat action back instead of like the origin action so that you can lie the and codex model to to reason like how am I go to do base on thi like action alreadi perform So yeah like you said like you point thi is the third sub figur here So we would take instead of instead of gener the entir plan at onc we just gener on action then we translat it and then we substitut essenti whatev output with whatev the translat thing is and then base on that creat the next action it make sens becaus you like almost like a guid like a bit of a guardrail for for the languag model Instead if you were to let it gener all at onc and then you translat each action individu thei almost like lose connect to each other right Becaus thi here might mitig some of thi thi stuff readi for a compound action like go to the fridg and grab a glass And the closest I hope that the closest sentenc is the go to fridg right The languag model might still recov and recogn aha I you know grab grab a glass yet So that is so these ar improv on and two And then the third the third thing you found that realli help is the prompt up here So the prime which I think in veri common to have these prime prompt to tell the model what kind of stuff you you expect as an output I wa surpris to see that you onli have on prime prompt Wherea in gener peopl put more than on usual peopl put like three or someth like thi Is there a particular reason why you us just on There is actual not a particular reason I actual found like I mean in the begin we were we know that we have thi data set right And then we we found origin we actual tri to train someth to achiev thi But in the end we found out like we even need to train someth And like now the question becom like like can you even leverag thi data set to some extent to make it us Of cours thi is someth like addit I mean it would definit be better without ani of thi But if you have thi data set you can actual found like thi most similar exampl to the queri task here For exampl like thi is appli lotion So like shave task shave is determin to be most similar Again judg by thi Roberta model us the same techniqu Yeah So I think that the main motiv for us thi But we thoroughli investig it like how you structur the prompt whether you add like multipl thing there and then or you chang the templat here Becaus I just defin thi templat from dai on like task someth step on someth step two someth Mayb there is a better templat Mayb you want to add some instruct there to make it better And so I like I mean thi is definit possibl And we investig them here becaus we just want to get the best perform out of thi We want to show peopl like thi is someth possibl and realli interest to us So why we end up like just us the most simpl techniqu here Yeah And to answer your question why we put multipl thing there I think on import reason is like becaus thi exampl plan that we put in front ar produc by human And thi is becaus a space constraint us a like oversimplifi version in thi figur specif But in practic these plan ar actual pretti long And thei actual alreadi take up a lot of space in the prompt So if you put more than on sometim it get too long And I mean mayb someth handleabl by larger model but we just opt for the most simpl case And I actual read thi like a recent paper investig why in context learn work thei frame thi as a implicit Bayesian infer problem And thei did come to a conclus that the longer the prompt if I rememb correctli it help the model So in thi wai you kind of like trade off the number of exampl you put and the length of each exampl So in those case I think you mention mani peopl put mani exampl befor the queri Those ar usual the case where the task thei care about ar smaller So for exampl you want to ask Einstein wa born somewher then thi is just a sentenc So you probabl want to put more than on sentenc there But thi case our case is an extens action plan So alreadi pretti lengthi And we want to go too crazi over here I mean Yeah sorri that the record ha stop on the screen side but we can still see it Yeah so yeah I wa I wa I wa quit interest in in that in the sens of the prompt structur becaus I know that can also make a big differ But I also like the sort of approach of not have too mani move part you know in in on singl in on singl thing becaus it make thing complic And for mani paper it it make you wonder like what wa exactli the thing that gave the improv here Now you you do veri good ablat of all of these differ improv which I realli like And you show that kind of the translat is the main part right here although the other thing certainli also help Have you ever So it remind me a bit of thi you know thi retro model these languag model that retriev from the internet as thei produc text it remind a littl bit of thi right in that you you you produc you go and retriev the closest sampl in the data set as you produc the text It Yeah I think the thi combin of retriev and gener is pick up steam And and it look pretti interest My question is a littl bit have you tri also becaus essenti you now reli on thi translat procedur to to produc the correct action Have you tri ani wai to like let the model know what the possibl action ar like someth like you know I can imagin mayb I you know I ask the model first and then I get mayb the five closest action ar the closest action in embed space And then I somehow put these in the prompt here like you know in between you know what am I go to do next Is it thi or thi or thi or thi right And then the model could mayb I could prime the model to output on of them And you know is there did you try ani ani wai of tell the model more even possibl in the environ Becaus right now essenti reli on on just the languag model itself Yeah a realli good question too So like we actual try the specif thing that you talk about like gener a bunch of possibl action and then ask the model again which of these ar best But we did try someth similar which is like be search So essenti in beam search you look ahead to see like what the organ ar ar like have in the end get the highest likelihood So we did try to constrain the vocabulari that can be us in the beam search But thi is onli conduct on smaller model becaus obvious the and codex model ar now open to fulli open to public So we we realli have full access to differ featur Like you restrict the vocabulari dynam Ye So onli done thi on smaller mode rel smaller model like the GPT-Neo And then I think I might have tri on GPT-J as well which is a billion paramet model And it actual turn out that thei do realli well with if you realli just constrain the vocabulari that wai Yeah specif just the beam search constrain the vocabulari you can gener But so my hypothesi thi is not thoroughli test becaus not invest on larger model as well But my intuit why it work so well is that thi languag model ar realli train on human text So it realli is realli us to how human speak a certain languag in English So like peopl speak thing in thi wai step on someth two someth step three someth So why if you realli constrain the model thi wai a lot of the world knowledg encod in these model ar lost So basic and person just a person opinion I think these model ar do like super intellig reason here basic just do kind of retriev what is train on So retriev thi like larg scale text So if you want to retriev better you better adopt the same wai that human speak a languag So like if you constrain the vocabulari you can get the most out of a languag model And you can realli tell if you adjust a temperatur like if you go differ temperatur thei can tell you like differ level of thing and thei can be realli realist But if you realli constrain it a lot of thi knowledg is lost and realli do too much like common sens reason here You mention thi a bunch of time I wa surpris to find codec as a model And so you have these ar sort of vanilla model And then you have the translat on where all your improv ar in there So there is the action translat there is the sampl even accord accord to the probabl and execut there there is the retriev of the closest prompt and so on And these translat model thei perform realli well What I wa surpris by and also by the result is that codec I mean that even in here like a code model but also that compar it hold up right not as good as the model but also veri veri much smaller So you know paramet by paramet codec is outshin GPT on thi task veri well How did you how did you even consid us codec And how can you explain that thi model is do so well Yeah so on intuit why we actual thi actual came out to be pretti surpris to us as well So we did find like thi codec model ar realli good at gener thi plan and actual from my own experi plai with thi model I did find like codec think that thi is part of some doc stream So actual imagin like peopl just like ask the doc stream here but instead of let keep gener the code we kind of just stop here So okai yeah finish the doc stream for us enough So so yeah so actual do some of thi kind of doc stream It gener thi doc stream thing And I the reason I think the smaller codec model ar actual better than the same size model is that becaus train on a more structur data So like code and specif mani of thi like thi code exampl in the dataset in the train dataset consist of doc stream and the code So not onli can handl code realli well you can also gener realli realist doc stream So and peopl in doc stream thei write in like Yeah thei write a novel Yeah So thei write someth realli step by step and have more structur in it So my intuit why actual doe realli well with thi test So you can realli process thi sequenti like logic reason better than the same size model But of cours if you us a larger model that potenti be more help Yeah Or I mean there is as you said there is still a lot of open like question about how exactli you structur the prompt Like mayb thi step on step two step three ideal for these languag model Mayb you need to more let them write like a Reddit post or someth about how thei went and got a glass of milk yesterdai and then translat that somehow But yeah pretti cool So on thing that just came to my attent right here is thi top row right here which I found hilari So thi is the task is complet Amazon Turk survei So the four step appar that you need to do is walk to home offic sit on chair switch on comput look at comput Like is thi the is thi is thi the thi is the descript of complet Amazon Turk a pretti accur descript mayb of what Amazon Turk worker do So like I said thi task ar gener by crowdsourc from human And thi the human here happen to be Amazon Turker So on of them decid that OK if you want me to gener some task I would sai like just complet survei on Amazon So thei decid to put on of thi here And we found thi hilari too So like I said so thi languag model so thei realli handl anyth that you want to So becaus we did put the exampl in the front So I think in thi case the exampl happen to be someth relat to comput and the model actual happen to reason or potenti you could just repeat the exampl But depend on other task it seem like the case But it doe come to the reason that like thi might be someth relat to comput too And like thi staff here Yeah yeah I mean thi is I mean it ha someth like melanchol And it also ha someth a bit as you said rebelli of like you know here do my my Amazon Turk work gonna you know just gonna put my Easter egg in there in thi in thi data set or or like show you but it also show someth I think about the interact with thi environ becaus you know if you ask me you know what did you do todai I could tell you you know I program thi I view a pull request I sent some email and so on But in the action space of thi environ thi would all just be character as go to desk sit on chair switch on comput look at comput And yeah so so it is realli mayb also a constraint of the environ itself And and and as I said I think the challeng is go to be so much knowledg in these languag model and we somehow need to get it out into the domain that we care about And yeah I guess I guess mani opportun ar still there And in thi particular environ is it so the wai I see it we have thi environ a environ but you never actual for the your studi you never actual had to actual execut anyth in the environ Is that correct Or do I see someth wrong here I think those when you sai ask you do you mean like like run in the environ Yeah like run the environ like actual give it to the environ becaus you evalu execut you can do with a parser right to see whether it match the action and constraint and the correct you evalu with the human Becaus my question wa also a littl bit like why I just run it and see if you know at the end breakfast but you alreadi you alreadi said that the task ar so so like how would you how would you detect breakfast right So so in term of so a bit background here for the virtual environ So it come in two version One is the I think thei call the evolv graph version which is a pure like you said a state machin a Python like read Python So it just goe in and then check which whether the action can be pars and then we satisfi the common sens constraint And the other version thei implement is thi is thi visual visual version where thei actual onli implement a subset of the act the total action support environ So I think thei so in the evolv graph version the Python version there ar action and in the visual version there ar onli action So limit Like the plan we can gener we can realli visual ar limit So also part of the reason we show the visual version to human Like can you tell us whether thi is success or not So yeah a inde someth we can do right now And I think like as a commun as we go go on like to thi next step with more complex task that human do everi dai instead of just like lower level task As a commun I think more effort can be can be put here and to develop better simul and also mayb beyond even household environ So ye just as a as a stori here I did plai around with the codec and then GPT three model to have a gener someth out of the household domain And seem like thei do have some a lot of knowledg for those as well So if you can ask how do how do I pai bill at a restaur And how do I work out at the gym And I think in on Twitter also someon tri to after the post of thi paper thei try to ask the GPT three model how do I start a compani So yeah thei do have a lot of knowledg for thi And as long as you can provid a set of action that ar necessari to complet these task I think no matter what what the granular is ideal it should be at the same granular as of human So ideal it should be thi model should be abl to gener someth someth sensibl and reason But yeah definit right now is someth that you definit can trust to put on a robot Of cours yeah yeah Yeah I mean alwai alwai seen peopl think when thei think GPT three or so thei thei and thei think for exampl of video game thei alwai imagin you know we can have our NPC our charact that the dialogu be gener by GPT three So it the dialogu is more realist But I think thi show that it can go further if we ar abl to map sort of GPT three knowledg into a sort of structur domain that we choos we could potenti also let these model gener the action sequenc of like of charact for exampl sai in video game becaus like common complaint that you know the guard thei alwai walk up and then down and then left and then right and then up and then down And right thei have these even if the dialogu get realli good their their behavior is still kind of lame either that or thei thei cheat thei know where you ar at all time But with I feel with model like thi we can almost like take thi common sens knowledg and mayb have the hope of transfer that to to variou domain and infus a lot of area with common sens And I find that to be I find that to be pretti cool Yeah in itself that would be realli excit and interest applic Yeah Yeah So I mean a lot of a lot of thing to be gai So I what I did I wa specif intrigu about clip I know if you ar think about thi or not But what I what I tri to do is I tri to take like a frame of Pac Man like and and you know like wall here and here and and and here and I had Pac Man be like you know here face a wall and then like a ghost behind Pac Man right And and then like these littl dot over here to to eat And so it wa like super clear what you have to do So I tri to feed that to clip And you know you can make a clip classifi thing by just evalu a bunch of differ string with it So I like try to I try to evalu the string go left go up go right go down or like Pac Man should go left Pac Man should go up but it never work out So if you can if you could get someth like thi run thi would be amaz Like with mayb with your knowledg mayb Pac Man the right environ becaus clip wa train on whatev pictur scrape from Instagram But I think just thi thi type of you know think beyond just the string in term of languag but think in term of I have some structur environ and I want to leverag thi thi knowledg of these model is is super cool Yeah that will be a super interest applic I think us clip here like becaus it fill in anoth modal which is imag could be realli interest So I think it kind of solv on of the major limit of thi paper name just the becaus current we gener plan regardless of the environ state so it condit on environ state and potenti us clip you can encod someth there becaus you can also take imag as input to to an imag can serv can serv as state for for for the environ I think that would be realli cool And yeah so yeah So just to be to be clear to the listen that the basic idea for thi I have from from a PhD student that wa partial in our lab call John Battista for scandal So the credit fulli goe to him of of thi whole idea I want to but I just it got me think so much about you know we can extract thi knowledg into into other modal And pretti cool Is there anyth you want to mayb sai about the experi Is there anyth that wa veri surpris to you or you know someth you expect or someth you particularli want to highlight Um I actual I think we cover most thing But I think I might sai someth about the the baselin here I assum you can probabl see except for the human refer we also got to find a GPD three version And we did find that fine tune can can be a realli strong baselin here Becaus as you can probabl tell the on of the measur here LCS which is the longest common subsequ Ye thi measur here is much higher than the other Thi measur basic calcul how much overlap there is in your gener plan against those plan written by human So kind of calcul thi IOU score So we did find thi to be a strong baselin And I think it still actual make sens to be a strong baselin becaus thi is train on such data And so thi is kind of to illustr that like if you do have domain data still realli help to train your model fine tune your model thi wai But if you have someth like thi you can potenti just leverag the knowledg alreadi in thi languag model Cool Yeah so where where doe your futur lie What ar you Are you go to ar you go more into thi direct Or wa thi sort of like a on off thing Or do you have I mean what ar the interest question that you ar ask now mayb as a follow up to thi Yeah so I person I decid becaus I am in a stage where like appli to PhD program and and also other posit So like but but as a follow up I think it would be realli interest as I mention on limit major limit of thi work is that we found a clear wai to condit on the environ state So that like if you realli place an agent in in the household for exampl there is no if you want to make coffe but no coffe but no there a automat coffe machin how would you make a coffe with some mayb similar devic So the agent can realli reason if you just put it thi wai becaus it condit on the environ state So I think it would be realli interest to like investig how you can also condit on the current environ and then and then reason from there But thi might requir some train data And I think part of the reason why we like go full length here to investig thi becaus thi is someth just for us to tell peopl like thi is an interest find and we mai be abl to leverag someth here But I think thi will be realli excit and like interest futur work Cool excel Wenlong thank you veri much for be here Thi wa awesom So great to hear from you know from alwai from the peopl who made the stuff So yeah thank a lot Yeah thank you so much Yeah And in the end I think I also want to like point that like thi is a group effort and realli a lot of thank goe to three of my advisor Peter Bill Deepak Pathak and Igor Mordech Excellent All right Thank you And I hope to see you again Yeah like it would be an honor to alwai be here Yeah 
[ML News] Google introduces Pathways | OpenAI solves Math Problems | Meta goes First Person
Googl introduc pathwai their next gener AI architectur open AI solv high school math problem And Facebook goe all on first person view Welcom to ML new Befor the video start a quick thank to our sponsor weight and bias I want to show you thi on featur that I just learn about Did you know you can emb a weight and bias report in notion actual not onli report but also other stuff by weight and bias So thei have thi neat littl page here Ironic it is actual a notion and it is super easi to emb live weight and bias stuff into notion So for exampl here I have a sweep and you can see the sweep is interact So you can do all the kind of thing us to analyz the weight and bias sweep Now I can just grab that URL get over to notion and creat a new emb past the link And there we go Look at that Thi is a fulli function weight and bias report insid of notion So you have all the interact here that you would usual have as you can see so I can look at my run I can activ them I can even go and look at my sweep control and variou thing Thi is realli cool if you work togeth with other peopl and you work on more than just weight and bias report you can take your note and notion and then emb the report and the sweep whatev into notion page I love notion I love weight and bias and veri cool to go togeth If you know weight and bias it is your on stop shop for all your machin learn experiment need from try out model optim hyper paramet all the wai to save your model deploi them and so on It run in the cloud free for person user and for educ There ar plan for team and for self host setup So all the more reason to go try it out Thank again to weight and bias for sponsor thi video And now get into it Bye bye Hello and welcom to ml new dive into our first stori Jeff Dean ha releas a blog post on the Googl blog Note thi is not the Googl AI blog Thi is the main Googl blog also given a TED talk about the subject and the subject is thi model call pathwai a next gener AI architectur We actual know much about thi architectur becaus all we have is that TED talk and thi illustr right here And essenti Jeff Dean imagin futur AI project to reli on thi new architectur where instead of have singl task neural network that train you have thi giant multitask neural network that can do all the task at onc And that would also be spars activ As you can see here differ task would leverag differ path through thi network Thi goe along with a few critic on architectur So he sai for exampl AI model ar typic train to do onli on thing Pathwai will enabl us to train a singl model to do or million of thing So the goal is to have on model do mani mani task at onc Second he sai model mostli focu on on sens pathwai will enabl multipl sens Thi refer to the fact that the input to current neural network ar singl modal Sometim two modal but mostli singl modal like imag or text or sound Thi pathwai architectur natur be multitask will also be multimod which mean that it could input ani sort of modal in thi TED talk he give the exampl whether or not you see a leopard or hear the word leopard or hear someon sai the word leopard or see video of a leopard that should essenti evok the same concept in your brain and therefor also in the pathwai model And lastli he sai model ar dens and ineffici pathwai will make them spars and effici Thi refer to the fact that our current network ar dens activ connect to everyth And veri veri ineffici He imagin thi futur pathwai architectur to be spars activ mean that onli veri small sub part of the network will be activ for a given input sampl And therefor the differ part of the network do differ thing thei alwai have to be activ at the same time Thi can also make the model much more effici in term of paramet and comput Now as I said that not a paper to go along with thi or an implement or even a plan of how to get there Thi is essenti a wish list and not a particularli new wish list like peopl have dream of oh we just make multimod multitask model where on model learn everyth Well yeah everyon wish that but you still have the problem Name for exampl catastroph forget If you try to teach the model mani task and then on task more you still have to ensur that it forget the old task which is veri veri difficult especi in thi pictur it seem like thi is a rather feed forward architectur right here without ani sort of memori modul or anyth like thi So how go to achiev that I know Secondli thei sai there ar mani differ task here Howev huge data architectur mostli reli on self supervis and then fine tune for individu task and not have differ task in parallel though multitask train is a thing And lastli the spars activ ar not trivial to achiev Again peopl have been sai thi forev like well can we just have a spars neural network probabl the brain is spars blah blah blah But how ar you go to get there Thi is just a wish list how go to get there I know The main problem with sparsiti be that if you have a spars forward signal then your backward gradient ar also go to be spars You mai never learn the correct spars wai through your network If you onli activ spars in the forward pass These ar all challeng that have exist forev But it seem like Googl is determin to solv these challeng I mean if thei can all the better but for now just a plan and an idea And excit to see what happen Open ey releas a blog post call solv math word problem where thei train a languag model to solv math problem Thi goe along with a paper sai train verifi to solv math word problem by peopl at open AI you can read it if you want Essential it is a data set of about of these high school math problem where you mainli need the basic addit subtract multipl and divis in order to solv the problem usual state as littl stori and thei have some sort of an answer Now larg languag model such as GPT three ar usual kind of bad at thi type of stuff mainli becaus thei ar not accur enough Thei do these simpl step that ar requir enough more like a languag model more like a convers model or a thing that simpli repeat some of the stuff it ha alreadi seen So the first approach the paper take is to fine tune such a languag model on these task And it turn out that go too well Veri often that make a lot of mistak as well And the solut come in the form of what thei call verifi So verifi ar model that ar not train to produc the solut but thei ar train to rate whether a solut to a problem is like to be the correct solut or not So now what thei do is thei us on model that thei fine tune to produc like solut and then thei us the verifi to rank the solut and pick the best on And that turn out to be veri veri power So seen approach like thi befor you rememb the Dali model of open AI also not onli us a gener model for the avocado chair but it also us the clip model in order to rank the output of the gener model So thi could be a more gener recip for improv gener model is train verifi and then gener a bunch of solut and rank them with the verifi As I said you can read the paper and the data set of these math question is avail to download Sam Altman tweet neural network realli truli learn not a fanci trick Thi is on of the most remark thing human have ever figur out and the implic ar difficult to overst Now not sure if he just want to start like a fire with thi kind of thing There ar mani wai of go about thi but it seem like the truth or verac of the statement entir depend on how you defin learn But it seem like Sam Altman and in gener what we see out of open AI is of the opinion that learn that human do that much differ from the learn that current larg scale neural network inher do Now thi is to be set a littl bit into contrast with what peopl from the more symbolicist camp mai think about these neural network and about the natur of learn and intellig in gener But again I guess it onli depend on the definit of word here and just put the modifi realli and truli in front of a non defin word suddenli make it defin But what do you think Let me know in the comment after you hit the subscrib button See what I did there Next new busi insid write AI research sai their output is be slow by lawyer after a string of high level exit get publish realli is a nightmar right now So the articl start off with a bunch of Googl controversi Obviousli some famou peopl were fire from Googl recent and there were a bunch of scandal around that And now on senior AI research who spoke with insid on the condit of anonym come forward and sai well the lawyer ar essenti up our neck right now so difficult to publish Thi is realli stifl publish insid of Googl and so on And the articl back thi up by sai accord to onlin record the compani publish piec of AI research in and in But the compani look to have experienc a moder slowdown thi year publish just research paper in Thu far Now thi is the onli thing where thei actual back anyth up that thei sai Now no doubt that thi is the case insid of these big compani thei give exampl whenev thei write word such as bia or fair then the lawyer thei would just have like ton of question or want to cross them out becaus thei just understand the technic term behind these thing Now noteworthi term like bia and fair actual have about technic definit and all in conflict with each other So exactli fault the lawyer What I found funni is that in the last section here a spokesperson from Googl took a statement and said publish paper at the same rate we did last year At thi time last year there were approv paper and thi year there ar so far The spokesperson said ad our websit reflect all paper and is typic updat a few month after public So thei had to buri thi on the veri bottom of the articl right here becaus thei want to like tell a stori about how lawyer ar so terribl and about how thi exit stifl Googl so much and get me wrong lawyer ar terribl And pretti sure that a pain in the neck But the claim that thi is especi ramp up now seem to hold apart from like on or two anonym peopl insid of Googl come forward And the fact that thei have to hide thi thing at the veri bottom which is pretti clear like a much more like explan than Googl now suddenli ramp up their eyebal of the lawyer like lawyer have alwai been like thi So insid call crap on you DeepMind releas their reinforc learn lectur seri Thi is a lectur seri about introduct to reinforc learn by DeepMind research at the Univers Colleg London and you can in fact watch all of them freeli avail on YouTub The slide ar avail and pretti cool if you want to get into reinforc learn It start out with the simpl framework and it end with deep reinforc learn David Ha tweet the follow out a pop up shop in Shibuya will sell cloth with adversari patch print on them to make a fashion statement Now while I understand thi exactli I do think pretti cool So the label or the brand or the store is call camouflag against the machin unlabel and the cloth featur adversari patch Now whether that will help in ani wai or form like quit doubt but it is a pretti cool insid joke if you meet other research The next on realli machin learn new but it is quit import A contributor to PyTorch ha releas a viabl solut for Python concurr So if you know CPython the refer implement for the Python languag ha thi problem that in a multi thread applic in order to keep track of all the object fly around it essenti is forc to do thi refer count And in order to do proper refer count it essenti mean that everi time a refer is increment or decrement it ha to lock down all the thread Thi is known as the GIL the global interpret lock And it is the reason why you can program multi thread applic in Python but thei will never be abl to us the interpret at the same time which mean that if you have CPU bound applic multi thread will just not help it will not speed up your applic at all you need to go to multi process So the rule for the longest time ha been if your applic is IO bound then you can us multi thread becaus easier to program easier to reason about you have share state and so on Howev if your applic is CPU bound then you have to go to multi process which is quit a bit more heavi more error prone so on Mani attempt have been made previous to remov the GIL but everi singl actual implement of a Python without a GIL had the advantag of be abl to run multi thread applic realli concurr but also the disadvantag that singl thread applic which most Python program ar singl thread applic would slow down due to these chang But now thi new suggest by Sam Gross who as I alreadi said is a major contributor to PyTorch is actual a viabl solut and is be evalu current which is pretti cool And it mai be that in the futur Python concurr program will get a lot easier Big scienc ha releas t zero plu plu which is a model that is a multitask train text to text model even exactli know how I should call thi But essenti thei took t five and thei train it with a bunch of differ NLP task that you all frame as a realli a text input So if you know what t five is t five is thi concept that when I have an NLP task rather than encod it somehow in a smart wai I simpli encod it as a natur languag prompt For exampl if I want to translat from French to English I simpli sai pleas translat the follow from French to English and then I put the French sentenc and then I train the model to autoregress predict the English sentenc Thi mean I can us pre train languag model as a start for these model And name that is what GPT three doe zero shot out of the box So the idea here is that if GPT three can do thi in a zero shot fashion these natur languag task that ar formul in the languag of sai of the input of English we achiev the same or better zero shot perform if we retrain the model on languag model as GPT three is but if we instead pre train the model on other task So T zero is thi model that take a bunch of differ NLP task put them all into the languag as a human would input them or type them up So compat with a languag model train all of them at the same time And it turn out that the result model can actual do new NLP task in a zero shot fashion much like GPT three but is wai more paramet effici at that So thi is pretti cool And the model is avail on hug face A bunch of exampl of what that can look like thei have differ version of thi model you can import it in the hug face API you can even try it out here on the websit And the thing I want to highlight is that big scienc some research lab or a compani actual a on year long research workshop on larg multilingu model and data set Thi is simpli a conglomer of a bunch of research from all over the world that is loos organ togeth for on year to investig these larg model So pretti cool that someth outsid of tradit academia or corpor research lab also come into the game and provid lot of cool stuff for the commun Definit check it out Check out their paper check out their model Speak of the hug face hub hug face releas thi tweet sai that the data set viewer is avail in hug face hub is essenti a preview where you can for ani data set go and see what kind of sampl ar in there not for ani data set but for ani that support the hug face stream API which ar like half the data set on the hug face hub Thi work for imag So here you can see MNIST and you alreadi saw some NLP thing So pretti cool Hug face hub is get more and more us by the dai Sight is a sort of a Googl scholar ish type of thing where you can look for public and then insid the public everi citat will be annot first of all with the context of where it goe So ani citat target if you click on it see sort of the context of the citat And second of all it is annot with the fact of whether the citat actual support the cite research or is critic of it or refut it So you have posit and neg citat And thi give you a much more accur pictur of how a particular paper ha fare in the research landscap in how it wa cite and not onli whether it wa cite Thi is done in part by an autom system And I believ thei alreadi have a giant amount of research articl in there and autom these extract of refer and thei ar score them us deep learn model What els there is a paper to go along with it Check it out if you like and give site a try It exactli free There ar differ tier right here with differ featur But if thi is at all help to you I guess it might be worth it Facebook AI releas a blog post call teach AI to perceiv the world through your ey Thi is a push by Facebook or Meta or whatev it is call right now to go awai from the standard data set where you have some third person view of a scene to realli first person data set So thei have a bunch of collect of data from around the world from differ peopl in differ life circumst in mani mani place And thei collect first person data mean I guess these peopl had head mount camera and had other sensor on and thei record just do everydai activ So the data set is call ego And what I think is cool about it is the data set gener process is differ from what other data set ar not onli the fact that it is first person and that it is you know distribut all over the world and not just done by a singl person or team but also becaus thei just told the peopl you know just record yourself do everydai stuff And then after the fact thei went ahead and thei defin task and thei annot the data for label So thei have the label in mind when thei collect the data or mayb thei had them in mind but thei collect the data specif to get some label first collect the data and then thei put differ label over top So for exampl differ task that thei imagin ar memori task forecast task object recognit whatnot thei have variou layer of label annot by human by crowd worker on thi data and the data set You know you can imagin that these the onli label In fact it is veri feasibl that our differ research group goe ahead and annot the data in a differ wai to creat their own task The blog post highlight the difficulti of ego centric data which is usual vastli differ from like a third person view As you can see here on the left thi object detector work quit well in third person view Howev in a first person view it just kind of fail So is thi a good wai forward to build more capabl system or a step into dystopia I guess up to you But if you like work with data like thi then give the state set a try not exactli sure how you can get a hold of it I think there is some sort of licens attach But yeah out there Tesla releas appar pretti randomli a guid to a configur float point format and arithmet So thi is a veri technic specif for eight bit and bit float point number and arithmet and is suppos to sort of standard or give a format to configur float point number So as I said veri technic actual also quit short And the gist here is that thei sai if you train AI model on realli larg scale like Tesla doe you might want to go down from bit number to bit number or even eight bit number Howev in these veri low regim you onli have whatev eight bit to plai with And therefor you exactli specifi ahead of time how mani bit should be the expon and how mani bit the mantissa should be Therefor thi need to be configur So not like in a bit number you have exactli thi mani bit for thi and thi mani bit for that in these new configur float point number thi would be a variabl that you can decid as you us the number So that allow you to trade off what kind of rang thi number can potenti have with the accuraci the resolut that the number can have in a particular rang see whether thi remain a thing pure us insid of Tesla or whether other peopl ar go to adopt it Microsoft introduc Pytorch direct ml thei sai train your machin learn model on ani GPU So thi is a compon for Pytorch that allow you to us ani DirectX GPU for do deep learn And all that is necessari essenti is that in Pytorch you sai to CUDA like if you have a CUDA devic now you sai to DML to direct ml And it Thi work on Window and on the Window subsystem for Linux So if still a Window user for whatev reason good for you All right more help thing that I saw thi week There ar a lot of help thing thi week not onli help librari the section is renam to just help like help me pleas PyWig is a high level batteri includ neural network train librari for Pytorch And ye whatev think is said here at the begin of the readm Doe the world need anoth Pytorch framework Probabl not We had thi project when no good framework were avail and it just kept grow So here we ar Yeah respect Cool If none of the current framework pleas you PyWig might be for you Lexa is a benchmark for zero shot reach of goal Thi goe along with a paper by CMU UPenn and U of T about reach goal after discov the world So these agent what do is essenti go ahead and just try out a bunch of stuff in the world without ani explicit goal And after that you give the model a pictur of a goal to reach and suppos to reach it So thi mean you explicitli train the agent to reach that particular goal or ani goal you simpli let them explor And after that thei have to reach a goal So Lexa is a benchmark that achiev thi And as I said thi goe along with the paper that give a veri veri veri good baselin for thi benchmark alreadi But the benchmark itself is avail to download if interest in do that kind of research give it a try Next Donni Jarr Huffner tweet out excit to introduc crafter So thi is a game sort of an open world game long term reason explor gener made for reward agent and unsupervis agent call crafter and you move around and block and food and you have to dig and you have to build and you have to craft thing never seen anyth like thi befor Thi is a first Thi ha no relat to ani game that seen so far No pretti cool So you can craft thing as you can see right here you can interact with stuff everi world is randomli gener Like thi is a Minecraft clone but amen to machin learn research to AI research So that is pretti cool becaus Minecraft just seem too complex becaus you can move like in ani direct and so on here realli discreet So these model thei have a much more easi time to go about it alreadi evalu differ of these AI learn mechan on it like dreamer PPO rainbow agent and so on And none of them realli compar so far to human expert But I think the game is pretti cool It is avail These RL agent can alreadi do thing like you know dig hole build bridg and so on a veri complex behavior alreadi emerg here it move out of the wai of a skeleton and then anoth on build a shelter Excellent crafter give it a try If thi video get more than three like do a crafter plai for sure Robert Lang releas a lightweight hyper paramet optim tool Thi seem to be a cool kind of person project by Robert and he releas it with pretti good document Colab there is an exampl And if just look for like a veri simpl wai to do hyper paramet optim then thi might be the librari for you As you can see differ strategi for do hyper paramet optim and differ wai you can defin them pretti much all you need even ha the fanci decor style as you can see right here Veri python Sayak Paul releas a Kera tutori on mobil vid so thi is a tutori that will guid you through implement mobil visual transform in Kera which is quit neat So Kera still as easi to us as ever And thi tutori guid you through build the architectur from the ground up all the wai to train it at the end you convert thi model to TF Lite so it actual run on your mobil phone Pretti cool John Seviero tweet out thi demo is surpris combin vit with GPT to caption imag with great result And ye actual I wa posit surpris Thi is a hug face modul where you take a exist text model like GPT two and you take an exist imag comput vision model like vision transform and you combin them So first you start out with sort of random cross attent weight that you fine tune just a littl bit and that can have realli realli good result Essential the model learn how to connect the latent represent from on model to the other model and back So thi is us right here to do an imag caption demo us GPT two and vit as I said and train onli about step on the cocoa data set So you can see the result Thi is a man swing a tenni racket on a tenni court that is veri descript But that is just an unhumanli precis descript of go on right here We have a blue and white street sign sit on top of a pole Ye that that is also a veri veri veri precis descript Person ride a skateboard on top of a cement floor Well I guess that ha some import Is it just me or AI model just bureaucrat But yeah pretti cool Bit and byte is a librari by Facebook research for eight bit optim and quantiz routin So thei have a bunch of optim such as Adam Adam w RMS prop and so on that work on eight bit instead of And that pretti reliabl save you of the memori Someth like Adam ha two or three differ buffer that for everi paramet you need to keep track of So thi can pretti quickli get pretti larg and save three quarter of the memori ha definit valu Now I love that call Facebook research But if you hover it sai meta research Is thi gonna go well I know Also is thi suppos to be like a pretzel Like it is is it suppos to be like a flat logo Or is it suppos to repres sort of like a Pringl chip you know like the saddl in I know Another help thing user friendli introduct to PAC base bounc by Pierr Aucur Now thi is someth I have no clue about But I know import And I have learn it at some point if try to get into PAC base bounc thi is a I believ over page introduct to it that seem to be quit well written introduc you to all the import concept in it So if interest give it a try Even face met whatev research releas Xformer hackabl and optim transform build block support a compos construct So if into transform and if you would like to recombin them try out differ thing insid of them Xformer might be a great librari on do that So you see all of these box here essenti thi librari make it pretti easi to just rearrang them connect them differ and so on Xformerb is a speech process univers perform benchmark Thi mean that thi benchmark ha a bunch of speech task so task in machin learn where the input is a piec of speech But the goal here is that you have on pipelin that gener a represent And then that represent can be fine tune easili to all of these task So not suppos to solv all of the task from scratch suppos to come up with that pipelin that gener the represent If you work on speech thi might be veri cool for you I know how to sai thi CCQA is a web scale question answer data set for model pre train Thi is a larg scale QA data set that I guess you can us for pre train question answer model Excellent Bagua is a librari that claim to speed up PyTorch So thei have a bunch of thing in here for PyTorch For exampl advanc distribut train algorithm perform auto tune gener fuse optim load balanc data loader and so on So these seem to be special algorithm that in veri certain case where you want to us PyTorch can potenti deliv a lot of speed up So if your problem fall into like the standard bucket where the librari is optim for mayb you can find someth insid of Bagua that is go to help you Bagua Bagua I know Tree extract is a PyTre modul system for deep learn in JAX So if you work with PyTre thi is it in JAX Good job PyTre for those of you know ar essenti tree out of Python structur So here for exampl a list which contain number and dict which themselv contain tupl and so on So a PyTre work with these kind of object and now you can us them insid of JAX and Tree X help you to do that in a more modul orient wai or a more object orient wai Reuter write AI can see through you CEO languag under machin microscop Thi articl essenti sai that thing like NLP and speech sound analysi thei now go after CEO quarterli announc thei analyz their voic and try to just recogn when nervou and so on And thei actual have a point in that thei claim thei can make better invest decis if thei do someth like thi But as you know as soon as you pai attent to anyth like thi the CEO ar immedi go to adjust and train to trick essenti these AI system So thei will us script speech much more in order to not trip the NLP system thei will train their voic act more I guess or let some press secretari speak for them All in all it just seem to be like you know if you analyz a speech and to to detect when ly and when not and then make invest decis simpli reinforc the like the sociopath that have no problem with just straight out ly that have no differ in their voic whatsoev So if you want to creat a world of more sociopath CEO than it alreadi is I guess then go right ahead Just just do thi Thi Thi is fine Excellent Cadburi the compani ha appar made thi ad for Indian local busi And not just an ad but paid thi Indian celebr to record essenti on ad and then thei modifi that ad us deep learn So thei have like three product categori like shoe and I guess glass and watch or someth like thi record the differ ad for the differ product And whenev the actor sai the compani name and the locat of the compani thei us deep learn to chang whatev the small busi is So essenti thi is a deep fake from the same actor to hi own face but to make him sai someth els So as a small busi in India you can go there and get your ad for your local busi the system will actual make sure that peopl that ar in your area ar advertis with your particular busi and peopl in differ area will see I guess the same ad but the actor mention a differ busi that is in that area Pretti cool a form if in India you know check it out And lastli thi shoe doe not exist Thi is a websit I guess analog to thi person doe not exist which wa a famou websit that train style gun to on a face data set So thi is style gun three which wa recent releas the alia free gun and train on a shoe data set So you can just refresh and look at shoe that the model ha come up with I guess these shoe all look like thei exist Thei might as well who know But yeah if look for uniqu design idea check it out look forward to mani more thing where style gun three is appli It seem to be the qualiti of these model and the eas of train them ha come a long wai such that it is in fact possibl to do thi for mani type of thing where you have decent amount of data such as shoe I guess All right thi wa it for thi ML new Thank you so much for be here forget to like and subscrib and let me know what you think in the comment I valu your opinion Definit Thi is not just a trick to get the YouTub algorithm to promot the video more and all of that kind of stuff 
Implicit MLE: Backpropagating Through Discrete Exponential Family Distributions (Paper Explained)
Hello there Todai look at Implicit MLE Backpropag Through Discret Exponenti Famili Distribut by Matthia Niepert Pascal Minervini and Luca Franceschi Thi paper is a paper that discuss in our regular paper discuss on Discord and so it is inform by everyth that I have heard there If you want to take part in these discuss and influenc my opinion veri welcom to do so The link to the Discord is in the video descript Alright get into thi paper right now Thi paper propos essenti a discret layer for neural network Thi is mayb how I can describ it And the basic setup is in thi figur right here So sai you have an input X which might be some sort of a continu input like an imag Thei do give an exampl By the wai the author thei have quit help code avail but also thei have made themselv a littl video about the paper And I also recommend that you go watch that video becaus quit help So what thei give as an exampl in the video which I find a good exampl is you have a map of and I think thei us even Warcraft map but you have a map and like a lake somewher and then like a littl hous right here and so on Your task is to go from the top left here to the bottom right So you need to plan your wai somehow through that Now you get thi as a graph that would be directli input into algorithm Howev you get thi as an actual imag right Yet the solut here is go to be some sort of a path some sort of a gold path the label Or mayb someth even deriv from the gold path like how long the gold path is So mayb five long or someth like thi So veri complic You first need to recogn where can I even go base on the imag on the left Then you need to find the shortest path base on determin where to go Then you need to evalu base on that shortest path you need to evalu some properti For exampl as I said how long is the shortest path or just you know follow the shortest path on the actual map So a mix of continu and discret element and specif the part in the middl describ by thi P of Z right here That is go to be some sort of a discret solver In the case here go to be a shortest path algorithm Now the question is how can we run backpropag if we onli have the label on the right hand side How can we backpropag I mean we can backpropag from the label through here right Thi is a neural network that mayb determin some properti of the shortest path But then how ar we go to backpropag through thi layer right here back to thi neural network suppos to extract the input graph to the Dijkstra algorithm from the imag And that is a challeng There have been some solut alreadi for exampl on famou exampl is a score match Sorri that is also an exampl But the famou exampl is the straight through estim Howev it alwai work It fail sometim And specif here the author propos a differ framework in thi implicit MLE framework go to look at how built up Thi is a veri technic paper And by no mean an expert in these thing I just try to give you a littl bit of the idea of happen right here so that you know go on And if you have someth like thi in your neural network like a combinatori optim solver or anyth like thi then you can just go grab their code and us that as a layer It is realli super simpl All right that wa the overview Now get into the paper Hold on thi video is sponsor by Weight and Bias Weight and Bias is your on stop shop for all your machin learn need It will track your experi with a singl line of code It will upload automat all your log all your configur everyth to your cloud It will automat grab all the output all the metric all the configur of your experi and store that in on neat locat So you can see your experi you can track them wherev thei run You can compar among the experi but you can go further you can then tune your hyperparamet accord to the result of those experi And all of thi is done automat in a distribut wai You can liter sit on your toilet on your smartphon and tune your hyperparamet and start new experi But not onli experi track and hyperparamet tune Weight and Bias ha tool for the entir pipelin of machin learn research from the initi idea up until the deploy and beyond that when you actual want to track what deploi Weight and Bias ha cool method to track all of your data set and their depend to each other as well as your model and all kind of other artifact that you might produc veri power visual for all the input and output of your pipelin as well as the model themselv All of thi run in the cloud But if concern about privaci there ar option to self host The system is free for person us and for academ and thei have great plan for enterpris small team larg team matter So thank you veri much Weight and Bias for sponsor thi video If you know them yet absolut check them out free make your life a whole lot easier Now get into the video As I said the problem right here is that you have these kind of discret task sometim as a part of an entir learn setup So the paper make differ contribut but here list out Thei sai we propos the implicit maximum likelihood estim as a framework for comput gradient with respect to the paramet of discret exponenti famili distribut So what we want is of cours gradient gradient of thi discret process in the middl And the discret process specif is go to be formul as a exponenti famili distribut And go to see how that happen Thei sai we show that thi framework is us for back propag gradient through both discret probabl distribut and discret optim problem And that would be the exampl right here would be a Dijkstra shortest path algorithm or an integ linear program solver or anyth like thi In fact on of the gener formul thei have is for integ linear program solv IMLE requir two ingredi a famili of target distribut Q and a method to sampl from complex discret distribut We propos two famili of target distribut and a famili of nois distribut for Gumbel max base sampl So go to look into how that work and exactli what it contribut And then yeah we show that thi simplifi to explicit maximum likelihood learn when us in some studi set and experiment evalu These point probabl not go to go into too much Essential in point four thei show that for some set thi reflect alreadi establish method in sort of a gener of method that have alreadi been around of method that ar mayb specif to a given set or problem And the experiment result well you just like their experiment result essenti show that their method for exampl outcompet the straight through estim method So the deal with discret thing in neural network The problem is of cours that we comput gradient with respect to discret thing Now take for exampl the straight through estim the problem try to solv or on of the problem You can formul it like thi You have some X you put it into neural network and out in the middl somewher You ar requir for some reason to sampl from some sort of distribut For exampl requir to thi produc a probabl distribut over a few class sai over four class And then what go to do is go to sampl on of the class right here And then go to continu with that through the rest of your neural network until at the label Now again as befor you need to back propag in order to learn through thi network which is easi But through the choic through the sampl procedur of that of that inner layer And hard So what the straight through estim doe is a bit of a trick It essenti in the forward pass you do the discret optim you do the sampl But in the backward pass you act as if you simpli propag the distribut as such So to the forward pass it is realli a discret sampl But to the backward pass it look like simpli you did you never sampl You simpli pass the whole distribut and sai well not sure like percent thi and percent thi The wai you would implement that usual is you have some signal call that H for mayb the histogram right here And what you would do is you would if you sampl from H that wa go to give you like S Well sai sai we take the most like state Right So we determin H and we take the most like state which which is sai S is the arg max of H That is your sampl Now what you would do in your forward pass is you comput the next layer H prime as S which and then plu H minu a stop gradient of H So the stop gradient Am I do thi correct No of cours not Of cours not Ye Oh ye do thi correctli Of cours OK So analyz thi in the forward pass The stop gradient ha no effect on the forward signal So these two here essenti cancel out these cancel out to zero Howev in the backward pass right sinc deriv is distribut over addit and subtract what you would do if you were to deriv the gradient of H prime essenti the gradient of S plu the gradient of H plu the gradient of stop gradient of H Now stop sorri minu minu stop gradient of H obvious ha no gradient So that goe to zero The gradient of S is also zero becaus a discret oper And most of these framework simpli tell you well the gradient is zero a discret oper If not sure that thi is happen you mai in fact also put a stop gradient oper around S And you can see what remain is the gradient of H So you see the trick in the forward pass These two cancel out Howev sinc in the backward pass thi by itself is alreadi zero becaus of the stop gradient oper the gradient of H remain right here Thi is a trick You can you can simpli swap out a gradient in the backward pass for whatev you like with thi trick Peopl have us thi to get gradient with respect to discret oper like thi But thi paper right here is an altern And as thei show in some situat it is more appropri to us that altern Howev it is also quit a bit more tricki So the first thing go to do The first thing go to do is go to take that inner thing right here that inner procedur And again go back to the task of find the shortest path So the input The input is some sort of a graph right Where you need to find the shortest path with cost associ with each of the edg and some start and some end goal And what we want is the shortest path Someth like thi Now the first thing go to do is go to encod thi problem into a binari vector Now how exactli we do thi is I realli know for shortest path problem but go to encod thi into essenti well not a binari vector but go to encod the problem into thi vector theta right here So theta in thi case what you would do is your theta vector thi is the theta vector it will have I guess it will have probabl for each edg it will have an entri with the neg cost of that edg associ in the vector So the neg cost of edg on the neg cost of edg two the neg cost of edg three Now why ar we do thi You can see that we ar go to multipli thi theta with anoth vector call Z And Z here is the call it the solut or the propos solut to thi inner problem And Z is now a binari vector So Z can either be on or zero in each entri And go to be on if and onli if thi edg here is part of the propos solut So ani path in thi graph can be repres by a given Z variabl right By simpli set a bunch of thing to on and zero I can select some of the edg And if select the correct on thei will form a path And if I have select the absolut correct on thei will in fact form the shortest path You can immedi see that for the shortest path the inner product between the two vector will be the highest among all the path So thi is how I formul my problem formul my problem between as an inner product between a binari vector and some sort of a weight vector theta such that for the solut of the inner problem like the shortest path algorithm or the k subset select or the integ linear program such that for the solut of thi problem it is the case that thi inner product is the highest possibl Now you immedi see that of cours I can make that inner product even higher by put all of the edg to zero right So you know Z right here I can simpli sai zero zero zero zero zero All the cost here ar neg Ergo I have no neg cost Ergo that is go to be zero And that is go to be the largest possibl solv the problem the problem Thi a path in the origin formul So the last ingredi miss right here is what thei sometim here call capit C Thi thing right here capit C is a constraint set So capit C would defin in thi case what the valid entri for the Z vector ar So Z must be in thi capit C class And I think C must be in the ye That defin what the valid solut even look like So in the simplest case if thi is a classif problem right Thi is a classif problem Theta would sort of Yeah you can think of thi as a classif problem and then Z would be select the class right You can model theta in thi case as just a vector of on And then Z right here could select the class by simpli put that entri to on Wherev of whatev class is select And the constraint set C could be easili model by sai the norm What is that The sum of all the entri which is probabl the on norm of Z must be equal to on That could be the constraint set Am I correct here not sure I can actual model I probabl model it like thi Like here there probabl need to be like there probabl need to be some sort of cost per class or someth like here And then I can model the constraint as sai the inner product of Z with a vector of on Must be equal to on That look better So that is actual part of the definit of the constraint set And the problem in these case is that thi constraint set make it veri difficult on obtain good gradient through thi discret problem Becaus right here as you can see not realli easi becaus most of the Z vector in the Dijkstra problem actual valid path So the issu here is that we need a gradient We need to respect the constraint set of the problem Thei go ahead and thei formul thi as I said as thi problem where you have thi vector Z is whatev solut you propos The theta is the definit of the problem The inner product is sort of the reward sai the reward mayb the invers loss of the problem And thei can now formul thi as a exponenti famili distribut by simpli rais thi by put thi insid of an exponenti function see if done it somewher right here Look at that Oh not even a minu sign All right So for now just trust them that it is necessari to formul it as a distribut and just kind of hang in there It is go to get veri complic but it is go to lead somewher So thei can formul thi inner process as a probabl distribut P of Z that is accord to the exponenti famili So as I said the exponenti famili here you put in thi thing right here There is a temperatur at which you sampl So what is that essenti is go to do is go to normal given thi right here Thi is the log partit function the normal constant Thi is essenti go to give you a distribut over the individu dimens of the Z vector And that is go to be normal and is go to be more peaki or less peaki depend on the temperatur right here So the process that thei formul thi as is you take some input X right here You put it through the first neural network to obtain the theta The theta is essenti the problem definit for the inner algorithm The inner algorithm you formul as a probabl distribut So go to have more or less like state with the more like state be the on that solv the inner optim problem More perfectli to more reward So Z is go to be a random variabl that is accord to that distribut For now you can just think of Z as a random variabl And the like state of Z ar the on that have the path that have a veri short path through the in our exampl Or whatev state solv the inner problem veri accur And then from that Z go to put that through anoth neural network go to give us our output And go to compar the output with the gold label And then go to back propag through all of it Our paramet ar the paramet here and here So the paramet of the two neural network F U right here Thi is easi to do right becaus we can simpli back propag from Y into the neural network And the paramet of H V the V paramet Thi is hard Thi is the hard part So what do we need to do in order to back propag all the wai to H sorri to the V variabl Well what we need to do is we need to the direct here is that the paramet sorri X becom theta becom Z becom Y Thi is with the help of the paramet V and thi is the help of the paramet U U is easi For V what we need to do if we want to have the what you can see right here the gradient with respect to V We first need the gradient with respect to theta and then we can onc we have the gradient with respect to theta Where is it Where is it I guess here Once we have the paramet with respect to theta we can us the back propag algorithm again to back propag into thi network and chang the weight V So how do we get the gradient with respect to theta Again thi is mean we have to back propag through thi piec right here which is the inner optim algorithm So here is it appear the chain rule expand Thi is thi here theta So we need the paramet the gradient with respect to theta and then we can us back prop OK Thi by the wai is the entir algorithm as go to be later You can see fairli simpl You can also see there is a lot mistak right here but I think my convers So what thei do is thei sai thi veri hard veri veri hard to comput thi gradient with respect to thi inner optim procedur right veri hard to comput a gradient with respect to the Dijkstra shortest path algorithm Essential have to know how do I need to chang my graph definit in order for the path to becom shorter or differ in some wai And veri hard Like all you can do realli is kind of try and see what happen I know anywher anyhow els becaus yeah rememb that what the theta is The theta is the output of the first neural network So the theta is the definit of the graph and that is produc by by thi neural network right here that look at the pictur and give you the discret graph So essenti what it give you is an adjac and an adjac matrix But still so the question is you know how doe my adjac matrix need to chang for the Dijkstra algorithm to find a shorter path sai a shorter path or well or a path that is more close to the gold label that I have becaus you alwai want to shorter you actual want to learn from data So the first step thei do in thi challeng in thi sub challeng right here is thei sai thi is too hard go to replac the loss right here thi loss the true loss of our output compar to the label with a surrog loss Thi L is an implicitli defin maximum likelihood object and go to calcul it gradient instead of the gradient of our true loss Now the logic of how we get there is the follow In thi inner problem we defin a probabl distribut thi probabl distribut Rememb what is thi P here P describ the solut space of in our case the Dijkstra algorithm So P is a distribut that would assign high valu to or high likelihood to path that ar veri short in the graph defin by theta and low valu to pass that ar veri long in thi same graph Now what we can sai is can we thi is essenti a distribut can we find a differ distribut we call a target distribut where we can show that in expect the loss the loss from thi target distribut right here is alwai smaller than the loss from the true distribut So essenti can we find a distribut that where the path that it output ar lower in loss lower in the final loss than the on we have So rememb we have X and all of that and the end there is Y right We predict Y and we compar the Y to the true Y go to be some loss and the question is can we reduc that loss right here So we necessarili want to find theta such that we find a shorter path But we want to find a more appropri theta in here such that the rest of the neural network can predict Y hat more accur in order to be closer to Y For in the in our exampl we want to if if our neural network right here is veri bad at actual extract a proper walkabl graph from the landscap right here Like if it recogn that thi is a lake you know it think you had all of thi is realli fine to walk on and so on the graph right here will be quit crappi the weight on the edg will be not accur right not infer correctli from the landscap That mean that thi network here will have a pretti hard time determin the actual valu of the shortest path becaus even though the Dijkstra algorithm doe a good job of find the shortest path on the wrong graph and therefor useless So what we need to be abl to do is we need to be abl to more accur extract the graph from the imag So we need to train these paramet right here So here we ask ourselv can we come up thi distribut P here the distribut of solut to the problem defin by theta We ask ourselv can we come up with a distribut that ha a lower loss than the distribut we have And the answer is go to be ye we can do so with a simpl a simpl sai trick So if you look at back at thi I realiz in like three layer deep of problem like we have a problem for that we have anoth problem to solv for that we have anoth problem to solv Our current problem is that we want to see can can we chang thi distribut such that the loss is lower How do we need to chang thi distribut essenti And the answer is go to be go to take the output right here and go to pass it through thi network go to look at the loss and go to back propag that loss until the point where thi algorithm stop And then go to take on gradient step into the direct right here and then that is go to be our new distribut So what doe that mean in our exampl right here go to take the graph that we output right here go to run it through Dijkstra give us the shortest path Rememb thi is a crappi graph becaus our network initi is not good go to put that through thi neural network right here that determin the cost and go to calcul the loss and back propag that So what doe that give us ultim that tell us well the gradient sai what how do I need to chang the output right here in order for the neural network that follow to do a better job And sai the output is well thi edg here ha a bad weight or in fact thi edg an edg right here miss or or someth like thi No sorri no that is formul wrongli What we ar go to chang is go to chang obvious the Z which is the solut so go to sai in thi shortest path that you comput someth wrong For exampl you should have mayb taken a differ shortest path or you should have weigh it differ or someth like thi And go to take a step into that direct So for exampl if the shortest path rather than up and over should have gone directli we know that the edg right here should have had mayb a lower cost associ with it or someth like thi So go to us gradient descent to see how do we need to chang the inner problem such that the rest of the pipelin doe a better job And what you see what you see right here somewher There OK So thi is the target distribut is thi right here So the same as the regular distribut of inner solut howev instead of input the graph as it is go to input the graph minu a step size time the gradient of the loss with respect to the output of the inner of with respect to the output of the inner solver So thi is us gradient descent in order to come up with a better problem definit right here Sinc these two ar vector multipli togeth we can us in fact the gradient with respect to Z and subtract that from theta becaus of the same dimens So go to ask ourselv what would be what would be a more appropri problem definit in order for the rest of the network to do a better job And go to be our so-cal target distribut And now our job now we have a pretti simpl job Our job is go to be well can we make it such that the current the current graph that we output right here is more like thi target graph So can we make the distribut P more like the distribut Q is the same as ask can we make the current graph that wa output by the network H more like the graph that would be more optim for the rest of the network And that is sai a solvabl problem In fact if you work it out the formula get pretti simpl So if we do it like thi and by the wai thi inequ here is crucial obvious becaus and but we see why given becaus of gradient descent in expect guarante that the Q distribut is go to have a lower loss than the P distribut becaus we do on step of gradient descent with respect to the loss Right So essenti we do a step of gradient descent in the insid and then our surrog loss is go to be well can we make the output distribut more like the result of that gradient descent Thi thi must be on of the most confus video ever but I hope still with us So what we want is to make these two distribut closer Rememb we said we back propag through the discret optim procedur So what do we do We said instead of back instead of back propag through the inner optim procedur go to replac that by a new object The new object ha two step Step on determin what would be what would be a better output for for the discret sorri what would be a better input for the discret solver And then step two is can we make the input that receiv more like the input to the discret solver Thi is where thi where we do the gradient descent insid And how ar we go to make distribut more like each other thi right here Thi is the KL diverg between P the actual distribut and Q the target distribut And go to be our surrog loss that we us instead of the loss that we cannot differenti If you if these ar both exponenti distribut exponenti famili distribut see that thi pretti easili cancel all cancel out and reduc And in the end the gradient of thi surrog loss simpli go to be the differ between the two margin So between the two mean of the distribut Now thi seem pretti easi but insid of the three layer of problem we get anoth problem So what doe thi mean Thi is the mean of the exponenti famili distribut when given a certain definit problem definit theta prime or theta if over over here Thi given that a sai a hard problem with these constraint set and so on Calcul the mean of such a distribut is hard in fact probabl as hard as as solv the the entir problem itself So calcul the mean of these distribut is not an easi task Sampl from these distribut straightforwardli is also not an easi task So what thi paper doe is it sai for under certain condit what we can do is we can replac the mean with thi And thi is a trick while a trick a method that thei call perturb and map by map thei mean maximum a posteriori So essenti mean that for the exponenti distribut what we can do is we can approxim the mean us map the most like state And the most like state For exampl in thi Dijkstra algorithm the most like state is in fact the shortest path by how we how we defin the problem Right So defin the problem as the inner product between the problem definit and the propos solut Now the most like propos solut if likelihood is given by the inner product Obviousli the on that maxim the inner product which is the on that by construct ha the shortest path OK So fairli convolut but thi is someth we can actual do So we cannot calcul the mean of these distribut but we can calcul the most like state And not so straightforward In fact it is a better estim So thei consid I think ye So comput the margin is in gener a that sharp P sharp hard problem scale poorli with dimension So map state ar often us to directli approxim the the mean Howev appar better if you us thi perturb and map thi strategi where you estim the mean not directli as the most like state but as an expect sampl from a nois distribut and perturb the state What doe that mean That mean that you can get the mean of the distribut again draw our Dijkstra graph right here You can get the mean of thi distribut by well by slightli perturb the problem So mayb slightli reweigh the edg sai thi edg is higher thi edg is now lower slightli perturb a lot of time And then everi time you calcul the shortest path So most of the time like thi will be the shortest path mode for most of thi But then everi now and then you perturb it so hard that you know thi edg now goe up veri high in cost So then have thi as the shortest path right here and so on But ultim yeah So ad all of that up get the expect over all the shortest path in all for a lot of perturb will give you a good approxim of the mean of that distribut The last question is a littl bit OK What nois distribut is appropri for thi And the answer is go to be the answer is go to be that is go to be a Gumbel nois And I think thi is thi now get a littl bit too deep But just to mention thi right here if in fact there ar some properti ar given and the specif properti that need to be given for thi to be accur is that you can defin the problem alwai such that such that The constraint set is given by a number K where you can see right here exactli K entri in Z have to be on If obvious not cover all of the problem consid but it cover a lot of the problem consid And even if not you can still appli it as I sai as thei sai just not as appropri but still appropri enough And thei also have a wai to sampl Gumbel distribut random variabl But I think necessarili we need to go into that You just need to know that the appropri nois distribut in fact to get a good estim of the mean is a Gumbel nois Gumbel distribut by the wai It describ extrem valu So if you want to know the distribut of of the maxima of some phenomenon that will be Gumbel distribut And then you have it at the end of the dai you would the surrog gradient would be given by the differ between perturb maximum sorri the maximum a posteriori solut of perturb right here And yeah so thi is a few layer deep actual look at the entir algorithm And see not that hard So what do we do in the forward pass We take X And as I said we get theta Thi is a neural network In our case it take a pictur and it extract the adjac matrix which is theta So it extract the graph that now go to run Dykstra on So thi data goe into thi forward pass right here What do we do In fact we forward propag the maximum a posteriori state of a perturb version of theta And thi here if you rememb thi here is go to give us the mean a wrong mu is go to give us the mean of that distribut that look for So go to be forward propag in so that is go to be forward propag to sai to the second neural network and go to give us Y or at least an estim of Y And then go to compar to the real Y go to get the loss and now back propag right So back propag we take the loss We go back We go back through thi first neural network until here and that is where thi start So the backward pass that would come in here Right Thi gradient here the gradient we get from the chain rule in the backward pass We also need thi step size lambda right here OK So what ar we go to do go to take that gradient and rather than give it straight to like the straight through estim or to the chain rule go to comput an updat to the theta to our graph definit right to our adjac matrix or our our cost cost matrix for the shortest path algorithm essenti sai how do I need to chang the problem definit for the Dijkstra algorithm in order to in order for the upstream sorri for the downstream modul to do a better job predict the correct label Why so go to comput an updat theta Then go to comput a thi surrog loss right here and the surrog loss as seen right here is go to be the differ between the two max perturb maximum a posteriori thing So go to be by the result that deriv Where wa it Where wa it here by these result right here Rememb thi is the gradient Thi is directli the gradient of our surrog loss and the surrog loss Can we make the output of the first neural network closer to someth more us So the gradient is directli given by the differ between these two thing So by the differ of margin which we approxim by the differ of maximum postur So thi requir us to run Dijkstra onc here in the forward pass and then requir it to run Dijkstra again here onc on the on thi updat graph And the differ between the two is go to be the gradient in which we have to updat our input Notic that talk I think a bit confusingli So here I alreadi said how do we need to updat our problem definit And you could think that you know we could feed that directli upstream but we The real gradient we want to feed upstream is right is thi thing right here Essential the top thing is how do we need to chang our problem definit So the downstream neural network can do a better job And thi right here is that what or sorri how doe the upstream network So the on that map X to theta How doe that need to chang it behavior in order to produc a better input to the solver Ye that is the least confus I can sai And then we return the gradient that gradient that we comput And thi is our substitut gradient for the gradient that would be Thi is our substitut gradient for the gradient of the true loss with respect to theta And sinc a gradient with respect to theta we can continu back propag through here back probat it into thi neural network here and updat the weight So that is it The onli thing not sure about is if thei realli return the Z hat right here Like it wa my impress that in the forward pass thei would actual feed the true the true Z upstream But not sure becaus for exampl where wa it Yeah here thei reli on Z bar which is Z bar is essenti mu So not sure exactli Might have to look at the code exactli but I hope you understand a littl bit of go on right here Yeah So recap We have some discret part in our neural network like a shortest path algorithm or some other combinatori solver or even sampl from or take the top k element from some distribut someth like thi Thi is not the entir algorithm but thi is on layer in the neural network The layer realli requir a discret oper to continu The question is how can we back propag through that in order to updat the rest of the network specif these upstream part right here that ar in front of it Thei need a gradient signal from the loss all the wai over here at the end So what do we do We us thi algorithm right here We forward propag sai we forward propag regularli in the backward pass We first comput a better target distribut prop a parameter of the target distribut Which essenti mean we ar go to construct a better problem definit a better problem definit that would make the downstream life easier So make the downstream life easier mean that we move into the direct of the gradient of that downstream loss We move with a certain step size And then we ask ourselv well have thi target distribut now can we make our upstream modul such that thei provid the solver with someth actual more close like the target distribut And that is exactli the gradient with respect to theta And that is go to be comput as a differ between two margin as shown And we cannot comput the margin becaus these distribut ar veri complex Thei have these constraint set and so on But what we can do is we can comput most like state exactli what these solver do And if we comput the most like state of these perturb input that is go to be a good approxim a good estim for the margin And there and then at the end we get the gradient There is a substitut gradient that approxim the true gradient with respect to the input I just want to highlight how why thi is so complic becaus essenti we have no idea how to back propag through like a shortest path algorithm The question is how do I need to chang the input right here such that someth base on the output chang in some wai For that I essenti need to know well if I chang the graph a littl bit Like if I up weigh thi edg right here How is the shortest path go to chang And thi is not a continu process Thi is a discret process right not go to chang for a while until I up thi too much and then all of a sudden the shortest path is a differ rout Like realli discontinu So what go to do and go to be a problem of select the hyper paramet like the lambda and the temperatur of the exponenti distribut Is go to be how exactli like how noisi do I have to make thi process to get an actual estim of how my output chang So essenti what I do is I perturb So thi ad ad thi nois right here I chang my graph a littl bit like thi right And then sometim the shortest path is go to chang If I do thi you know a million time then I have a good idea a littl bit of how is my shortest path chang with respect to an input chang So essenti what I do But the problem is I need to tune the hyper paramet If I chang too littl the shortest path is not go to chang at all and go to have no idea you know what how I need to adjust becaus no gradient If I chang too much the shortest path is just go to fly around wildli chang everi time And again I have no idea how to chang anyth in order to go into a specif direct So the challeng right here and the addit challeng I want to do it a million time for each forward and backward pass Idealli you want to draw on sampl and have that sampl be a good low varianc estim of what look for Cool So also like left out part of thi like entir part of thi paper that you can still look at if you so desir But thi is the basic idea Again you can take thi code you can take it like insid of a layer I think I have it open right here avail code in torch and in TensorFlow Thei give a littl bit of an exampl of thi is not the entir algorithm Thi is a littl bit of an exampl of on part of that algorithm to essenti show thi inner routin where you have to come up with good set of problem definit So here you see the essenti the sai the true problem Thi is on the left You can walk on the right path and you cannot walk on the dark squar And you can see that if you for exampl sampl the if you sampl at all if the temperatur ar set to zero then thi is what you get you can see kind of the shortest path but not realli good Right If you up the temperatur a littl bit and let the algorithm do some explor on you know us the inner algorithm you can see that over time you get a much better a much clearer pictur of what the suppos landscap is is look like So thi again thi is not the entir thing Thi is just thi inner part an illustr of why you need appropri amount of nois for that inner part You can see that over time as the algorithm infer the essenti the everi time it solv the shortest path algorithm it get a good idea over time of how the landscap look like All right I invit you to read the paper Check out the code Check out the video that wa made by the author themselv sure link somewher link it give you a fresh perspect And with that And thank you so much for listen see you next time Bye bye Oh experi Well OK Well experi better than other stuff Cool Excellent 
Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
Hi there todai talk about switch transform scale to trillion paramet model with simpl and effici sparsiti by William fetu Barrett is off and no on should see her of Googl Brain So as you can see right off the titl go toward trillion of paramet GPT three had billion paramet Thi paper claim to have a model with a trillion paramet Now is it realli five time bigger or time bigger than GPT three a debat question becaus the trillion paramet ar not us in the same wai as in a classic transform Thei ar us actual in a spars wai why the word sparsiti is in here And the wai thei ar us in spars manner is thi new architectur call the switch transform not entir new the built on mixtur of expert In thi paper also call mo e that ha been around for a while and go to see what that is Now on a high level switch transform take mixtur of expert to an extrem in that it is a transform And and the feed forward layer is divid up into these expert And the switch transform onli rout each token to on expert onli the spars part So the mixtur of expert previous thei alwai claim you need at least two expert in order to get a stabl train signal the switch transform manag to get it down to a singl expert So like a hard rout of inform to just a singl endpoint per layer of each token So in that mean you can now scale the expert And you can scale the number of paramet in the model without make the model comput more a veri special notion So you can up the paramet of the model But if a forward pass of a data point will still have the same amount of flop that it need to forward propag through the network veri special architectur right here So yeah why sai trillion paramet not necessarili compar to the billion paramet of someth like So how do thei do it Becaus previous it wa claim it wa unstabl thei have new wai of make the train stabl such as select dropout select cast of paramet to differ precis and a better initi So the high level overview of the paper And dive into it explor kind of what mixtur of expert is and how the model work And what turn out a veri long paper as you can see when paper have a tabl of content a lot of fun But a lot of engin as well And mostli interest in the model here what it can do and what doe it how doe it sort of fit in to the big world of transform and languag model and so on Last thing I want to sai trillion paramet is you know a catchi titl that most of the paper thei work with trillion paramet model thei work with model in the in the order of billion of paramet And at the end thei build a model with a trillion paramet it do as well as their model with in as their smaller model Thei also it feel like thei put that much work into it becaus probabl also quit fuzzi and expens But just know not go to have trillion paramet model around anytim soon Just yet Interest fact the origin ResNet paper also built a layer convolut neural network Even though the ResNet we have todai you know thei ar mayb or layer deep thei did build a layer model So mayb compar it a bit to that on just like we can do it not necessarili we need to So here you can see someth thei discov The curv on the left is veri veri known to peopl that ar in the languag model game sai or in the in the scale up AI game And that is as you increas the size of the model the loss will go down And loss as I understand it So test loss I believ that is perplex So scale properti exactli that that might be perplex or test loss on some downstream task In ani wai as you scale up the model paramet the model get better and better and better The interest thing right here is twofold First of all I believ thei do hold the data set constant So the data set is alwai the same the amount of comput you put into it the amount of either number of step or time is also alwai the same And in thi specif case the amount of flop per forward pass is also the same The onli thing that chang is the number of paramet Again veri special to have a model where you can scale up the number of paramet yet the flop requir to forward propag stai the same So you can see here that there is a almost unhalt decreas here it flatten out a littl bit toward the bottom though that is not necessarili doe not necessarili mean it will ever flatten out befor you know at zero I will approach zero I guess So and you can you can see that you know thei scale up the model quit a bit And also their main comparison here is the T five base So the text to text transfer transform By the wai if you know what a transform is or what a languag model is is best you go back to my earlier video and look up like the GPT three paper or the attent is all you need paper made video about lot of these thing I assum that you know them You can see right here that if you compar to number of train step for exampl the thi switch model all of them no matter how big thei ar thei provid massiv gain over like someth like a T five And thei also do thi in time So thi paper is veri much about trade off you do requir more storag for your weight So you have to have more memori more RAM Howev that memori can be distribut it can be shard becaus thei us thi mesh TensorFlow librari to implement the switch transform And becaus their model ha thi sparsiti thei can effici shard the model So you trade off more memori which can be shard But what you gain is train speed in both in term of time and number of train step requir So you ar much more effici Note that thi onli all of thi hold in thi super larg regim right We thi is thei sai also discov the speed up in smaller model But you know as far as the paper is concern we ar talk about million hundr of million of paramet billion of paramet even trillion of paramet togeth with these giant corpor corpora of of text So sort of the regim we ar in And the result do not necessarili transfer down to the lower scale problem that you know you might face with your lone on colab in the corner Alright so in a transform you have a transform is noth els but a bunch of these layer right here Thi is thi is in itself a transform layer in it basic form And it consist of sort of two part it consist of thi self attent right here Now the standard transform self attent what wa introduc in attent is all you need And been us ever sinc in all the transform Thi on right here is a is an as I understand it a languag model So you know thi thi is veri standard Howev after the self attent you have thi feed forward layer Now usual what you do is you have an input sequenc and you transform that through multi head attent into anoth sequenc right here Okai And then what you do is you take each of these thing and feed them through a feed forward layer And if I am as I understand it thi feed forward layer is simpli you know a regular feed forward layer that you would find in a neural network and you pass them you pass these thing individu So thi here a vector you pass it through here and boom that becom the next layer represent thi thing right here you pass it through as well boom that becom thi on and so on right you pass them individu to get the next layer represent So thi thi part right here the attent part it sort of aggreg inform and relat the individu item of the sequenc to each other And transform them into you know a new sequenc where sort of all the everi token can gather inform from everi other token what the attent mechan doe step on In step two everi token is isol everi token is for itself And the feed forward layer simpli determin you know given on token given token number on what is you know given it represent in thi layer what is the best represent for the next layer Okai So token number on of the next layer So the multi head attent is kind of relat token to each other and the feed forward layer thei ar relat layer to each other Okai so up here you would have the next multi head attent layer So you can see the feed forward layer as sort of translat from on layer to the next layer right get sai oh you come from thi layer go to translat you such that the next layer understand you And that happen on a token by token basi Now you can see thi is alwai the same feed forward layer for all the token right the token ar sort of treat like a batch of sampl The idea of thi switch transform and also of the earlier mixtur of expert transform is that it might not be a good idea to have onli a singl on right thi is the onli feed forward layer the same for all the token it might actual be a good idea to have a coupl of them that sort of special in differ thing So what could that be You know in a in a basic world thi could just be like on for noun and thi could be a feed forward layer for verb verb token that ar verb token that ar adject and sort of mayb here is like punctuat token right You might think well if you ar a noun token the next layer might want to look differ at you than if you ar a punctuat token right So thi translat from on layer to the next layer can now happen depend on what the token repres Right Now we we of cours first of all we have these annot And second is not necessarili that you know we want to alwai divid it by noun verb adject punctuat Idealli we want to learn thi rout So we simpli want to sai look instead of just on feed forward layer we give the model a feed forward layer Give the model four feed forward layer feed forward layer on two three and four And for each token the model can decid to which of these feed forward layer it send the token to So here you can see thi is a token Now you know we ar deal with word piec just sai the word more I wa like I wa thoroughli confus by when I saw thi like huh why doe it sai more paramet but here the string more right and the string paramet And these ar in the vocabulari and thei get an embed vector associ with them So go on here Then thei go through self attent as you can see here both go through self attent and then each on of them is rout to on of these four expert Now the on here the on on the left and the on on the right these ar the same expert right just duplic visual here but these would be the same weight matric in there So you have four feed forward layer in thi layer And each token can be rout to ani on of them And thi rout here thi is learn So in here you have a matrix thei call it like wr And us wr you simpli do an inner product of wr with your input right here call that h with your input h I guess thei us h for a differ thing I think thei thei call thi x again So you do thi with x And then you get you get h which is your rout And then you simpli build a histogram you normal the histogram I think with a softmax And that those ar your rout weight So veri much like anoth attent mechan except that the queri thi thing here these ar like the queri these ar sort of the queri of thi attent mechan And thi here these ar the kei and the valu So a good kei and the valu of thi attent mechan The queri ar just learn So the queri ar not dynam gener And the kei and valu thei ar not Yeah a a weak analog But you can sort of think of it like thi So there is thi rout mechan And it decid where a token get goe to Now as you can see the router is soft that mean there is never a on or a zero right here alwai kind of a number in between but thei hard clip that So thei hard clip it thei just rout it to the maximum As you can see here number two is the maximum And thei just rout it to number two thei rout it proportion or anyth thei just to take our max and thei rout it through thei do multipli the output by the actual number that thei got out here So if the router is unsur then the output is less If out is sure the output is more But thi hard rout is the kei right here And that mean you know befor befor have on feed forward layer So ani token that goe forward goe through on feed forward layer If you do a mixtur of expert in the classic sens and you rout it in a soft wai you now have four feed forward layer So everi token goe through four of these comput So basic multipli the amount of comput by four becaus multipli the amount of paramet by four right you have four time as mani paramet Now when you do thi arg max rout like the switch transform you have multipli the number of paramet in your model by four but ani token will still onli incur on feed forward layer That mean you keep the amount of comput that you do per forward pass the same And sort of the kei right here So now thei can scale up massiv the number of expert while still keep the amount of flop the same And notabl you also need a ani data transfer in between the expert Everi everi expert can be can you know receiv their token and then do their independ work So you can effici shard thi across mani mani machin Thi is how thi look So in in thi case you have three expert and your sequenc ar of line of length six So you want to sort of rout each token there and there can be overflow like everi token is independ rout So it can happen someth like thi that a you know a token like three token get rout to on expert but it onli ha space for two token And thei have some trick like thei have thi capac factor right here or thei can rerout These ar veri much engin thing which ar import but you know thei chang the sort of final final result Now I want to go down here where thei have a displai of thi shard more like an explan of the shard which I think is veri illustr So how what do thei essenti do If you think of mani machin you have machin So each littl squar here is on machin Okai Here ar the differ wai of how you can shard a model and model shard Now we ar not go to build a machin anytim soon that can hold a trillion paramet just not go to happen Okai so you need to somehow shard the model or the data or both And these ar the differ wai how you can do it So if you us data parallel that is the easiest that is also directli built into thing like pytorch and so on What you do is so the top row show how the model weight ar split and the bottom row show how the data is split So how to read thi is when you do data parallel the weight ar split such that each of the core ha the same weight you see so thi these weight right here ar the same as these weight ar the same all the same So thi is shard the data is run so that you take a data set you take a batch of data And now you distribut thi data point goe here thi data point goe here thi data point goe here and so on You distribut the data and you do the forward propag And at the end you sort of gather them again right So you gather them togeth Again becaus you have to you know calcul your gradient Okai so data parallel the model is spread out And if you want to do an updat to the model then you need to commun around these weight Okai so all these differ piec have to then commun with each other when a weight updat If you do data parallel here is how the data split alreadi seen thi So on piec thi piec of data is split over core So you can see like thi core right here onli ha thi littl piec of the data and not all of the data On the other hand you can do model parallel In model parallel you can see exactli the other wai around name that on core onli ha a littl piec of model right And but everi core get all of the data So thi data here the bottom row is data all of the data The point here is that if you do model parallel what you do when the model itself fit right over here the model fit on your machin but not the whole batch at the same time Model parallel you do when the model itself is not the same You do when the model itself fit what you have to do is you have to take your data right And you have to send it sequenti So mayb thi is the first layer like layer on weight And then you have to comput layer on and then you have to send it to layer two and so on So you have to send it sequenti through the through the shard of the model right Becaus you want to forward propag through all of the model Thi is ha veri veri much of a cost of commun you can build veri big model but it come at a cost right At the end you get your y and you calcul your loss and you back prop again backward through the whole thing You can mix them right You can do model and data parallel So here you can see that the weight so thi is thi is layer on weight layer two layer three layer four And here again you have layer on layer two layer three layer four and so on So you can mix the two in that you can have model and data parallel if both your model and also your data fit in a singl machin And you can see here that the thi upper left part receiv thei receiv the same data but thi here receiv differ data right So you split your mini batch into four differ part And you send the first part up here like data on you send that up here and that goe through the model in thi sequenc sequenti fashion you send data to right to here and so on So we mix the two Now in expert and data parallel what thei what thei do in the switch transform So thi here is the switch transform And thi here over here will then the switch transform trillion So for the trillion model thei actual need to mix all of them But you want to add you know if you can you want to avoid model parallel model parallel is realli the thing that kill you becaus of the veri high commun cost So in the switch transform thei have expert and data parallel What doe it mean So the top row is how the model weight ar split And you can see the weight ar split but the differ color mean that differ weight So here ar weight number on weight two weight three weight four and so on Now alreadi had thi over here right Differ weight in the model parallel case were split over differ machin Howev if you look at the data the data is also split and the weight not the same And these ar exactli these expert So expert Thi mean that you know thi piec of data here onli goe to thi expert and then to the output thi piec of data right here onli goe to thi expert and then to the output right there is no commun between the differ expert wherea here you have thi super high commun Okai so you can see you can scale up the expert as you scale up your data as long as each shard of data is rout to onli on expert And then of cours you can mix the expert model and data parallel If you realli if not even a singl expert fit on a machin right If the case you need to again shard you do model shard on the expert Alright so the switch transform as I said thi here is the switch transform that the most of the paper is about And now we can dive into the result The result ar pretti spectacular Thei mostli come compar as I said to T five base and T five larg And as you can see right here the switch model ha significantli more paramet So or here billion paramet compar to not even a billion of T five larg yet the number of flop is match So thei build model where the number of flop for forward prop is match But the the number of paramet ar higher So you know it is somewhat of a fair comparison right You have the same amount of comput done per forward prop And now we see what doe it help to just have raw gain in paramet And it turn out it help a lot probabl alreadi seen that we get these massiv speed up massiv sampl effici over a dens model probabl so thi look at exactli in the in the intro thei also have benchmark on see thi down here thei also have benchmark on multilingu on multilingu data set And you can see in everi singl languag the switch transform gain on the dens transform by quit a bit So thi is in thi is lock space as you can see and quit impress actual And these gain ar in time as well as number of step So pretti pretti cool So as I as I said the the trade off here of cours is that you need more machin you need to actual add more machin And you can see thi largest model that thei built is thi switch XXL which is match in flop to transfer to the XXL model yet ha mani more paramet and beat the at log perplex and as I understand in downstream task by quit a bit Thei also built thi trillion paramet model It is not as good mainli becaus thei as I understand it thei just want to get to a trillion paramet And I think I think you know train realli easi at that size So thei scale it down as you can see it ha less number of head less number of layer but the number of expert ar wai up So how thei scale to a trillion And the result ar you know better than the XXL which is impress given that it ha less flop per token Howev it is still wors than the switch XXL So the trillion paramet model still you know still not everyth to have a lot of paramet you actual need to do good trade off And here trade off too mani paramet for you know less number of head and less number of layer And that hurt again So veri veri interest stuff right here The last thing I want to look at is their trick for get thi to work So thei detail three trick for get thi to work And thei ar right here three trick how thei can do thi And peopl befor them have said no you need at least two expert otherwis unstabl So thei do select precis with the larg spars model which mean that if for some of these comput it you know it it pai off to do them in higher precis you want to send around these flow precis thing you want to send those from machin to machin right So you have your input you have your multi head attent And then here again thi is whatev x prime and then you send that to the expert Right here ar the differ expert And then you send that you send that back And why okai now you want thi here is commun cost If you were to send around float vector a lot of data that you have to transmit So rather send around bit precis right as thei do right here And howev if you do bit precis you know the whole machin learn part work as well So what thei do is thei do as soon as it as a as soon as a vector arriv here thi is in bit thei scale it up thei cast it to a bit vector thei calcul us the bit vector And then thei cast it again to a bit vector to send it back And that seem to work So thei do select select cast the precis up And also thei do select dropout down here So thei do expert dropout which mean thei appli dropout to the whole network uniformli as you would do regularli normal but thei sai thei can do a much larger dropout rate at expert layer And that make a bit of sens becaus the expert each expert is onli us veri spars So it make sens to up their dropout rate Becaus you know in the end you might drop out as much signal from a spars us expert if you rais the dropout rate then you do from a dens us layer in a with a smaller dropout late rate And the last thing is that thei simpli do better initi So thei find if thei scale down the the initi scale of the origin transform by a factor of that lead to a lot more stabl train astound that after so mani year still someth like initi can you know make or break such a model that is just insan to see a lot more to thi paper thei do a lot of downstream task thei also talk a lot about you know thi is not onli thi model thei do a lot of optim under the hood thei us mesh TensorFlow and so on clear that a lot of work ha gone into thi And interestingli enough thei can also distil these model So what thei can do is thei can take thi larg model and thei distil it to a model that is as big as t five base a dens model So thei go from a spars larg model and thei distil it into a dens model that is equival to t five And thei do outperform t five if it were train from scratch and thei gain up to someth like So of the gain thei made from here to here thei can retain by distil it down Thei sai thei can distil it down wai over of the model which is also pretti interest And you know pretti cool Becaus then you could sort of distribut the train model around and peopl could us them Alright so that wa it for me definit check out the paper and all the experi downstream task and so on a veri cool paper ha a lot of cool experi 
Optimizing Neural Network Structures with Keras-Tuner
What is go on everybodi and welcom to a tutori slash showcas of the Kera tuner packag So on of the most common question I get on deep learn tutori and content in gener is peopl ask how did you know to do n number of layer or why n neuron or why did you do dropout why did that degre why batch norm all these thing like why did you do that And the answer to that question ha alwai been trial and error and anybodi who tri to tell you thei knew what model neural network wa go to work is a dirti liar trial and error Now of cours there ar some task like MNIST for exampl a paper bag could solv MNIST to accuraci or more So obvious there ar some problem that ar just so simpl ani network will do and then there ar some neural network that will solv most problem especi like imag problem stuff like that But for real world problem that solv yet the solut is trial and error And thi ha histor involv me write for loop to solv it and then in that for loop I just tweak thing and then I run that overnight and then I save valid accuraci or loss or both and then in the morn I just see okai these ar the attribut like three layer at node per layer it seem to be the thing and then now test with batch norm becaus everi time you chang on littl thing like dropout or not batch norm or not and all those thing you got to keep test So anywai histor just been ugli for loop to be honest with you But recent I came across the Kera tuner packag which doe everyth I wa do but better and it doe a few other thing that I even do which is pretti cool So I thought I would share it with you gui basic the crux of it is you got a model and then you can defin littl hyper paramet object insid that model and then you creat thi tuner object and then it chang those hyper paramet and the hyper paramet you can specifi you have to not everyth ha to be a tunabl paramet but the on that you do ar you can do anyth like an int float you can do a choic a boolean and the int and float ar like a rang with a step size So anywai pretti cool to get it you just pip instal keras-tun Also us kera tuner now as a tutori person thi throw up ton of red flag for me but still go to do it anywai But mean on of two thing it either will never get updat again which would be sad or two it is go to be updat a ton in the next year and thi tutori could be render out of date veri quickli If hit error check the comment section googl the error or you can pip instal kera tuner the exact version that us So onc you have that readi to get start but first a quick shout out to the sponsor and support of thi video Kite which is an ML and AI base auto complet engin for Python and it work in like all the major editor sublim text vs code pycharm atom spider vim but why Anywai all the major editor and actual a good auto complet honestli I hate auto complet so when thei reach out and were interest in becom a partner I wa like see but actual pretti good It took me a littl bit to realiz how good it wa and in fact I us it for about a hundr hour now and the real test wa when I remov it like I just disabl it just to see what the differ wa and I wa like oh wow so mani big differ the biggest thing is like the auto complet not just variabl like method and even like snippet of code and like the import ar so much nicer huge differ show some of them here but I highli encourag you gui put a link in the descript for you gui to check it out super cool and it come with kite copilot as well just kind of like a stand alon app if you want it anywai and basic like live updat document for whatev work on which again is super us so yeah a realli excit sponsorship I take like ani sponsorship if you notic like I do VPN mobil game all that stuff becaus it just realli make ani sens but thi is actual a realli cool product so excit for them to be support the free educ and I definit suggest you check them out the auto complet is the best so okai cool so go ahead and get start so first we need a data set and we need a model like so mani thing that we need just to even get start with Kera tuner so go to try to like truncat thi as much as possibl but first we need a data set the data set that go to us is fashion MNIST so like MNIST onli like I said MNIST is like too easi so kind of no good for show thi so go to us fashion MNIST so go to sai from tensorflowkerasdataset go to import again just as an exampl here ar all the data set fashion MNIST so then fashion MNISTloadData is what we need to load the data and then just to shill out complet kite copilot come down here it return thi right here so actual just go to copi and past thi sinc most peopl probabl run kite copilot right now you can go to the text base version of thi tutori or download kite real quick anywai so all your data and I think just go to show a quick visual exampl of the data and then probabl just copi and past a model honestli I want to wast too much time so first just quickli displai on of the data so go to sai import matplotlibpyplot as plt love auto complet thank you veri much pltmshow oh no it tri to help me mshow and then do xtrain go with the zero-th and then pltshow and then also go to do a cmap cmap equal grai here onli becaus go to be all color if I and peopl ar go to be wow loud peopl ar go to be kind of confus so go to sai python ktuner tutori so thi is kind of the data so again a by like mnist black and white got classif just articl of cloth rather than handwritten digit so in thi case like a boot or a shoe of some kind I realli know what that is so do a differ on see hopefulli we can get someth a littl more recogniz so some sort of short sweater thing so what deal with so just a littl more challeng to get like accuraci as compar to like mnist so thi is a thing that we can practic tune on mnist just work so onc we have thi we actual need a model now again I realli see ani benefit in thi tutori for me to like write out thi entir model for you gui it would just be a wast of time so actual just go to copi and past either copi and past thi into the descript or put a gist or you can just go to the text base version of the tutori but thi should all be except for thi hp here thi should all be total understand by you gui there should be noth here that is like whoa that so hp stand for hyperparamet which is what go to us to go through thi is a comment that I made for the actual tutori so thi just creat our model object return the model pretti simpl so from here you can defin the model and do afit for exampl so the other thing that we need to do is import so go ahead and from tensorflow or actual from tensorflow go to import kera then we need to import all that layer inform so from tensorflowkeraslay import all of these basic go to need everi singl on so dens actual not us dropout but ye what els we got we got activ got a flatten and got some max pool cool done ran over my face a littl bit again there is the text base version of the tutori and again realli just try to run through thi there realli be anyth confus to anybodi here so onc we have thi real simpli we can test thi I just want to make sure it work so go to sai model equal what do we call that build model so then from here we should be abl to sai modelfit yet again cool thing from kite is thi entir snippet boom done for you and then you can kind of tab through it so for exampl batch size go with epoch so then again just tab through that which is pretti cool valid data is go to be a tupl of inform in thi case what ar we go to do go to sai x train y train so x train or x test rather oh it tab me over x test y test and then actual in thi case I care about verbos I had it there becaus I wa build thi tutori in a notebook first so ok so then we can do that oh we need to have our x data so x train and then thi will be our y train and we need to reshap the data befor it complain to us x train equal x trainreshap and neg on twenti eight twenti on again if thi is confus to you I strongli encourag you to go back to on of the deep learn playlist ok so that should all work I do want to test it real quick to make sure it work befor we get into the actual tune aspect of thing so go to save run that again pleas work oh becaus we have that hp there let me just move it real quick put thi back pleas work whoa ok so train we can actual see alreadi work we could just go to on epoch so for exampl after on epoch actual not that accur like percent not much but whatev you got I can just tell you not abov percent ok so the question would be ok what do we do from here can we actual do better than thi model so the fashion MNIST dataset is still easili learnabl not easi is be percent accur as oppos to like MNIST so what you would do from thi point like if me and try to compet in a Kaggl competit now just go to start chang stuff but what we can do with the Kera tuner packag is we can like autom that entir process so what like to do now is go to move that over and go to import a coupl of thing again go to try to keep thi super short so go to as short as possibl anywai go to just copi and past that in so from Kera tunertun go to import random search and then from Kera tunerenginehyperparamet just import hyperparamet and again these ar the thing that allow us to do like the int the float in rang choic boolean and I want to sai anoth on but I forget it but anywai again text-bas version ha link to all the doc just try to show you a gener idea of how all thi work the next thing I want to do is go to import time and then go to specifi logdir equal and for now thi is kind of a silli on but anywai thi is a stupid F string to us but whatev time time time for now the logdir will just be a timestamp but you could add other thing to thi if you want but again try to keep it short and simpl so ok so build model now we will pass the hyperparamet and so far noth is actual uniqu here so the next thing that go to do is go to come down to the veri bottom here and go to comment out these two thing becaus thi no longer will even work and in fact just go to get rid of them becaus that build model is no longer function in the wai that we have as long as go to pass hyperparamet there so first go to specifi the actual tuner that we intend to us and go to be a random search and here go to pass a bunch of thing first is the actual function that we intend to us build model no parm there the random search object here will automat do the hyperparamet stuff for us so you just pass the function name here for your model then got object thi is the thing that interest in track so in our case val accuraci what interest in then go to sai max trial go to set thi to for now I know for now we have no dynam here so it matter go to set that to explain that in a moment and then go to sai execut execut underscor per trial equal and then we just give it the directori so directori equal logdir so max trial and execut per trial so trial is how so like thing can get option can explod veri quickli when allow ourselv to have a dynam number of layer a dynam number of node per layer or featur per layer and then a bunch of boolean oper like do we want dropout and then if we do what rang of dropout thi can explod to like thousand or even million of combin pretti quick like with a two layer convnet pretti hard to get to million but it can get to a huge number like a thousand differ combin of model is not go to be veri hard so here when we sai max trial thi is just how mani random pick do we want to have and then execut per trial thi is what you would like if just try to search for a model that learn at all then I would keep thi to if try to eek out the best perform I would set thi to like or or mayb even more as long as like shoot in the dark just try to find someth that work or just a gener idea of what seem to work I would keep thi low but the point of thi is is each dynam version go to train it thi mani time so if got some sort of you know when it randomli sai hei do layer at node per layer it will train that on time but you might want to sai or time becaus if try to eek out like accuraci given differ run from the exact same model you might find that you get more than a differ higher than so if realli try to eek out perform on you might be seek a model that ha a smaller standard deviat of valid accuraci but also you might want to run it a few time to figur out what the true averag is becaus you might have just gotten lucki or mayb you got unlucki or whatev so hopefulli that make sens if that make sens feel free to ask in a comment below so our tuner object and then now what go to do is actual do a tuner search so tunersearch and in thi case go to specifi our x data which will be x train and then our y data will be y train and I think go ahead and tab that over actual I suppos if we want to be all I think correct to remov those x y and then we need epoch so how mani epoch do we want to train everi singl time again thi realli depend on your need go to set it to on just becaus to iter over thi stuff it still is go to take a while and in fact probabl not even go to do it but local just suggest mayb start with on batch size again thi is go to vari by what run thi on go to sai and then valid data did I do I did dang it got to type it valid data equal been so spoil I realli had to type too much x test and then y test ok so now what thi will do is search given these paramet right and then what kind of stuff is it go to search go to base on thi inform here onli go to run on trial again go to just quickli run thi to do a bug check and then actual start make the model truli dynam so cool run good so now what go to do is come to our actual model and start ad thing that make it dynam so first of all so for our input layer sai we take and we want to yeah sure mayb be but mayb we want it to be anyth from to but we want to sai like right we want to have a step size there so the wai that we do that is hp capit I int and then go to give it a name and so go to sai input unit not shape and then the start valu max valu so to and then the step size go to sai so and in fact just add the actual name to thi on not go to do it everi time but just to make it super clear min valu max valu max valu and then thi is our step so just make sure on screen not cover it so now the input unit is a dynam valu that will be randomli chosen but somewher in that rang okai so yeah look good so again go to save that realli quickli and just make sure we run no bug cool so in theori that is with a differ you know who know I know which on it wa in that case the tuner doe save all that inform just again bug check at thi stage so random search and so okai got a uniqu or a dynam number of for the input to thi thi convnet and then we can come down here and then so the input layer is a littl uniqu becaus it ha an input shape that we need to make sure we retain but then the subsequ model or the subsequ layer can ar all basic the same and in fact go to remov max pool becaus go to caus troubl for us go to run out of thing to pool if we in some case so go to remov that and now go to sai is go to in rang and again go to pick hpint and then the name in thi case will be n underscor layer and we will go from to and sinc we want the step size to be we need to input anyth more so from here do that so now model add and then again what if we want thi again to be uniqu so go to take hpint here copi that past that over that rather than input unit actual go to sai underscor i make thi an f string and go to sai conv i unit and again to cool so look good to me model add relu ok I think good we can go ahead and save that come down to here go ahead and run that make sure it run it doe ok so I could continu to let that run and in fact it might see I think we said on epoch so hopefulli at the end of at least on epoch it should give us a brief summari I would hope oh right we said onli so at the end of it ok so we can see fine in thi case we had a neural network so that input convnet wa so the input unit wa then we had two conv layer so a littl bit out of order actual but the input unit wa here right so we actual had a so it wa by by not by but each layer so the input layer is like the convnet ha featur and then it feed into featur then the featur so it sai n layer is but actual plu on more just n layer is the answer to thi for loop here here right so the actual number of layer is n layer plu on I hope that make sens come back down here so then thi tell us what the score is it also tell us what the best step for that score wa which is us but we onli did on epoch anywai ok so we get the inform but of cours thi wa rel meaningless becaus we we onli test on thing so the next thing to do would be to make thi a much much bigger test but each test take thi sai second so and gonna vari depend on how mani layer you do and all that so just go to vari what done is I went ahead and creat I just went ahead and save it as a pickl object so I took tuner at the end you get thi tuner object so at thi stage all I did wa import pickl I save the tuner object to a pickl thought I had save it but I made a mistak so I end up just retrain everyth so here I am like an hour later got my pickl got my directori and just to realli quickli show you insid thi directori and then insid of untitl project we have all of the trial that ran and then for each of the trial we have a checkpoint which obvious is the model and then the trialjson file contain obvious your ID but then the hyperparamet more importantli and the score that that model got so even if you have the pickl you could still go back in time and get thi stuff but anywai it would be kind of challeng and tediou and annoi especi if got mani iter per trial have to pair up I guess by exact hyperparamet I realli know anywai it would be annoi so nice to have the pickl save instead just to come back later so yeah so cool and so you have that and then also I want to sai here got is it the oracl I think insid the oracl that ha the best inform definit not tuner but you can also interact with the tuner object which is why I save it as a pickl so first of all we can get the best hyperparamet right out of the gate just by run that and in fact befor I do that let me comment out thi thi and then we want to do the search again so go to comment that out cool ok so just run that real quick and we get here the best hyperparamet now I realli find that veri easi to read but then the next thing that is a littl more us to me is the result summari so thi should give us the top I think by default top model so here thei ar and as we can see here the top got basic the best wa but anyth from to within what accuraci it look like or so pretti close there and also is the veri best interestingli enough in the other on that I train that I lost the best wa so I do think we could for sure find someth more than and if you run thi total possibl that find some that ar better than and the reason for thi differ is even though it seem like we made veri minim thing to make dynam we actual did a lot and so to actual run through all of those I actual total even know so you could have up to and then layer of each uniqu convolut layer and then of all of those combin time all the possibl combin here I mean it would just be a huge number got to be hundr or even in the thousand I know someon go ahead and do the math if you want anywai a lot so a lot of combin but that just goe to show that I alreadi know that somehow with thi exact code you can get actual just a function of run through all the combin so again veri nice if someth can do it just automat for you so final the last thing I will show is that you can actual from here get the actual model so a tensorflow model so print tunerget best model and then do a dot summari here but thi is an actual model so you could actual do a dot predict as well just go to do a dot summari just so you can see it I think thi is the easiest wai the hyperparamet is still kind of hard to be like ok but how would I build thi model again wherea a dot summari realli just make a lot of sens to me so anywai just do that realli quick but mostli just to show you an actual tensorflow model so as we can see got the input layer activ pool right so you just build thi model exactli to these spec so ok I think enough inform hopefulli you gui have enjoi like I said thi is on of the most common question I get the answer is probabl not as intrigu as you mayb would have hope but hopefulli Kera tuner can actual make your live easier if you have question comment concern whatev feel free to leav them below and again shout out to Kite if you want to try them out thei ar a total free plugin and like I said pretti awesom like realli enjoi it so hopefulli you gui will enjoi it as well and thank to them for support the video 
Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
Hi todai look at batch normal acceler deep network train by reduc intern covari shift by Sergei Ioff and Christian Skedadish not my not the best pronounc Segedi close enough Alright so thi is a bit of an older paper and I think still good to look at it relev and peopl just kind of throw batch normal into network and mayb realli know what do So look at it So what these peopl argu is that in a network usual you have structur like thi So if someth like that it mean that your loss kind of thi is a two layer network your loss is a composit of the first layer on the input u with paramet theta on and the second layer with paramet theta two So conceptu that would look someth like thi You have your input mayb an imag right And you put it through the network and it becom some intermedi represent right x zero x on and or mayb call it even h h on hidden represent right Then that becom then through the layer becom h two and so on right So these thi stuff here these thi would be weight matric w on w two that transform the imag into a new imag or whatev So what argu is that well if you onli consid a singl layer like the first layer here kind of the same if you onli consid the second layer with the h on now as the input right pretti natur to see each layer of the neural network is kind of like it own transform take input and produc some output So what peopl usual do with the veri first input here with your data in machin learn gener is so-cal whiten the data which mean that thei have thi over here The data is whiten I find it But what it mean is you basic want to if you have data sai here is a coordin axi you have data and you want to you might want to do a kind of a linear regress on it and you have data kind of like that right It suit you to transform thi data into by first of all look where it mean is mean is about here then subtract that so here here and then kind of divid by it standard deviat in each direct So a standard deviat here and there is a standard deviat here So you would transform thi data into someth like mayb someth like thi So you see that the mean the mean is now in the middl and not so elong anymor So you have a much easier time to kind of learn someth on thi data than on thi data over here simpli becaus our classifi usual tend to reli on like inner product And if you do an inner product here you have on of these vector here and you do some inner product alwai go to be far awai from the mean and therebi the inner product ar go to be larg no matter what right Wherea here if you take a random on and then anoth random so if you take two random point here two vector from the mean ar almost the same Wherea if you take two random point here thei tend to look uniformli in the direct So kind of the sens we know that machin learn method work better if you widen the data first So these peopl ask hei why do we onli do thi at the veri begin right Why we if each layer is basic take it input and learn someth each layer is basic a machin learn method why we just widen the data to everi singl layer or you know everi singl sub compon of a deep network And the kind of basic step here So thei argu how thi ha been kind of tri befor or what kind of method you would usual get and why these so good Mainli becaus you kind of need to intermingl thi whiten with train the network And therebi if you just go about thi naiv then you would not you would not you kind of produc artifact from train So thi section thi section here where thei argu that not realli you realli go about thi super naiv But what thei do super complic but thei just do it in a smart wai So jump directli to that What thei sai is okai look at what thei call normal via mini batch statist Right sai we have some some D dimension input x right And just go to look at per dimens So we onli care about per per individu dimens normal Alright so what do we get What do we need to do When it take the kth dimens go to subtract from it the mean of the kth dimens within a mini batch right within a mini batch of data So mini batch mayb someth like exampl or exampl or someth like thi and then divid by the varianc of that mini batch So thi is thi is done over here in basic so you comput mu B mu of the mini batch which is simpli the empir mean of that of the data at that particular layer And then you comput sigma squar B which is simpli the the empir estim of the varianc of that of comput on that particular mini batch And then you transform your data by subtract that and by divid it by thi and thi thi thi constant here is simpli to prevent from divid to by you know two two small valu So you get like in numer problem So what doe it do It doe basic what we what we did abov But now what thei sai is okai we want to make sure that thi transform can potenti you know repres the ident becaus sometim or like a natur natur if you had to do someth with your input when give it to the next layer like the veri baselin is to do noth to it right to do the ident transform But if you do if you do thi you probabl end up with the ident transform except if the mean is exactli zero and the varianc is exactli on right So what thei sai is okai also introduc two new paramet to thi here is thi gamma and thi beta here And these ar learn like other paramet in the network we learn the paramet gamma and beta and gamma and beta ar simpli gamma is simpli a scalar that thi transform x is multipli by and beta is simpli a scalar that is then ad to it So in each dimens of your hidden represent you basic learn how to scale it and how to shift it scale and shift after done the normal So first first you do the normal Where is it Right First you go from thi type of data to thi type of data And then you sai well but mayb actual more benefici to you know have it not center or whatev So so that the network can actual learn them to transform thi somewher Thi might seem thi might seem redund but realli power Becaus what basic sai is that okai thi probabl the best you know distribut thi probabl is better But if the network kind of if the backpropag algorithm or the train algorithm decid that thi first represent wa actual us it ha the option of go back But it also ha the option of go to ani other kind of form of distribut So so pretti power in term of what it doe Okai not realli correct here that it ha the power to go to ani distribut becaus onli kind of per dimens scalar that it learn but still it the potenti to transform the distribut by these learn scalar is is pretti big Alright So basic it the whole the whole shebang You normal your input to each layer by thi formula And then you introduc new paramet that you learn along with your network paramet So thi kind of ha some implic First of all on implic is thi here If you build a batch norm into your network it kind of learn thi thi plu beta which is basic a bia paramet If you think of a tradit kind of fulli connect layer thi the fulli connect layer becaus the scalar here is onli per dimens But the bia in a fulli connect layer is also just per dimens So the beta is equal to a bia in a fulli connect layer So if you have a batch normal after or after a after a fulli connect or convolut layer or anyth that can or sometim ha a bia paramet almost not worth it to kind of learn both So you would rather just onli have the on from the batch normal and leav and us the convolut or fulli connect layer without a bia So kind of on implic Another implic is we have just lost the kind of the the abil to have determinist test time infer So much like dropout which is kind of random drop out of node here we have quantiti that depend on the mini batch So not onli the individu sampl but thei actual depend on what other sampl ar randomli select to be train with that particular sampl So kind of awkward if you kind of want to have some determinist reproduc thing at test time So what peopl do is and here thi is discuss what peopl do is while train thei us these quantiti the did the quantiti we just discuss but thei keep kind of a run averag over them So what I would do is in each mini batch I would comput thi mini batch mean and thi mini batch varianc And I would keep quantiti I keep run averag of them right And at test time go to plug in these run averag So noth depend on the mini batch anymor so that pretti neat trick I think And you can you can even imagin like at the end of your network train simpli us these here to kind of fine tune the weight to these exact paramet So on thing kind of you have to pai attent to So in usual neural network librari there ar there ar paramet you can set whether or not thi network is in train mode or in test mode And depend on that the batch norm layer will us the mini batch statist or will us the kind of all over data set statist Alright the second thing is train So how do you actual train thi thing Becaus now you you just write we we start with our with our multi layer network up here Right right First go to put my thing through And then put my thing through Right and the the back propag here is is quit easi So let me get rid of thi The back prop here is quit easi You go to L and mayb you want to drive it by theta on Right so you first go to drive it by the hidden represent on and then the hidden represent on with respect to theta on So the hidden represent would be whatev come out of here or sorri not I and so on So you kind of chain rule your wai through here But now in between these layer here you have these batch norm thing And so that the author discuss how we now do back propag in the face of these thing So basic what thei discuss It actual pai to to have a graph of go on So here is x thi is the input to our layer right So what do we comput from x we comput mu just call it mu or mu B call here right Thi is the mean of all the So thi is x x i until x well x on until x n thi is the mini batch We comput the mean and then from thi and from thi we can comput thi estim of the varianc right We need both Right so we now have the mean and the varianc over the mini batch So go to take on of these just the i th on right And go to us thi and thi to comput x what comput x Is it call hat Yeah probabl call x hat right Yeah we saw about x hat So x hat is x hat i is x i minu mu B divid by sigma squar B the squar root of it plu thi kind of littl constant here gonna leav awai the littl constant for clariti sake Actualli in the calcul here But so then we have a new paramet gamma right And go to us it and our x hat to comput and also thi beta here to comput y hat y or y just y And of cours thi is i thi is i So and thi here is our final output of the layer So you can see now the back propag path if you go through here So the back propag path if we have some loss come in here we back prop through y i right So here is the L the loss to y i here right So if we want the for exampl the back prop with respect to beta what we do is we simpli and thi is thi is over the mini batch of cours we simpli back prop here through thi path So in our in our formula for beta there should be onli mention y i And what we see here right In our formula for gamma there should onli be mention of y i So becaus the path lead onli through y i Oh no sorri actual becaus the of the what I mean is of the deriv with respect to y i Of cours the we also have to pai take into attent that thi is multipli here by thi x hat i where of cours not the case when we just add someth becaus the deriv of two of an addit like x plu b with respect to b disregard x wherea if x time b it disregard disregard x All right So if we Yeah so you can you can go back So the interest bit basic come when we want to find out okai how becaus here is here is anoth layer right down here somewher there is anoth layer And we basic want to know thi input here to the next layer how do we comput it in the face of thi mess here Becaus we not not so easi right So you have to see we have three path here we go back through x and let me get rid of these blue line We go we go back through x hat directli to x We go on path is through here And on path is through thi thi mu So basic have to comput deriv with respect to sigma squar and mu and for that we need the deriv with with respect to x hat So basic the wai back prop work is you just find all path from where you ar to where you want to go And then you you kind of iter comput thi So thi on here is the easiest the easiest as you see here thei did it on top Well first thei did thi on which is simpli go from y to x hat i start then thei go from x hat i to sigma squar which simpli involv kind of the revers oper of how you got it Thi is simpli a deriv formula here of the of the divis by squar root Then you can us thi you can us thi quantiti here to comput that So basic you just go in revers of how you comput the oper in the first place we said we need mu B to comput sigma squar B Now we need the deriv with respect to sigma squar B in order to comput the deriv to mu B And onc you have that and you see the addit here the add here is the fact that whoop is the fact that two thing contribut to mu B So two path lead to lead to mu B One path is from from here and on path is through here Right So here there should be a green Sinc two path you have two compon to your deriv and you add each of them So how to be And then thi here with respect to thi x here we have three path right becaus we have three arrow go out of x i on here on here and on here So we have to take into account all of them Right So thi on is pretti easi the first on Then the second on sorri thi the second on goe through thi mu B which alreadi comput And the third on goe through the sigma which also alreadi comput Right And these ar ad Becaus all the path you have to add all the path in the back prop algorithm Mayb do a video on back prop later to get to realli dive into how thi work And final thei thei comput these these alreadi discuss So in essenc the whole thing is differenti You just have to kind of pai attent how to do it But the whole thing is differenti And therebi you can basic back prop through a network that ha these batch norm layer in a built in So pretti cool I just want to quickli jump over to the result And yeah keep in mind thi paper is from So network that big Back then We know that much about train yet But the interest thing is thei basic discov look we can we can have drastic fewer step in order to reach the same accuraci And these ar kind of the activ of the network over the cours of train So without batch norm you see especi at the begin larg fluctuat in the activ And becaus becaus thei us batch norm now no such thing So basic the reason for that is pretti simpl right While you learn and you learn your layer represent here sai x and x is fed through layer and there is hidden represent each in between right So try to learn all these paramet sai thi on here w three But at the begin of train everyth is kind of prone to shift around a lot So when you chang w on that kind of chang the entir distribut of your hidden represent after the fact So basic whatev you learn for w three is now alreadi almost obsolet becaus chang w on basic and w three wa kind of assum that it input would would remain the same becaus what you assum in machin learn your input distribut is kind of the same So why at the begin of train you see these kind of larg varianc And with batch norm thi tend to go awai So pretti cool Thei also kind of show thei mainli show that thei can reach the same accuraci as other as other train method but with much much fewer step and thei can go much higher learn rate than other So becaus becaus of that so pretti cool I encourag you to you to check out the rest of the paper us batch norm in your network sometim work sometim work strang enough But 
GPIO Basics with LED light - Raspberry Pi and Python tutorials p.6
go on everybodi and welcom to anoth Raspberri Pi tutori In thi tutori what gonna be do is introduc the GPIO pin on the Raspberri Pi how to us them and all that So what we have here is on breadboard two male to femal jumper wire on resistor the resistor should be somewher between like ohm and then an LED light thi yellow it realli matter what color you choos though And basic what the GPIO pin do for us is thei allow us GPIO stand for gener purpos input output and it allow us to either send out a signal either high or low so just a binari signal out or receiv a signal back in and all kind of wai that we can manipul that to have a lot more dynam to it Now the GPIO pin ar label for us and gonna pull up thi imag from Adafruit and these ar the GPIO pin abov that kind of grai line the older pi and if you have on of the more later version have all pin and you can go all the wai down there toward the bottom of that imag Now the wai these ar orient if we go back to here as if the same wai thi pie is orient in thi imag or thi video right here so that translat directli to these pin here so the top leftmost pin is volt and then the two top rightmost pin ar your volt output Now what gonna do is gonna rig up a simpl LED light that will just turn on when we ask it to via our program To do that and to creat the circuit go to be us thi breadboard here so thi is breadboard the wai that it work is basic all on these edg here sometim have like two row and breadboard will look a littl differ but gener gonna look someth like thi and these ar connect vertic basic so by column I suppos you could sai and then got these middl bit each of these ar connect by row so for exampl sai thi thi spot here is connect to thi on is connect to thi on and so on but these ar not directli connect to these connect them yourselv and then like thi on thi closest on I suppos thi on is not connect to thi on or thi on but thi on is connect to thi on so to help you understand that even further just kind of circl or squar some exampl of on that ar alreadi connect but of cours you can connect them yourself with jumper wire or us the resistor to do it and so on and then even the LED bulb will us So what we want to do is wire thi all togeth so a quick diagram of the actual wire that gonna do now thi is not a real wire diagram by ani mean but I know whenev I wa first start out the last thing I want to see wa a wire diagram I want someth a littl more obviou to me so basic what done here is thi green is the Raspberri Pi the call the fill in box by color or the actual jumper wire so black to black and so on and then the resistor is thi like blue blob and the LED right here and label and all that so just like super simpl so come back to our Raspberri Pi now go ahead and connect everyth and let me just pop up the diagram there so actual kind of a littl hard to see if your screen is not veri big at least that yellow so the yellow here is basic six pin down and then three pin down for the the ground so if you want you can take a screenshot of thi also you can go to the text base version tutori and save thi imag alwai good to have an extra on around so anywai yeah ground here the third pin down on the right and then also be us GPIO pin also just for the record these ar the Broadcom name a littl unfortun the Raspberri Pi ha like the board name for the pin and just like the physic locat basic of the pin you can us the board name pin or the Broadcom name and the Broadcom the processor that us you know the actual chip how it recogn those pin the name of that pin accord to the chip so gonna us the Broadcom name okai so so go ahead and wire thi up just go to connect here thi thi blue cabl will be my ground so go ahead and connect everyth up plug in so it realli matter the order that we do thi but yeah so ground here and then the femal end for the actual LED here and then ground will connect right here and then us the resistor here and there and then next to the resistor will be the LED bulb and now on thing to take note of see if you can see that you notic that on end longer than the other not all LED bulb ar gonna have that but when you do have that the posit end is the longer end so sinc thi is the ground and we actual want the neg end to be on that side so go ahead and plug that in if for whatev reason your circuit work flip them around you probabl just made a simpl mistak otherwis the bulb could be burnt out all kind of reason why it work or your circuit bad or whatev okai onc done that now we can just plug in for the actual resistor kind of touch we want them to be touch you want metal to metal there now in realiti actual trim that resistor down so it stick up so much but yeah make sure the resistor directli touch sai the posit end of the the GPIO cord anywai or the LED rather okai so got everyth hook up and again accord to thi diagram if you need more pictur got some better pictur of the actual physic wire of thi thing in the text-bas version so if still confus or whatev definit check those out but yeah so now go ahead gonna plug in the Raspberri Pi gonna fire it up and then gonna get to the actual code to make thi light turn on and off right with the Raspberri Pi all connect turn on in the termin via SSH go ahead and chang directori to the desktop and also just make sure got RPI GPIO chanc ar you alreadi have it but just make sure so sudo apt-get instal Python dash RPI dot GPIO and we alreadi have it so no problem so now gonna go ahead and do is gonna nano LED exampl dot PI and now gonna go ahead new import RPI dot all cap GPIO now mind the cap capit R capit P lowercas I dot all cap GPIO import that as GPIO then gonna also import time and then now gonna do is GPIO dot set mode GPIO GPIO dot BCM so BCM is short for broad com the other option is if you want board name but gonna us the BCM broad com name becaus I just think it make more sens and we just want to make sure us the same name as as we think we ar anywai you want to mess up the pin so now done that readi to set up the pin so like like I wa sai befor our pin that we connect to is pin for let me pull it up here so we connect that third from the top right ground you could have connect it to ani of the ground not a big deal which on but our actual pin gonna send an output signal to that LED bulb to actual turn it on is go to be GPIO pin number again you could have plug it into ani of the other GPIO port not essenti that you do the exact same port or pin rather as I do anywai what gonna do now is GPIO dot setup and gonna set up pin number and gonna call that a GPIO dot out it can also take an input so it could be GPIO dot in now gonna do is GPIO all cap GPIO dot output and gonna output to pin and gonna output a high signal it could be GPIO dot high or low those ar the two signal that you can output from your GPIO so gonna do high and then go ahead and time dot sleep do five second and then do GPIO dot output again to pin GPIO dot low so a low signal and then when all done go ahead and run GPIO dot cleanup and that should be everyth that we need so go ahead and exit that save ye cool go ahead and run Python LED exampl dot PI and let me just pull my screen over here just so I can make sure that show up on camera no problem go ahead and run that hard to tell with that yellow light but it is on and I think we us five second yeah and now off so hopefulli your turn on and off mind there okai so just like a realli quick basic exampl of us the GPIO on wai to send out a valu now what gonna do is us that distanc sensor to actual input bring back in some data from our from a sensor so what gonna be do in the next tutori a littl slightli more complex setup but not too bad so what gonna be do the next tutori if you have question comment concern whatev up to thi point feel free to leav them 
How far can we scale up? Deep Learning&#39;s Diminishing Returns (Article Review)
Hi there I saw thi articl in IEEE spectrum call deep learn diminish return the cost of improv is becom unsustain Thi is by Niels Thompson Christian Greenwald Qihong Li and Gabriel F Manso And I thought it wa an interest read becaus it talk about the comput limit that reach with deep learn todai And I have it over here in annotat form though it might not look as pretti I think the articl it lead up to the point where it show just how much comput will be need to make further improv in deep learn and what the consequ of that might be and some of the wai that peopl ar try to get around it Now I agre with everyth the articl sai but I think a a pretti neat read pretti short So I thought we can talk about it a littl bit So the articl start out with essenti prais deep learn for achiev so mani thing for exampl translat between languag predict how protein fold and mani other thing plai game as complex as go thei sai it ha risen rel recent but it ha a long histori Thei mention And Frank Rosenblatt at Cornel thei design the first artifici neural network thei sai ambit outpac the capabl of hi era and he knew it Apparent he said as the number of connect in the network increas the burden of a convent digit comput soon becom excess So why ar deep neural network work Becaus of cours comput have increas in power massiv just for comput power there ha been whatev a million fold increas accord to law And usual just measur in someth like CPU instruct And now we went even beyond that build special purpos hardwar such as GPU which actual special purpos for thi but also TPU So thei sai these more power comput have made it possibl to construct network with vastli more connect and neuron and henc greater abil to model complex phenomena And of cours these ar the deep neural network that power most of advanc in AI Thei draw a comparison right here thei sai like Rosenblatt befor them deep learn research ar near the frontier of what their tool can achiev essenti claim that we ar in a similar situat todai we have the model that can achiev thing And we know pretti much that scale them up can increas perform Howev kind of at the limit of how much we can scale For exampl I report on thi that Sam Altman appar said will not be much bigger than it will be train more effici will have some smart in it on how process it will us more comput but it will not necessarili be that much bigger in scale So the first thing the articl touch about deep learn is the fact that deep network ar over parameter For exampl the noisi student model ha some million paramet yet is train on onli million label imag which is the ImageNet data set Now of cours the noisi student model if I understand correctli also mai leverag unlabel data but grant neural network ar massiv over parameter thei have more paramet than data point avail Therefor thei should horribl overfit but thei Thei sai classic thi would lead to overfit where the model not onli learn gener trend but also the random vagari of the data wa train on deep learn avoid thi trap by initi the paramet randomli and then iter adjust set of them to better fit the data us a method call stochast gradient descent Surprisingli thi procedur ha been proven to ensur that the learn model gener as well Now pretti sure that we ar not yet sure why exactli deep network overfit or why thei gener as thei get over parameter I know there ar some proof around SGD and so on But these proof usual requir assumpt that just make them complet lose touch to realiti But the core messag is true deep network ar over parameter and that is probabl on of the reason why thei work so well And be over parameter thei ar quit flexibl thei sai at the good new is that deep learn provid enorm flexibl The bad new is that thi flexibl come at an enorm comput cost Thi unfortun realiti ha two part Thei sai the first part is true of all statist model to improv perform by factor of K at least K squar more data point must be us to train the model Doe thi realli hold for all statist model Is thi from the same theori that sai the statist model should overfit when over parameter not sure The second part thei sai of the comput cost come explicitli from over parameter Once account for thi yield a total comput cost for improv of at least K to the fourth power mean for a tenfold improv you would need to increas the comput by Now regardless of whether you think the theoret analysi is actual accur here again thi is from the same area that sai these model should overfit horribl it matter becaus these peopl have actual collect data And thei sai theori tell us that comput need to scale with at least the fourth power of the improv in perform In practic the actual requir have scale with at least the ninth power So when you actual measur how much peopl need to scale comput in order to achiev a given perform then actual much wors than the theori predict In fact thei have these neat graph right here So on the left you can see the percent error I believ thi is the ImageNet classif data set And on thi axi you can see the time Now here you can see that over time as time progress the error ha come down and down and down again as new state of the art model were propos ever sinc the success of Alex net And if you extrapol that you can pretti clearli see that around we should be at approxim of error See I thought had to actual do someth to reach a new state of the art on ImageNet But as it turn out we just need to sit here and wait until Okai joke asid thei overlai thi graph with anoth graph right here And that is the comparison of again percent error on the y axi But now not the year in which the achiev wa made But it is number of comput in billion of flop And notic the log scale down here Now I have to sai thi graph right here make it pretti clear that there might be someth like a relationship even mayb a linear relationship that you can extrapol right here not so sure like these model ar up here and then goe like here and then it goe here and then it goe here And then it goe over here to And realli without that you probabl have a line that goe someth like thi Now in ani case if thei do actual the line that do then you can see that if you extrapol the same thing to thi error rate you do end up at someth like to the flop And thei also compar thi to the equival carbon dioxid emiss For exampl right now we ar somewher between the gener by the averag US resid in on year and the gener by the averag US resid in a lifetim the current model somewher in between to train them onc if you actual extrapol thi to the error rate to the to the flop then it becom suddenli gener by New York Citi in on month So the entir citi of New York Citi for on month is the same as GPU go brrrr to train ImageNet Now that is pretti shock I have to sai you know it check out thei have done the research thei extrapol correctli here and thei come to thi conclus the equival sure thei ar measur correctli and so on I do have sever problem with thi though The first on I alreadi said the zigzag in thi graph right here realli suggest that you can simpli extrapol over these advanc Also the point seem to be quit out there So if there wa ani architectur search involv if there wa ani giant free train involv or anyth like thi sure like that that add to the emiss but it sai that you cannot achiev the same thing with someth els So whether the slope of the line is realli the black on right here or more like the blue on I drew it make quit a bit of a differ actual make a exponenti differ So a bit doubt that you can realli pinpoint thi error point to five year in advanc Okai now so three year but still and speak of equival not all energi is equal For exampl Googl pride itself in be zero emiss Therefor if Googl train a model there is no equival presum Now I think carbon neutral and zero emiss and word like thi ar sometim a bit of a scam but still not all energi is equal And especi these larg compani thei can distribut their workload across the planet to where the energi is us most effici And lastli and thi I think should realli the main point here is that we have made advanc None of these achiev here that made over the past year ar onli scale up the scale up alwai came with some sort of invent that made it more effici or more viabl to scale up residu network all of a sudden could scale to mani mani more layer becaus of the invent of the residu connect or the addit depend on who you ask So the residu network becam bigger and deeper without have to wast more comput In fact thei had less paramet than mani equival model of the time So I think we should neglect the invent we make along the wai in order to scale up Now of cours peopl ar alwai go to put in whatev flop thei have in order to achiev the best possibl number But I think for most of these advanc it wa realli new invent that trigger the usag of these flop rather than the other wai around And the author of these articl actual agre a littl bit thei sai is it realli reason to extrapol like thi and extrapol thi wai would be unreason if we assum that research would follow thi trajectori all the wai to such an extrem outcom We face with skyrocket cost research will either have to come up with more effici wai to solv these problem or thei will abandon work on these problem and progress will languish which is true So rather than be a warn cry about go to wast an entir emiss for a month for on model more of a warn against go to have to come up with new method and differ wai of train these model And we reli on scale to bring us advanc Thei also give some monei number right here Thei said for exampl DeepMind trade system to plai go it wa about million in cost when thei train AlphaStar thei purposefulli try multipl wai of architect an import compon becaus the train cost would have been too high In thei made a mistak but thei fix it due to the cost of train it feasibl to retrain the model and so on And also mention that cost about million to train Now ye of cours research that train these giant model come with substanti cost So you have to think twice if you realli want to do your grid search and whatnot So the experiment methodolog ha becom a bit differ But also you have to keep in mind these big number million million and so on First of all thi realli that much in comparison to what the peopl cost that work on the model And second of all thi is almost necessari All of the model that we see todai have cost substanti more in the past to train but someon had to do it first I can onli train BERT todai becaus Googl ha invest ginorm amount of resourc try out how to train it train the first on at consider cost And onli after that have other peopl jump on price have come down train got more effici And now I can do it from the comfort of my home essenti on a collab or on my home GPU And thi the case with all invent somehow at first just a few realli expens becaus custom becaus we figur it all out yet And then over time cost will come down effici will go up and the easi is just much better So rather than sai Oh wow deep mind spend million Oh no like cool You know sinc do thi two three four year I will be abl to do so for simpli million and pai you know so the articl give some solut to that differ avenu though thei ar mostli a littl bit pessimist about most of them So first of all thei said you can us specif processor design special for deep learn Now the newest gener of GPU ar actual a littl bit tune to deep learn but there ar also tensor process unit And there ar a number of other hardwar vendor that try to get into the space of specif build chip for deep learn What thei critic here is the fact that thi hardwar ha to do trade off thei have to increas special for gener And also with special you face diminish return And of cours the more special you ar the less you can invent new thing becaus essenti lock into what the hardwar can do Thei also discuss train network that ar smaller but thei critic that often thi increas the train cost becaus you essenti train a big network and then you train again to make it smaller to distil it And also not the solut to reduc train cost But it might be a good solut if a model need to be train onc and then larg run in infer mode such as Thei also discuss meta learn where you essenti train a good initi for a lot of problem And then you transfer that initi solut to new problem So if you have a good meta learner thei will be at an excel start point for solv new problem therefor reduc the train cost in each of these new problem But thei also mention that and I agre meta learn is yet at the stage where it realli work The train you put into the initi meta learner often pai off to new problem Ye it work in paper but in paper you alreadi know which other problem go to measur it on So thei sai even small differ between the origin data and where you want to us it can sever degrad perform Now thei also mention thi paper right here Benjamin Recht of the Univers of California Berkelei and other have made thi point even more starkli show that even with novel data set purpos construct to mimic the origin train data perform drop by more than Now I want to highlight thi a littl bit becaus thi talk about a paper call do ImageNet classifi gener to ImageNet Thi is also usual call ImageNet Becaus what these author did is thei tri to follow the protocol of the origin ImageNet data collect as close as possibl and come up with a new test set the so call ImageNet not a train set is just the test set And thei show pretti convincingli that for ani classifi that perform in ani wai on ImageNet it perform on ImageNet will be someth like point lower a fairli straight line So thi is what the articl talk about Howev the articl talk about thi paper right here call Identifi Statist Bia in Data Set Replic by MIT and UC Berkelei which show pretti convincingli that there is in fact a differ between the data collect mechan of ImageNet and It is a subtl differ but there is a differ nonetheless That differ make it such that there is a signific differ in what kind of imag ar chosen for the two data set And when you correct for that differ then thi drop in accuraci for ImageNet almost entir vanish Now okai the articl is right in first instanc there is a small differ between the origin data and the new data And that sever degrad perform But thi particular differ in perform is due to the new data set have a differ methodolog and that directli make the sampl harder not like the sampl ar differ in some sort of a there ar differ kind of imag is that veri directli becaus of how thei collect them thei ar more difficult to classifi the same data but more difficult So we be surpris that perform drop by In thi particular instanc I just thought it wa interest to mention sinc the articl specif focus on thi paper right here and I think thi paper is a good exampl of what try to sai Okai so the conclus to all of thi here is the final recommend that the articl make to evad the comput limit of deep learn would be to move to other perhap as yet undiscov or underappreci type of machin learn And of cours what thei mean is that thei want to bring the insight of expert which can be much more computation effici and that we should mayb look at thing like neuro symbol method and other techniqu to combin the power of expert knowledg and reason with the flexibl often found in neural network Now why doe everi discuss about the scale of deep learn alwai end with well we should us more expert system and reason and logic and the neural network understand anyth Now grant it is okai to suggest thi probabl a good wai forward But as of yet as of now the neuro symbol system ar actual just the expert system as well thei ar so so not good And of cours the case with ani young research topic But just becaus someth is computation effici it mean that we should switch to that becaus of it Now be super duper happi if symbolic make a comeback if we could somehow combin algorithm and deep learn if we could combin reason and knowledg base and input from domain expert and all of thi But as of todai that is not realli a benefit more like a substitut So you can make machin learn more effici by input lot and lot of prior from domain expert complet cool But what seen over and over and over again is that as soon as you give the ML system enough data it start to outperform these expert And I think what like to see from a neuro symbol system or anyth like thi is that in fact it doe outperform even the most data hungri machin learn method that the symbolic is not just a substitut for more data but an actual improv over ani data that I could find And just someth that I person seen you might disagre but I seen a convinc argument yet that that is the case for ani of the symbol system we have todai comput effici alon is simpli not enough But hei tell me what you think What do you think about thi articl Do you agre with them Do you not agre with them link the 
Line Finding with Hough Lines - Python plays Grand Theft Auto 5 p.5
go on everybodi welcom to part of our Python plai Grand Theft Auto tutori video seri thing In thi video what go to be do is build on the last video where we at least got the region of interest Took a littl longer than I wa hope but got it And now what go to do is us the hue line algorithm to find the line at least the major line in our imag data So go to keep the ROI becaus go to find line after ROI becaus just wast process to find line outsid of the region that actual interest in So the next thing go to do here is I guess do it in process imag So after done the region of interest go to have basic go to do the hue line So go to sai line equal and sorri if not hue line by the wai if like Hoff line or someth I know Someon feel free to correct me below I alwai mispronounc all these name Anywai p and forget the p there Now we feed through the process imag which need to be edg You would never I mean you probabl could I know what the result would be it probabl be good though You want to feed through to do the hue line again sorri if mispronounc If you want to do those you want to make sure feed through some sort of edg detect which in our case is the canni edg to the hue line or Hoff line or whatev So process imag oh I am still on screen surpris thi is such a mess Let me at least pull up to here so I make the mistak and not show anyth So we feed through an imag imageri that ha edg just edg basic And then what go to pass is a bunch of variabl The is on go to pass numpypi divid by thi is your theta The threshold will be the I think is it min length Let me pull up the document link to the document in the text base version of the tutori It will also be in the GitHub and all that Oh perfect here we go Let me pull thi up so you can learn more about these thing if you want Yeah got minimum line length and then maximum line gap So minimum line length is like how long should thi line at minimum be right Like do we want to detect two pixel long line I think so So the minimum length and then sometim through alias or whatev a line gap So for exampl let me just pull up thi imag becaus realli clear So like obvious we probabl but if you look at thi like a gap between these detect line Well from the actual imag actual realli a line The problem is the edg detect algorithm detect that there wa an edg and then not an edg or someth But in realiti and just go to happen with the edg detect algorithm try to see if ani other I realli see on but kind of an exampl of what can happen So you want to allow some sort of gap The other thing happen let me print out the imag again is you can see look at all the alias happen here veri jaggedi So like for exampl these touch I get ani closer but these realli touch So that is like to be a problem veri shortli from now Okai so for the threshold What wa it Min line length go with pretti sure in pixel And then max line gap I know Thi will all probabl be tweak later on But I just want to get somewher So what thi doe is return an arrai of arrai that contain the line Which is kind of weird And show you what I mean But go to mess with us just like thi want the bracket around it go to return to us someth similar I know why it think that wai but it doe So go to return line base on the edg imag that you pass through it So all we have ar just coordin X Y X Y So and if you drew a straight line between those those ar the line it found Okai got to draw these line So rather than muddi up thi on go to creat a draw underscor line function It exist go to creat it But for now just go to sai what like to be abl to do is draw line where On the process imag and then with what The line that we just found Okai So now do the draw line function So it doe just somewher around here defin and actual draw line will wind up be a realli import function So just go to put it at the top so easier for us to find For now go to be super simpl Draw line It take an imag and it take some line that go to draw Then for line in line thi is go to return like you might think line would contain like thi someth like that Okai You might think the case not the case like thi and a bunch of those and not sure why But anywai so for line in line go to sai now chord for coordin equal line zero width becaus stupid Now we can refer the line themselv insid So then what we can do is Thi is a wai to just liter draw line on a imag And then you pass the coordin for the line So What is the color of that line just do And then the thick of the line sai three One would be on pixel three will be three pixel So it will be easier for us to see the actual line Now for coordin go to be chord zero on two three Cool So actual just go to draw it right on the imag And the wai that OpenCV work is it let you modifi the thing So we need to return it or anyth just kind of modifi it kind of strang but thi will actual draw the line on the thing We actual need to return an imag So actual total cool done Now draw line Cool Okai So I think about readi see what typo done todai So go ahead and run thi alreadi kind of made a pretti big mistak but get to it Okai So we can see at least a line there Thi alreadi broke No still go Interest But probabl go to break ani moment now Pleas work Did it break thi time Let me see here Yeah it broke Okai So on error with our logic here is here four line in line Well if it find ani line not go to work out So what we need to do is just throw thi into a try Accept Fight me Pass Okai Oh commit a horribl sin All right So it felt good though It felt so good All right so hopefulli you can see that in the video So wow that gui total just went around me What a prick Anywai so you can see like lot of littl line here but not quit what Dude the peopl of Grand Theft Auto ar pretti freak weird But not quit detect everyth we expect And again most like becaus of the alias go on there veri jaggedi So now what we want to do is let me close thi and let me bring thi back up So we can fix that with just blur it a littl bit So blur is just what it sound like You can take like a squar of pixel and just kind of blur it Like basic what do Just an algorithm that doe that for you So what do is add a Gaussian blur So you see that Okai so befor Okai let me think about thi Yeah we want to definit blur it after the edg not befor the edg becaus then the edg detect would just do a line again So after the edg like to blur it So process Oong equal We ar go to blur the process underscor imag We blur it with a I rememb if want squar bracket or brace Okai Googl it at the moment if not clear Let me just pull it over as look so you gui just bore I suppos do thi Gaussian Okai yeah So it want Okai All right do it Where am I Here we go So do the That sound like a grand plan And then what wa the last paramet there A zero What is a zero not quit sure what that zero is It would be nice if I care throw in the zero see what happen I just want some blur I need to get too fanci about it Done move thi over now run thi look good alreadi find line everywher Look oh so mani line Okai All right So now freez frame here Beauti So now if we were Can you see No you Oh ye you can see my mous Veri difficult though not veri easi Anywai now we have line And my think is us a littl bit of logic we could Well first of all we could chang the blur a littl bit or we could chang the paramet on the actual line function So we had the minimum distanc and then we had the max gap So what we could do is chang Chang hard becaus updat so it keep push me We could chang thi like min distanc to And then just rerun it Wow realli That make as big of a chang as I wa kind of expect Well some of these line ar like pixel or someth So do then And then we could probabl lower the gap Wow ar you kid me Are you What the hell Thi is not even listen to me Those pixel long full of it just full of it Mayb account for the gap and not draw on the No becaus return the line chang the gap to pixel I think in pixel I swear chang Oh my gosh I wonder if do someth wrong Somebodi can Somebodi feel free to correct my code Thi just feel wrong Mayb What if we did So just go to take a few thing befor I just quit life a littl better I guess but not that much better veri frustrat Anywai go to return back to do What wa the initi set do like That should be probabl good enough Despit thi be rel My idea is to We can tweak thi a littl bit if we need to but my idea is to If we just take some of the longest line or even better we could take a lot of littl thing we can do to probabl improv thi What ar the longest line What ar the best-ish look line Where ar most of the line take place Someth like that where You could take slope and bia You could take slope and bia What line have veri similar slope and bias For exampl a bunch of line right here that ar pretti close to each other Then here these ar quit a few line here Unfortun a lot out here that not realli too happi about definit some tweak go to need to go on go to chang to Try that again Anywai definit some tweak need to go on but definit also some logic that we could throw into here to make thi be a littl more accur Anywai go to go ahead and cut thi on off here I think mayb in the next video try to get us to come up with some wai of determin the lane Then probabl probabl try to figur out from there whether or not we should move to come up with more rule or if we could try to make an AI at that point I know Anywai question comment suggest whatev leav them below Again all the code will be on GitHub If you have idea for improv share them with us Befor we go let me just Let me stop thi and then just show you the end Like I said everyth will be up on GitHub but basic the end hue line wa for the threshold We can modifi a few other thing but first just try to come up with some sort of logic that will pick a lane and then if have a problem at that point we can come back to thi I also I rememb how fast those loop were see what our frame ar So pai a decent penalti on frame rate alreadi About What ar we get like five frame a second now So have to see on that on how we can hopefulli somehow improv that try to think have to look through thi and see if I can come up with a better wai to make thi more effici First do the lane and then from there see what we can do about dumb thi down But that might be anoth reason why we might want to go the AI rout where as strang as it might be we can run that on the GPU It might be actual faster that wai But anywai a lot of stuff to think about go to cut it here In the next tutori we will do our best to try to find some lane Probabl what I come up with be the best and hopefulli somebodi can come up with someth that is a littl more robust than what go to push out But see what happen 
Pickling and Scaling - Practical Machine Learning Tutorial with Python p.6
What is go on everybodi and welcom to the sixth machin learn tutori in thi tutori go to be talk about pickl a littl bit about scale And then go to move on into dive into the inner work of linear regress and of cours the other algorithm so Pickl realli realli have anyth to do with regress Just simpli is a good Good thing to have it at your dispos so you can save yourself a lot of time So first of all what is pickl pickl is just serial of ani python object So thi could be a dictionari or in our case it could be a classifi or a whole host of other thing So first of all what go to do is go to import not in cap go to import pickl Okai also I notic Yeah here for some reason I wa like doubl defin Why is the label thing my mortal is show anywai Um yeah just get rid of that if you did that hopefulli hopefulli nobodi follow me into that mistak but anywai Yeah so that now What go to do is the wai the pickl work like a file right you open it you save it right and then when you want to us it you open it You read it so go to come down basic You know at what point would you want to save the classifi right so like think logic when should we save the classifi Well should we just should we save it here Probabl not right I mean you could you could save that classifi but an untrain classifi So you probabl want to save it here right so like what is the purpos of save a classifi to avoid do the train step Becaus a veri tediou step right that go to take the most time in our case It realli take that long becaus we have that much data and we were thread it But you can imagin if you had gigabyt and gigabyt or even terabyt of data that you were do thi on Everi time you want to make a predict you would not want to Have to retrain an algorithm so instead you can save it now of cours if you save it You know you might want to retrain it like onc a month or someth like thi But you have to retrain it everi time you want to us it so What go to sai here is go to sai with open and go to sai linear regress dot pickl as WB As f so go to open thi file with the intent to write and go to just us temporari variabl as f And then go to sai pickl dot dump What ar we dump go to dump that train classifi so CLF Where ar we dump it f so that dump the classifi and then to us the classifi all you would actual do is you would sai like someth like pickl in equal open and Then we would open thi file Open with RB and Then we would sai CLF equal pickl Pickl dot load and just go to load pickl in so whoop hello What I do here you go copi thi past okai so now go ahead and just run thi and see how we do Okai so everyth work at thi point and whether or not you recogn the fact We have actual renam classifi or redefin classifi here So now what would happen if we did thi Right now there is no definit of classifi not save it as a pickl simpli read a pickl so if I refresh thi to reload thi what we get is There we go so you get thi inform here and the pickl is save in the directori work in so that would be if I could find it here open it up right your pickl data so in our case the pickl data or the classifi is actual a realli realli small classifi not a big model But anywai we save ourselv the time of actual train that classifi Also I said it in the introduct video but sai it again Rememb that we live in a time where you can spin up a server For a veri short amount of time and you could do that so if you have like a slower comput Mayb your onli comput is a rel slow laptop got a I know on of these littl netbook or someth If you ar on of those peopl you can spin up a gPU cluster You can just spin up a regular server You can spend a veri power comput and basic rent it for a few dollar an hour on lino digit ocean or Amazon web servic and If you do that what you can do is gener what I do if go to us a big server Well pretti much all these host work the same wai you take your data You put it on their server and like set everyth up so transfer all your data there set up all the code that you want to run and all that and while do that us like the smallest Version to have so pai like a half a penni an hour to rent that version of a server and onc readi scale that server up and Then pai mayb a few dollar an hour Mayb even ten dollar an hour if crazi Scale it up run your oper take your classifi save it to a pickl take your classifi Scale back down the server destroi the server whatev and done so Just a coupl of quick you know pointer becaus not realli go to be talk much about that For scale for a while but these algorithm actual you can scale linear regress veri well so Just keep that in mind so anywai realli all I have to sai about pickl and scale and now what go to actual be do is Write our own linear regress algorithm To learn how that actual work and all that so veri excit stuff if you have question comment concern Whatev leav them below otherwis as alwai Thank for watch Thank for all the support subscript and until next time 
Linformer: Self-Attention with Linear Complexity (Paper Explained)
Hi there todai go to look at Linform self attent with linear complex by Sinan Wang Belinda Li Madian Kapsa Han Feng and Hao Ma of Facebook AI So on a high level thi paper observ that often the wai we build transform the self attent matrix is low rank and can be approxim by first project the signal to a lower dimension space and then perform these inner product that ar respons for attent in there And therebi you save a lot of the complex of multipli full sequenc length full sequenc length by full sequenc length matric but instead do these oper in the lower dimension space And thei achiev a linear scale of the transform attent And figur out how that is As alwai if you like content like thi consid subscrib share like and comment if you feel like it Okai dive in Thei sai larg transform model have shown extraordinari success in achiev state of the art result in mani natur languag process applic Okai so these if you know what a transform model is you can watch my video on the paper attent is all you need That wa sort of the begin of these transform And it introduc the attent mechan that go to look at todai If you know what an attent mechan is not go to have a fun time in thi paper Thei sai howev train and deploi these model can be prohibit costli for long sequenc as the standard self attent mechan of the transform us n squar time and space with respect to the sequenc length Now why is that So realli shortli to recap recap thi the attent mechan the thi attent these transform thei transform for basic sai thei transform on sequenc into anoth So here we have five token and the next layer will output five token okai for five token in five token out And the question is how do you rout inform between these five token from the first layer to produc the next layer In a feed forward network you would simpli connect everyth to everyth and sort of learn the weight of these of these connect not what we do here In a convolut network you would simpli connect each node to it immedi neighbor like thi But thi is also not what we do here What we do here is we rout the inform accord to the inform itself So accord to the incom inform right here we rout the inform that goe out And we do that by express kei queri and kei So these incom inform is transform first of all into what ar call kei Now kei ar simpli vector So each node is go to expos a vector right here And each node in the higher layer Now these ar produc by the same from the same inform down here but go to draw it conceptu on the higher layer So each node here is go to expos a queri which is sort of like call the queri is call for what kind of inform do you want from the lower layer And the kei is sort of expos what type of inform thi node contain right now Now the inform is simpli rout by look at the inner product of these of the kei and the queri So thi inform right here would probabl be rout to thi node right here wherea thi on would probabl be rout here thi on would be rout here In fact thi is a soft assign So not like a hard rout a soft rout everyth is rout to everyth with differ weight but the major goe to the place where the inner product is high And thi on is again rout here So you can see thi is the attent mechan In order to do thi we need to comput the inner product of everi singl on of these queri with everi singl on of these kei Okai and thi if our sequenc length here is of length n is go to requir n squar oper Okai Now here is anoth paramet we need to pai attent These vector here thei have a certain dimens and the certain dimens go to call d the inner the embed dimens of the vector Now in modern transform you can think of n as someth like mayb token go into a transform like thi And the hidden dimens here also is in the same order of magnitud So you can also imagin thi to be someth like Now if you think of these matric if you multipli the kei by the queri howev you want to do it like thi then you have the kei ar n by D and the queri ar D by n Okai Now sinc n and D in thi case ar the same dimens thi matrix is of rank of rank have to be but a pretti good bet that of rank Mayb approxim lower rank but now thi actual the modern wai of transform as such becaus usual what we have is multi head attent which mean that go to split thi inner dimens right here go to split these vector into mani mani lower dimension vector and then have attent mechan on these lower dimension vector And such that you onli have on attent mechan you have multipl attent mechan So you can rout differ kind of inform with these multipl attent head Now sometim you would split thi you could split thi in a modern transform up to like differ head But here go to sai go to split thi into four sub vector each of dimens Okai so go to split thi up And now if thi in thi product here is onli comput on these lower dimension vector So all of a sudden you no longer have n by D but you have like n by D over four And now thi is still but thi now is So the rank of thi matrix is go to be Mind still the thing that come out is still a by matrix but it is of rank And that mean even though thi matrix contain vector that ar of size thei could be thei could be repres accur by a matrix just dimens Okai so these these dimens actual onli contain inform that is dimension in natur just distribut over dimens but most of these ar redund So in fact in these modern transform these thing here thi matrix here is low rank And therefor what thi paper sort of exploit we could we could approxim thi by dimens Okai thi is our start point Thei go on and thei sai in thi paper we demonstr that the self attent mechan can be approxim by a low rank matrix We further exploit thi find to propos a new self attent mechan which reduc the overal self attent complex from n squar to n in both time and space The result linear transform the linform perform on par with standard transform model while be much more memori and time effici Alright so dive into their thing Thi is how thei formul the attent mechan So right here the attent ha queri and kei as you can see here Now these w matric you can larg ignor the W simpli map the queri to so thi is these ar simpli D by D matric that ar a linear transform of the queri you can sort of overlook them for the argument in thi paper So these ar the kei and the queri we talk about the valu here thi is the actual inform be rout So what we want to do is we want to comput thi product between queri and kei right here and scale it appropri But ultim thi is thi product then run thi through a softmax oper That mean we we normal it such that it sum to on the distribut sum to on And then we want to rout thi inform accord to that distribut Okai So how thei formul an attent mechan Now notic someth Thi thing in here is what thei call the matrix A And thi is what demonstr to be low rank Now the actual thing that you would need to be low rank for their paper to hold is the matrix P which is differ becaus thi is after the softmax right So if the matrix P is low rank then you have a legitim claim of approxim thi rout via a low rank matrix Howev if P is not low rank you Okai All right Now the first thing go to show is that thi is in fact a low rank So self attent is low rank And for that thei make an empir investig into Roberta So Roberta is a model base on BERT And I have made video of both BERT and Roberta I believ if sorri if you want to go look those up But it is on of these transform model And thei take two data set wiki and IMDB and thei run them through thi model And thei look at thi P matrix So thei look at how thi thi inform rout matrix is built And then thei calcul the eigenvalu of that So you calcul the eigenvalu And by look at the eigenvalu you can look at the rank of a matrix broadli speak So if you list the eigenvalu in order of their size then a matrix that is sort of high dimension ha a high rank would have sort of a slope like thi And that mean as you go as you go to the next and next and next eigenvalu thei drop Like if you order a set of uniformli distribut number if you order them then it would look like thi right So there is no particular dimens better than ani or ha much more inform than ani other Howev if the matrix is approxim low rank you would look someth like thi And that would mean that most of the inform is concentr in veri few dimens And those ar the on with veri high eigenvalu And most of the dimens have no inform The thing you see here is simpli the cumul sum of these thing So if you calcul the cumul sum of thi get that over here So if thi is veri high rank you would expect a curv that goe like thi sort of slant but not veri if thi is veri low rank you would expect a curv that goe veri much into the corner right here And thei show that the gener shape here is such that there is thi kind of a kink to it as you can see here Now also notic that the axi here start at So actual thi come from down here somewher and goe up and then goe like thi So thei have a I feel thei have a legitim claim here that these matric ar approxim low rank And here thei look at I actual know at which layer thi is or if thi is in all of the layer overal or someth like thi But thei look at how thi develop insid the layer So thei look at the alwai the eigenvalu And thei discov that as thei go deeper and deeper into the network thi cumul eigenvalu is higher and higher That mean that network put more and more inform into fewer and fewer dimens in thi rout as you go up the layer So it get more and more skew As you go up the layer it get more and more into thi corner right here So their claim appear to be more and more true Now I have sort of thought about thi a littl and tri it out a bit myself And I invit you to just follow me here shortli So right here I have a matrix that is just a random Gaussian matrix of size by If we look at the eigen spectrum of that so I have thi function SVD it simpli give me the eigen spectrum of that then you can see that it sort of fall off uniformli And that will result in a in thi cumul sum of of pretti much flat curv or slowli ascend curv like thi Okai Now if we actual have a low rank matrix thi would look differ thi would have thi sort of typic kink in it And we can demonstr that by make a lower dimension matrix So just take just go by of thi lower dimension m And look at the mt Now thi onli goe to becaus we onli get back singular valu So make a lower dimension matrix actual by So if we do thi thi is sort of what do in the in thi Thi will construct a by matrix but that is onli of rank Right And you can see that at the singular or eigen valu thi snap right at the at the on So sort of like what thei what thei have Okai so seen the differ between a sai higher rank matrix and the low rank matrix in thi cumul sum plot Now I want to go back to the origin matrix right here Of cours the matric thei look at these rout matric not Gaussian not sort of distribut with mean zero and the nice varianc thei ar the result of a softmax oper And in particular that mean all posit And that mean that their mean is not zero So if you look at a data set and mean not zero and you calcul like the eigenvalu or in thi case the princip compon you will find that the first on will be veri strong becaus that must account for the fact that the mean is not at the center the first few will be like thi So it is sort of mayb we can replic thi right here So sai put m through first go with the absolut valu of m Okai not much of a chang okai not much of a chang but alreadi see that thi axi start at zero So go actual how do we do thi Xlim right Xlim zero none So okai So the first on you simpli have to imagin or I can do even someth someth more we can just put a zero in front here And that should do the trick No ye Oh X I meant Y god a dumb Nevermind thi will work as well So you alreadi get thi sort of kink And put it into the softmax So we put a softmax And that give you also thi kink Now you might think that wait thi is that thi kink look a lot smaller than the other kink So but if we simpli modifi modifi the standard deviat of thi random matrix And you can see that the spectrum immedi chang right Becaus of the interact now between the softmax and the standard deviat If I onli were to chang the standard deviat on the normal M matrix and we can actual try thi right here that do much that would still look pretti much the same just differ scale But in the interact with the softmax now thi chang the spectrum dramat And here as you know these these transform have alwai sort of like layer normal and so on So probabl the standard deviat if we if if these ar sort of Gaussian the standard deviat befor the softmax would be a lot smaller So go someth like thi So smaller than on And can we run thi pleas And you can see that thi kink immedi appear Now not not the same thing as thi other as thi here becaus thi is a lot smoother as you can see right here But still I feel that thi might not actual be a result of the you know the fact that thi is an attent mechan but it simpli might be the result of that you appli a softmax Now still that chang the fact that it is approxim a low rank matrix So everyth thei sai hold but yeah mayb mayb on should also look into why exactli that happen But in fact it is low rank okai it is approxim low rank demonstr thi And now thei go to their first first theori below we provid a theoret answer a theoret analysi of the abov spectrum result Okai so the theoret analysi theorem on is self attent is low rank And go to go to the second on is low rank And go to go through thi Just glanc at it for now Thei sai for ani of these queri kei valu and these matric which of cours you can ignor for now for ani column vector w of matrix v w and w here the inform that need to be rout There is a low rank matrix p tild So thi p tild here is go to be their low rank approxim of the P matrix you can see still n by n but go to be low rank In fact go to be of the order of the logarithm of the rank of the full matrix Or well the full matrix of the rank that the full matrix could have as we have alreadi seen the full matrix have full rank but yeah okai So if you us and thi is the type of guarante you get so what do we see here it basic mean that thi distanc here is smaller than thi And thi here thi is just the norm of on of these vector project time thi error coeffici epsilon So all it sai is that the distanc on the left is smaller than someth And occur with high probabl Okai so the entir guarante here the entir formula just basic mean that thi thing is small thi norm is small thi norm thi norm is the distanc between these two thing Now what ar these two thing thi is the inform that we want to rout And thi is the rout matrix And that simpli mean that if I rout my inform us the P tild thi approxim then I be too far awai as if I had rout my inform us the origin P matrix Okai it what the theorem sai The theorem sai if I rout my inform us thi approxim then I am not too far awai as it had I rout my inform us the origin rout matrix that I sai how go to construct thei simpli sai there exist a low rank matrix like thi And the proof of thi and sort of worth look at the proof of it it us the Johnson Lindenstrauss lemma thi thing here or the JL for short and go to get thi out of the JL Now the Johnson Lindenstrauss lemma in a classic sens sai someth like thi if I have data in a high dimension space here in a three dimension space okai I have data distribut and I us a certain kind of project matrix And there ar a number so the JL give condit on what these project can be But for exampl a randomli sampl matrix with zero mean Gaussian entri and on over k standard deviat where k is the dimens you project into can do the trick So if I project my data in a certain wai into a lower dimens here dimens two then the project data is relat to the origin data by the fact that the distanc between the point in the origin space will not be distort too much So the distanc between these point ar approxim preserv through thi project Okai so the the Johnson Lindenstrauss lemma Now notic here there is no refer to the fact that thi data is or low rank simpli high dimension data project to lower dimens and the distanc ar approxim preserv And thi theori here and look at it for a while now thei simpli defin okai thei defin thi P matrix as thi attent mechan And here you can see the A matrix discuss befor which is actual low rank but we know yet if the softmax is thei write it as thi form right here of the exponenti of each entri of A divid by thi diagon right here So in the softmax of cours you have the exponenti of each entri divid by the sum of the entri and thei write thi simpli as two matric But ultim thi is a matrix right here And all thei do is thei take thi P matrix and thei appli the Johnson Lindenstrauss lemma by have thi project matrix R and R is entri from thi Gaussian as I said so thi is the special type of project that the JL address And then it simpli sai if you pull if you thi here is go to be your P tild So if you project R in thi manner and obtain P tild and then you us P tild instead of P then thi thi is go to be veri close In fact you can reformul the JL into differ variant such that it give you thing like thi thing like sai that the distanc between thi project version and thi unproject version is go to be a constant smaller than a constant time the norm of the unproject version that is equival to sai that the distanc ar preserv Now you can see right here nowher in thi theorem is the fact that thi is self attent And nowher in the theorem appear the fact that thi inner matrix A is low rank or even that thi matrix A exist you can do thi with ani matrix P right the JL concern itself with the natur of thi matrix P it sai ani matrix ani sort of high dimension data you can project to low dimension data And thi hold if you choos the project correctli which thei do right here So to claim that thi theorem prove that self attent is low rank to me is a bit a bit of a statement that is not warrant Like thi here should read someth like the Johnson Linden Strauss lemma exist or someth like thi It not not sure like convinc me otherwis but yeah So thei go with thi So thei sai given the low rank properti of the context map matrix P now again I disagre that thi ha been shown except empir One straightforward idea is to us singular valu decomposit to approxim P with a low rank matrix P low as follow So what you could do is you could simpli learn these low rank matric and approxim P through it or you can decompos P as such and then have these easier inner product in dimens K But thei sai howev thi approach requir perform an SVD decomposit in each self attent matrix which add addit complex Therefor we propos anoth approach for low rank approxim that avoid thi ad complex Okai so thei now come up with their model and their model goe as follow So here on the left you see a classic attent mechan with their project built in what propos is thei sai project the matrix K us on of these random project And then thi attent rout if you rout if you now multipli so you multipli K and Q right here K time Q and then you put it into the softmax and then you us it to rout thi W So thei sai if we build in thi project matrix that will project K to a lower dimens and then we have as expens of inner product Now the import part to see here is that if you think of thi lower project the first thing you think is that you project thi inner thi hidden dimens D right to allow a dimens And not the case here you actual project the end So in in a conceptu framework so you can see right here forget about thi thi is thi W matrix In a conceptu framework you see here is thi n by D matrix which ar the kei So n is the sequenc length and D is the dimens And what you want to do is you want to project that by thi matrix which is K by n So you want to reduc the sequenc length You can see in thi matrix right here why that might work becaus n is much larger than D And that mean thi matrix can be at most rank D right So you should not lose too much you should sort of be abl to preserv the inform If you project thi n to a K where the K if the K is still larger than the D or approxim in the same order of magnitud you should be abl to preserv that inform if you do it in a smart wai So conceptu if we have our five token sequenc like here and the next layer produc five token again what we first do is we sai we know we know that the inform we want is not five dimension actual two dimension Becaus okai sai the thi inner dimens D is is two as well So we have two dimension vector each thing expos two dimension vector So we first project the sequenc of length five to a sequenc of length two And we simpli do that in a random manner So we have a random Gaussian matrix that assign weight to mix these five into these two And again becaus the JL work for ani sort of data but in my argument if you you know think that thi here is low rank of rank two then you lose too much inform by project it to a sequenc length two And now we do thi attent mechan So now we expos the kei And now we expos the queri up here And now you can see instead of rout five thing with five thing you onli have to rout five thing with two thing And so instead of have o n squar you now have o n k if k k is the number right here Okai so thi is the idea you project the sequenc length And it come from the fact that the sequenc length is much larger than the dimension And therefor you can sort of preserv the inform if you project in a smart wai Thei build thi in thi fashion right here So the attent mechan now befor we saw it wa between the queri and the kei right here thei built now thi project matrix here that project the kei into a lower dimension sequenc And the now such that thi will result in an n by k attent matrix we saw over here you need to rout n by n thing you need to rout n by k So thi thi rout tabl in here is now n by k Now the next layer as you can see here it actual need to produc a sequenc of length five again right So we alwai transform sequenc of length five into sequenc of length five But now we have we have thi n correspond to the sort of correspond to the next layer and thi k correspond to the down project sequenc of the last layer And in order for that to fit we of cours also need to down project the inform that rout So if we down project the rout tabl we also need to down project the inform that rout we do thi by a similar matrix F that is also sampl in thi wai in thi special wai And that give us a k by d So we have project the sequenc to size k And if we multipli these two thing again of cours get out an n by d matrix which is the signal for the next layer Okai so an n by d signal come in down here project down to k sequenc length and rout up again to n sequenc length And you have again an n by d matrix here Cool So how thei do it And thei build thi into the transform Now as I understand it these project matric again not learn thei ar do thei ar built up in thi JL conscrib wai thei ar not learn thei ar fix onc And then that at least how I understand it So there ar no more learnabl paramet Okai so here thei have a demonstr where thei up the sequenc length And you can see the batch size decreas but just to sort of keep the total amount of flop to be done the same you up the sequenc length and down the batch size As the sequenc length increas the standard transform requir in infer time goe up And thi here as you can see thi is not a linear scale a log scale log two So thi goe up with the sequenc length and it should go up quadrat right And you can also see that the linform keep fairli constant for the same k Now of cours as you increas the k of the linform the infer time will go up becaus now depend on n time k and not on n time n Okai So look a bit further of how you have to choos that k up here in the first theorem we there wa a alreadi a hint to it In the first theorem you had to choos k by five log n And thi is a problem So here you have log n that mean not so O of n k is equal to O of n log n Now not linear actual the same as the reform But thei want to get to a linear place And theorem two explain goe now to a linear show how you can make self attent linear Okai thei show again blah blah blah blah Now you have to choos k at the minimum of these two thing And you can see right here that on of them is independ of n So that mean as n grow of cours the minimum is no longer go to be thi here the minimum is actual go to be the thing on the left And that is depend on just D Okai so you have D log D in here And that make sens Becaus in the veri begin we said hei D is actual much smaller than n And that mean the inform that is contain in these matric is at most rank D So if we down project to k we should adjust k to what D is right if we adjust k to about the same thing as D guarante to not lose too much inform So now we choos k accord to D instead of accord to n and therefor the comput is linear in n And n time k is like n time D to log D So linear in k and linear in D How do we get there So the first thing thei do is thei make these sort of Johnson-Lindenstrauss statement again But now instead of the gener statement thei plug in their actual modifi attent mechan So here thei have a bound on the distanc between if I rout my thi is the inform to be rout right If I rout my inform us the origin softmax and thi in here is the matrix A if the origin attent mechan I be too far awai as if I were to rout my inform us thi modifi attent mechan Now the tricki part here mathemat I believ is that is exactli the softmax what what I allud to right So thi softmax is the tricki part becaus if thi a softmax so if the softmax here thi would simpli be a project down and a project up and the dilemma would almost appli as it is written right You have to actual do anyth But the question is if thi insid the softmax is low rank can you make a claim that the entir softmax then is also low rank And not entir clear becaus becaus oh ye done thi So you can see right here that the softmax we have actual done the softmax of a low rank matrix So we have alreadi seen the low rank matrix itself and how it immedi snap to the to the upper axi after Now if we do the same thing for the softmax of that and we probabl have to take awai some of these dimens the first few go with go to dimens and look from there Okai same thing Okai pretti good I did not expect that Hi there So thi is Yannick from the futur realiz been an idiot and how I construct these low rank matric right here by multipli MT by itself of cours a better wai to do it is to construct two independ dimension matric like these two sub slice of M right here and then multipli those togeth and look at the SVD And you as you can see right here So the softmax of thi is now not of thi super low rank anymor still low rank but not not veri not like hard low rank So if I just look at the matrix without the softmax then you can see it ha a veri peak that by at which give us the indic actual rank which we alreadi knew But if we now introduc the softmax then you can see that thi vanish and no longer dimension And onli approxim low rank as you can see Alright back to Yannick in the past who is wholli surpris that the two that if you multipli MT by itself that that will give you back the the exact same thing Alright so did we try thi befor Mayb we did Okai but the mathemat difficulti still remain and their main thing here is so thei have a first first version where thei pretti much plug it into the JL again and thei thei get out thi k is the k need to be by log n But thei sai thi result doe not util the low rank properti of matrix A And the result k ha a depend on sequenc like n And then in the appendix thei final go through the math to show that now if thei choos E and F like thi thei can actual pull out thi and show that the k is where we where you have it the k is independ of n like thi And I think the main the main step in thi proof is the step B here where thei sai us the fact that the exponenti function is Lipschitz continu in a compact region then we can choos a small enough delta such that the as you can see here thi now directli relat to thi project matrix within the exponenti function to the project matrix out of the exponenti function So you can basic sai that if I project first and then us the exponenti function not too differ than if I first us the exponenti function and then project Okai so the the sort of of of catch here Now thei onli do thi for the exponenti function not the actual softmax As you can see here throughout thei do it to the exponenti function and also here in their statement The softmax the exponenti function the softmax is the exponenti function divid by the exponenti function The exponenti function divid by the sum of the exponenti function but I believ that thi gener straightforwardli Alright so for given choic of delta and k thei have shown that the linform in fact can do in a linear fashion what a transform can do in a quadrat fashion And thei ar not too far off Okai their point right here Their result on these benchmark oh sorri first go to the perplex in languag model So thei show right here that thei pretti much can keep up with the standard transform as you can see here So with the standard transform thei can keep up here Now think that thi the the comput is n time k okai so someth like thi linform with k equal will onli so instead of n by n n time k it save you too much in that case But not too surpris that in fact you have the same perform becaus probabl the standard transform is distribut over more head than two So the inform necessarili ha a lower dimension to One thing I want to draw attent to though here is that you can see that here not realli done learn yet And as you can see the standard transform sort of surpass all of these model toward the end I wonder I wonder what happen I be surpris if thei end up sort of at the same place But I wonder if these diverg even more right here after that Thei also compar with a higher sequenc length And the standard transform outperform the linform But of cours the point here is that the linform is much much much faster and can keep up Now also the scale here of the perplex You see these ar percentag point in perplex but I actual tell if that matter or not I think I think the origin transform paper the perplex hover between like three point someth and five point someth So thi might actual be sort of signific differ And not sure Thei investig differ method of share these weight of these of these project And thei seem like thei find real differ but I want to go into that becaus thi video is alreadi realli long And then thei look at what happen if thei up the sequenc length that thei put into the linform And you can see that the linform can deal with higher sequenc length and arriv at the same perplex Though again I know how much how much differ that is and the scale here is larger than befor But yeah So how doe thi fare on these benchmark where you first train a transform with pre train with languag model and then you us it to do certain NLP task And here you can see that the linform is on par in some of these task with the origin transform But also you can see like a pattern where you can see pretti wild result in that you know sometim the the linform here will be better than thi but then also variant of the linform will be wors And even be wors than thi And sometim be better Sometim thi linform is good And sometim the origin model is the best So thi sort of point to you can make the gener claim that the linform destroi your gain but also not like a a better model simpli a faster model that in some task can keep up with the origin model And thei show that of cours thi is the real deal here that as you go up in length the perform gain and also sorri thi thi wai the perform gain and the memori gain that you get by the linform ar dramat Of cours the longer and you go into the lower dimens you project the more these gain ar but of cours the more perform go to lose potenti Hello again Janek from the futur just want to draw your attent on thi beauti broader impact statement in thi paper sai our work focus on make transform more effici everyth cool potenti posit inspect impact of effici transform pretti cool It also ha potenti impact on train transform on imag sinc we can support veri long sequenc veri cool Furthermor there ar posit environment benefit Veri cool I mean these ar all veri cool thing Thei sai as such we see no immedi neg ethic or societ impact of our work beyond what appli to the core build block of deep learn Do better Now thi thi I honestli I agre with them right I complet agre with them that thi is sort of a good thing You might trade off you know some accuraci you might make some approxim but get a much faster model And thi model as ani model can be us you know for thing and that thei now have to pull out of their out of their butt some wai in in over five step of intermedi layer Thi could be us for bad it just seem ridicul So good on them for defi the pleas also think about neg impact right here All right back to back back to past Janek All right thi wa the linform paper I hope thi somewhat make sens made sens to you I had to read it multipl time for it to make sens to me But ultim all about the fact that you have these multipl head And therefor your inform is probabl lower dimension and you can abus that and to just calcul in thi lower dimens 
BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding&amp;Generation
Hei all thi is a comprehens paper review of the paper on Blip Thi is a model and a techniqu for bootstrap own data set in vision and languag pre train which is pretti cool So the video is a comprehens review dive into the paper see what the paper is about explain you in it And by the end of the video you should have a good understand of in the paper in the next video which go to releas tomorrow go to be an interview with the author of the paper So also be sure to check that out becaus that answer a few veri veri interest question that I had while read the paper itself So I wish you a lot of fun Let me know what you think in the comment and see you around Bye bye Hei there thi video is sponsor by Zeta Alpha which is a new neural discoveri and recommend engin for paper Ye for scientif paper for trend in research and code in AI Their goal is to becom your research assist and streamlin how you organ share and stai up to date on the latest R&D Thi is realli cool becaus the flood of paper in machin learn is sheer overwhelm in recent month Zeta Alpha us neural embed base search and can give you the best recommend of research that match your interest and that you want to miss And what better wai than to just try it out So first I start off search for paper which is the blip paper And thi is realli cool becaus not onli do I get the paper I also get the GitHub code implement And I can directli see the impact on social media that thi paper ha Thi is much better than someth like Googl Scholar which would just give me a few link to the paper itself I can now save thi paper under a tag categori that just go to invent right now And I can us Zeta Alpha to find similar research Here go to limit my search to the last three month So I make sure that I miss anyth that ha recent been go on that I should know about when review thi paper Now I also like a bunch of those other paper So go to save them as well to the same categori Once I have a bunch of paper in my categori I can us again as Zeta recommend engin to give me more suggest paper to add to the same categori base on what I have alreadi in there And I can also share thi entir categori with my teammat becaus everyth Zeta Alpha doe is not onli for individu but also for team Thi is realli power and can dramat acceler your discoveri of new and relev research Now thi onli work for categori that you defin Once you interact with the search engin Zeta Alpha is go to be abl to give you a list a feed of recommend from archiv from confer from blog from GitHub and much more Thi save you a ton of time and let you stai up to date with whatev is happen If at all into ML research thi is hyper relev for you And I definit invit you to check it out Now thei do have a free tier but I got you a great deal If you go over there right now and us code Yannick get off a person assist subscript Again go to Zeta dash alphacom us code Yannick for off right now Thank again so much to Zeta Alpha for sponsor video And now get into it See ya Hello there todai look at blip bootstrap languag imag pre train for unifi vision languag understand and gener by Junan Li Dongshu Li Taim Xiong Stephen Hoi yeah it of Salesforc research So thi paper propos two thing One is a new architectur and I want to sai a new conglomer of exist thing So an arrang of modul for multitask pre train Thi model will take in an imag text pair and perform multipl task on it it ha multipl loss and therefor end up be abl to do multipl thing Now that be said thi is a pre train method So the idea is that for ani of these modul take them you recompos them downstream and you fine tune them on a task although thei do have some zero shot result So thi is on thing and thi could be realli cool if thi alon turn out to be success becaus it lead the path to a futur where we have much more dynam composit of model and where we would pre train these model with a lot of differ task in in on thing rather than pre train them on on just a singl task like languag model The other thing is a bootstrap method for the data and these two thing ar not necessarili disconnect although I do lament the fact that two thing in on paper a littl bit But a bootstrap method for these imag text data set that includ train caption and filter which mean that thi a part that learn to synthet gener data And then there is a part that learn to distinguish good from bad data And that allow them to collect lot and lot of data from the internet and filter out bad badli poorli label imag which there exist a lot on the internet and also allow them to augment the data set by label imag themselv So thi is also realli interest And it feed realli well back into their model becaus their model is uniqu capabl of do thi be the multitask model that it is So go to go through the architectur through the data set bootstrap method And keep in mind that I think you know if thi catch on thi could be there could be recip in here for futur research that lead us to a much more dynam world where we compos these modul much like we compos differ modul low level modul and deep learn we could compos these higher level modul and loss and do lot more multitask pre train mayb even dynam configur But dive in So vision languag pre train thei sai as recent recent been you know the hit for exampl if you think of someth like clip and not even pre train but there ar lot of architectur that do vision languag pre train mean thei take pair of imag and text So have like some sort of an imag and have like some sort of text that goe with it And try to come up with a system that connect the two in ani wai thei sai the major the exist method have two major limit So first of all the what thei call the model perspect thei sai thei ar either the exist method ar either encod base or an encod decod architectur So in an encod base setup what you would do is you would take in both of these thing and you would try to come up with probabl a number that repres how well thei fit togeth So ar thei good togeth or not Thi is the clip architectur essenti So in encod base model thei critic that encod base ar less straightforward to directli transfer to text gener task So not not simpl to take clip and actual make it produc someth Rememb if we have to if you have to produc an actual imag with clip we need to do thi diffus clip guid diffus or clip guid GAN VQ GAN So realli cumbersom to make clip gener an imag And probabl even more cumbersom to make it gener text becaus not train on that So thei critic on these method not easi to make them do gener task Wherea encod decod model have not been successfulli adopt for imag text retriev task So an encod decod model is where you would take the imag probabl and then make the produc the text So you train it as a languag model to autoregress produc the caption And realli neat for produc caption But you cannot necessarili do thi task up here veri easili with such a model You will you will be abl to do some thing but not necessarili success becaus the task is realli a differ task So both both approach for do thi current ar not ideal The other thing is the data perspect Thei critic that these model ar pre train on imag text pair that ar essenti scrape from the internet So collect it from the internet And thei sai noisi web text is suboptim for vision languag learn known for a long time that there is a trade off between scale of data and qualiti of data And ideal have both if howev if you scrape from the internet So sai you scrape websit and there is like some text and there is an imag somewher and the imag will have alt text And usual us as the label in these system So if you know in the HTML if you have an imag tag how how the browser know an imag you have the imag tag you have the sourc attribut which lead a URL usual that lead to the imag but then you also have an alt attribut And realli recommend that you put an alt properti to the point where framework and linter and so on thei will yell at you if you have it So what doe thi do Thi specif is for visual impair peopl for screen reader but also for bot to know what is in the imag So you put a descript there Howev a lot of peopl do that And I think it make it actual wors that linter and so on almost requir you to do it Becaus if you want to do it just go to put like some some dumb stuff there like imag or peopl do lot of search engin optim in there So sinc you know the search engin usual look at the imag itself but at the alt text thei try to come up with buzzwordi thing so that rank high in search result So not necessarili the best qualiti data And their bootstrap their bootstrap method right here is help in that of get higher qualiti data out of the internet So how do thei do thi The first thing thei propos is thi model the multimod mixtur of encod decod Thei sai it can oper either as a unimod encod or an imag ground text encod or an imag ground text decod So yeah go to look at these thing But I think here thei sai it can oper either as on or thi or that not like thi not like that exact same model can do thi just that thei put all of these model into on big model And then thei just us the part of the model that doe the particular thing So not necessarili super duper unifi is what I want to sai Yeah thei train the three the three sub part of their model with three object which also go to look at The second part is thi caption and filter Thi is what thi is what boost the data set qualiti Thei sai thei learn from noisi imag text pair by clean them by produc more and clean them Thei train a caption which whose goal is to produc synthet caption given web imag and a filter to remov noisi caption from both the origin web text and synthet text So the caption will get imag produc label for these imag or produc alt text And then the filter goe over both the gener on and the collect on and just filter out everyth that it deem to be qualit low standard Of cours thi need to be train on a high qualiti data set But these sort of bootstrap method seen a number of time in the recent past that thei actual work In fact thi model thi paper here seem to be a good accumul of zircon of recognit and good practic over the last few year And go to point those out as we go through their their contribut Here thei sai we show that the caption and the filter work togeth to achiev substanti perform improv which okai I know what substanti mean in these kind of task but I an improv Thei ar thei achiev state of the art perform in a wide rang of vision languag task And interestingli also thi is a properti of mayb synthet data gener thei show more divers caption yield larger gain Thi might also be a good lesson for peopl who want to go and appli these method Lastli thei sai next to have state of the art in downstream fine tune task thei also achiev zero shot perform when directli transfer our model to two video languag task So never thei were never train on video languag task never pre train never fine tune yet still thei have a good zero shot perform which is okai like if you understand imag then there ar go to be some video task that ar your that particularli good at right So dive into the model And alreadi shown you a diagram of the model thei quickli go through thi here thei have three part thei have actual well I want to sai four part to their model One part on is a visual transform a VIT as the imag encod So again thei take an imag and thei take a piec of text and now thei do stuff with it And the first part is thei encod the imag us a visual transform all thei do with the imag thei encod it us a VIT With the text thei do three differ thing The first thing is thei also just encod the text unimod So put the text through an encod And that with those two thing alreadi thei have essenti reproduc clip Except thei sai the same as BERT Yeah so reproduc clip with those two thing becaus now thei can set it up thi visual transform and the unimod encod thei can set it up as a similar metric So the unimod encod will give you some vector in an embed space the visual transform will give you some vector in an embed space you can set up a contrast loss to check whether these two thing go togeth and whether thei ar apart from sai ani other encod imag or text You can do thi via contrast learn you can do it via regular method But essenti thi is what come to known as encod onli model The second thing thei have is thi imag ground text encod So the imag ground text encod doe almost the same thing as the unimod text encod Howev it encod the text separ it encod the text while incorpor attent into the visual transform go to see how that goe in a second But essenti it produc a vector sai thi on And while produc that on the path as it produc that it incorpor inform from the visual transform So it will thi here is the output of the visual transform it will incorpor that at multipl layer here via cross attent into the process So thi here is realli a joint kind of encod of the text given the imag why call imag ground text encod What thi can do is you can build a classifi on top of thi like a binari classifi becaus it is a represent of the text that ha but that ha alreadi the inform of the imag insid of it So kind of a joint represent of the imag and the text So you can build a classifi for exampl whether or not the two thing go togeth again but you have to us a contrast loss you can in fact us a supervis loss and classifi and build a classifi The third thing is thi imag ground text decod Now again be imag ground that is a that is a long what is go on Some thing up here an imag ground text decod The imag ground text decod is much like the imag ground text encod in that it incorpor cell across attent Howev a text decod So what it will do is it will actual produc text So it will auto aggress produc the text while incorpor again inform via cross attent from the visual represent You can see that thei have a differ section on the pre train object These just map to these three part So the imag text contrast loss which is the loss for the first part there is the imag the imag text match loss which is the loss for the second part And again thi is just a binari classif task where the model us a linear layer head what thei call it an ITM an imag text text match head but a linear layer to predict whether an imag text pair is posit which mean match or neg unmatch given their multi model featur The special thing here is thei do have a hard neg mine strategi So thei go to the top part here thei go to the joint no sorri to thi joint encod to thi part and thei look which on ar the hard neg which mean that neg that have a high contrast similar and thei us those specif to train thi loss here The last loss is a languag model loss which is obvious relev for the third part Thi is a cross entropi loss that maxim the likelihood of the text in an autoregress manner If we put all of thi togeth we get thi model right here Again if we go through it the input data ar two thing the input data ar the imag down here and the piec of text here Again we know these go togeth becaus scrape them from the web So these two we know thei go togeth Thi is not an unsupervis train Thi is essenti supervis learn for two thing that we know go togeth The first thing is go to encod the imag through the imag encod the imag encod Thi is the imag represent Thi is just a bit thi is a visual transform I know I think thei freez it but thei mai start from a checkpoint All of thi is jointli train So all of these loss as I understand them ar jointli train So then we have the vision represent What we can do is we can put the text first of all through the text encod You can see we can append differ token right here to let the encod know what current do becaus we also have some paramet share go on So the text encod get the input text It will also comput an encod And then we have thi contrast loss between the two encod Thei need to be close for pair that we know go togeth and thei need to be far apart for other pair You can do someth like in batch neg or you can as we said mine hard neg from the contrast from thi part Well that make no sens You can mine contrast so you can mine hard neg for that part over here given thi part over here Which make me believ okai mayb I read close enough Mayb thei also just train on of the loss mayb for each batch becaus thei have to sampl differ for the thing It make too much of a differ whether thei train it realli all jointli jointli or alwai activ on of the three text pathwai Thi would be interest to figur out Yeah So the last thing the second thing thei do is thei give it to thi imag ground text encod Again thi get the text and a littl token to show go on It will encod and now you can see that it ha thi cross attent modul And the cross attent modul as it encod it incorpor inform that come from all the wai over here that come all the wai over here from the imag So the imag represent is part of the encod here which mean thi thing ha inform about both the text and the imag Now yeah of cours still a still not symmetr right We the joint encod is asymmetr in the sens that it is the text that is encod base on the imag and it allow them to onli comput the imag represent onc So thei onli need to do thi pathwai on the left here onc and then thei can reus that represent for all of the for all of the differ path in the text here Yeah you can see that on the left thi is the differ on the left here thi is skip the cross attent is skip We have cross attent just an encod of the text itself And here realli a joint encod which mean that thi thing here contain inform on both the imag and the text And we can perform ani sort of task that we want with thi joint encod In our case we simpli train it on a veri similar object as the contrast loss in that a binari classif it need to figur out whether or not the two thing actual go togeth or not The third thing again almost the same is thi decod the text decod same input except a littl decod token There is a differ in that thi is bi-direct the other two modul have bi-direct self attent becaus thei ar encod So thei get to us bi-direction Here we us causal self attent which essenti mean that in the text you onli get to attend thing So if you produc a particular token right here you onli get to attend to token that ar behind yourself Thi is a bit of a hack becaus otherwis we train these thing with batch or in parallel It is definit possibl to us bi-direct self attent as long as you cap as long as you mask whatev come next So you want to mask sort of the futur but within the past you could total us bi-direct self attent Again thi is just a hack to make train easier But becom come to be a popular hack So do it Again you can see cross attent come from the imag And here you can realli see that necessari right If I want to actual produc text I need some sort of inform of what I want to produc And so thi languag model loss here realli need the cross attent realli need the input from the imag So again thi come from here from the imag represent So there you have it an unholi concoct of mani differ thing in on And thi is all train jointli right And yeah excit about thi becaus I think not necessarili thi particular arrang like I have lot of stuff to critic or or like lot of choic here that ar kind of arbitrari Like why is thi asymmetri in you know I have the imag encod onc and I have cross attent into all the text encod Why not the other wai around Why we do imag gener task Why we do ani sort of mask model like mask languag model Thi could even be in the imag lot of stuff sai to critic But I think what thi thing show is that a good recip for the futur could be to combin lot of these differ method togeth combin lot of them into on big thing reus part intellig and then train them jointli We could even think of framework that do thi automat or that allow you to realli easili set thi up with a few line of code and it will figur out by itself like the framework would figur out itself how what it can compos and how how it could reus What you can also see right here is overshadow it a littl bit with my thing right here but color and the color indic share paramet which is also realli interest So you can see that essenti the text encod three separ encod but thei larg share paramet For exampl the feed forward paramet ar share the cross attent paramet all share except of cours not activ in thi encod The bi-direct self attent paramet ar share The causal self attent those on ar separ over here But if we had some sort of other autoregress autoregress modul thei would be share too So share whatev you could in these architectur and that reduc the overhead but also in their evalu realli help which I guess make sens Well I know if the task ar too distant you might get thi catastroph forget but in their case it doe help Ye which I can I can I could guess right by for exampl the bi-direct self attent right here sinc these two modul ar almost do the same task reason that thei would share paramet So gone through a whole lot of thing that thei sai down here Thei do reason through their choic a littl bit even though I think I think these choic thei ar either arbitrari or guid by experi you know just see what work better Thei do bring up some hypothes of what thei think you know why why do thing work and why do thing work Thei sai the text encod and decod share all paramet except for the self attent layer The reason is that the differ between the encod and decod task ar best captur by the self attent layer So essenti sai that whether you want to encod or decod that is mostli go to be differ in the attent layer not from the architectur perspect but from sort of the how the task is done perspect And that I I think necessarili you can sai thi right Like you necessarili sai the feed forward layer have a similar job in or have similar featur and perform similar function whether encod or decod I just think out of the box realli evid that we need to be support by evid So yeah but it seem to work well in empir evalu And so go to go to with them share the paramet but the reason ar more hypothes So the second part thei go into is thi cap field Again thi is a bit disconnect although it plai well into their model Here thei critic how these data set ar usual collect Thei sai alt text often do not accur describ the visual content of the imag that ar scrape from the web And why thei have a bootstrap method So what thei do is thei collect a data set from the internet And yeah well I find thi diagram here to be a littl bit complic So just go to make our own So thei thei have the internet go to thi is a globe with you know the line and so on So go to collect a big chunk of data of pair of imag and text imag and alt text from the web realli noisi And what go to do with thi stuff is go to train a first blip architectur or a first not how thei call it MED architectur multi someth someth whatev their model is on top just go to train that with thi noisi data And go to be our first iter model Now thi is realli noisi so far and so on But what go to do then is go to fine tune thi go to fine tune a filter and a caption So go to fine tune a filter and a caption on supervis data There exist some supervis data set And on of them I believ is the Cocoa data set Ye the Cocoa data set So thi step here we need supervis data and supervis data of imag text pair So human made caption for exist imag which a sort of a proxi for qualiti So of these thing we can be sure that the qualiti is rel high If we could find some sort of an autom wai to get realli high qualiti imag text pair data it necessarili need to be human label It just need to be high in qualiti So thei us that to train a filter and a caption Now what is the filter and the caption model Now these ar go to be fine tune version of their MED model For exampl the caption take in an imag and give you a caption a synthet caption Now thi is someth our model can do If we you know we just take two part So we take thi part and we take thi part right here Thi is now a caption model So the idea here the gener idea of blip of thi MED model is that we pre-train all of these thing togeth and we sub-select or we rearrang even the differ sub compon and then fine tune them on a downstream task And on easi wai is to take two compon simpli deactiv all other and let them run in infer mode So now we have a caption model The caption the filter model on the other hand veri similar but it take an imag and a piec of text both insid and it will output a score of whether the two thing go togeth or not Now thi of cours we can achiev in multipl wai but we can achiev thi in the probabl the most high qualiti wai by take the imag encod and take thi part right here that is specif train to jointli encod You might ask why we us why we us thi modul right here and then us thi contrast estim We could also do that definit but usual there ar alwai multipl wai of determin similar You can have sort of the two stack encod So here is the imag and here is the text You can have separ encod for them and then at the end determin whether thei go togeth And usual good if you want to do someth like a search index becaus you can pre-comput a lot of these thing You can pre-comput all the embed for the imag and then at infer time if you have a queri us text you want to search an imag via text you onli need to encod the text Wherea with a joint encod realli differ You need to input both into the encod and that will give you a score at the end And if you want to build a search engin like thi then for everi singl time you issu a queri what you need to do is you need to go through the whole dataset and encod the queri here togeth with all of the imag get the score for each on and then evalu that So you can see there is a trade-off The left side is wai friendlier comput wise if you have an exist dataset The right side is qualit higher becaus dure comput through these layer the two thing can alreadi attend to on anoth Wherea realli the onli interact here is the end over here So thi is qualit better estim of whether the two thing match or match And why go to have the filter here Sinc work sinc filter the dataset we can jointli encod the two thing anywai So go to fine tune that part to becom our filter So now we have a fine tune part on caption on filter What can we do now Well we can take our dataset thi thing right here and we can us the caption to produc anoth dataset by just take the imag So we just take the imag here We put them through the caption and we get anoth dataset So we get anoth dataset go to have the same imag right But go to have differ text So thi is a synthet dataset We can then join the two dataset togeth So join the two dataset and then we can put them both through the filter So go to put them both through the filter and the filter will simpli filter out ani imag text pair that is not adequ which mean that it will filter out ani imag text pair which match well togeth given the fine tune of the filter on the supervis or high qualiti dataset So then we end up with a dataset of and we can restrict it like to onli have on caption for each imag or someth like thi And we end up with a dataset of imag text pair which is larg becaus augment it with synthet data but also is of high qualiti becaus we have done the filter Now that all of thi be said again thi highli reli on the qualiti of the dataset that we fine tune on and of the divers of that dataset as well becaus you can also imagin if that dataset contain much of the domain that look at then your filter will learn to essenti down rank everyth becaus it sai well my dataset sai these two thing go well togeth becaus I actual have just no data in that region So a bit of danger in do thi You realli need to pai attent at what dataset fine tune but thi is how you bootstrap a good dataset So you can see go from here to here and you can think of multipl thing Again I think thi paper is less about the particular method thei choos And I think more about you know what could be recip for the futur And I think in the recent time seen a lot of synthet data gener first of all be realli help seen thi in a number of reinforc learn applic a number of even NLP applic So synthet data is realli realli pick up I want to sai with advanc in SIM to real and so on And then also thi approach of filter Thi ha come up more and more in recent year where gener model ar pair with discrimin model that either re-rank their output or filter their output for qualiti Thi seem to be a veri good recip for achiev gener task in gener Not onli train a gener but train a ranker or filter on top of that pretti computation effici easi to implement And yeah I think a good recip for the futur And on can think of variou wai here to improv thi like to do thi bootstrap multipl time Yeah to collect the supervis data set in a differ manner and so on I think a lot of possibl here that ar not yet explor which I find to be pretti pretti cool So essenti all Yeah Okai no I wa actual wrong here You can see the filter is actual fine tune on both of the object to learn whether a text match the imag So thi both the contrast and the singl classifi loss Although I do think I do think the filter like what thei actual pai attent to at the end is go to be thi thing right here is go to be the classif head But I guess it hurt to us both loss as you fine tune it And sinc all paramet ar share essenti you realli have you realli have you can like easi to try and not too much of an overhead So the method Again thei have thi concoct of modul that thei all pre-train jointli with their respect loss And then on the other hand thei have thi bootstrap method where thei can directli us their model right the wai these integr these two Sinc thei have a model that can do all of these differ thing thei can fine tune that model to becom a filter or to becom a caption And the same thing hold for the result downstream Here thei have some exampl by the wai of gener And so the bottom text is alwai a gener on The top text is on from the data set Anyth red is filter out by the filter Anyth green is accept by the filter Yeah so thei thei also discuss a littl bit of the danger of do thi of train the filter and the caption from the same pre-train state on the same data set which is that like there is some go to be some confirm bia in that the filter will uprank thing that the caption produc becaus essenti learn from the same data why thei share thei fine tune them separ to combat thi a littl bit But I still think that go to have some of that in there definit But you know thi is you know thi is a real data from bridg near my hous which might be true right but not veri descript and the filter realiz it Yet a flock of bird fly over a lake at sunset pretti descript Another interest thing is that thei us nucleu sampl here which is a common strategi but thei do find that us nucleu sampl lead to better perform and that becaus it gener more divers and surpris caption which contain more new inform that the model could benefit from Thei compar thi to beam search and beam search essenti goe for the highest likelihood sampl It tend to gener safe caption that ar common in the data set henc offer less extra knowledg I think also realli cool recognit right here that if we sampl thing from gener model we might have differ goal and therefor it might not alwai be good to like it might be good to have an object or a sampl method that encourag divers alreadi seen thi in alpha code and my question there wa alreadi a littl bit do we even have the correct train procedur for thi becaus we train maximum likelihood or do we have the correct sampl procedur for thi All of these ar interest question and I think thi kind of research valid that not all the same like depend on what we want to do our train and sampl procedur need to adjust I want to dive too deep into the result Thei ar outperform other thing by some margin I necessarili agre that thei outperform thing so heavili as thei advertis but research current Again thei allud to the fact that thei share paramet here and why that is Thei sai share all the layer except for the self-attent lead to better perform compar to not share the part I believ right Total you share number go up good But then thei sai if the share attent layer ar share the perform would degrad to the conflict between the encod and the decod task and thi I think yeah thi stuff need evid becaus I mean yeah fine with just go with the number Here you can see the variou wai thei combin the thing for exampl for visual question answer thei first encod the imag then thei feed that to the text encod then thei feed that to the decod So you can see you can not onli sub-select modul but you can rearrang them right Becaus you fine-tun you can adjust the paramet So thi connect alreadi exist in the previou model but thi connect So you can sort of rearrang and recombin these modul to do variou thing You can see here we have two imag or a doubl imag encod or I guess the imag encod just get two sampl And then we also have two on a duplic of these cross attent modul and then we output that into a newli train merg layer So thi is the excit part right here And I feel realli I want to necessarili go into thi becaus we might go into thi in the interview but I feel a futur where we have framework code framework where thi kind of stuff could be support in an automat fashion where I have to you know go and realli hand defin exactli how I want these thing combin but I could have a more high level descript languag that allow me to do thi whole pre-train arrang and thi recombin for downstream fine-tun realli excit All right go to leav it at that I hope you had a good overview If you want to dive into the result you know feel free lot of tabl in here a realli thorough evalu which is realli cool becaus it lend a lot of credenc to their method And with that let me know what you think 
Naive Bayes - Natural Language Processing With Python and NLTK p.13
go on everybodi Welcom to the NLTK with Python for Natur Languag process tutori video In thi video go to be build on the last video and actual complet our Naiv Bay algorithm to categor thing as either posit sentiment or neg sentiment So first of all the process usual goe as follow go to have when you have a data set like thi a label data set go to have two thing go to have a train set is equal to someth and go to have a test set equal to someth You have to split them up and you want them to be the same data becaus that creat a lot of bia So the train set go to sai is featur set and thi will be the first featur set The test set will be featur set and that will be and on So the first train against and the second test against So when we train we take the data we feed it to the algorithm we sai here ar the word here is the word in the top list of all word These ar how mani time these most common word appear in neg review how mani time thei appear in posit review If thei appear significantli more in neg review that probabl mean that word is veri import to a neg review and vice versa for posit review So with the train set how we train it and then with the test set we feed through the featur set and we tell the machin what categori thei ar We ask the machin to tell us that categori The machin tell us the categori and then we compar it to the known categori that we have and we see how accur is the machin Doe it get it right More often than it and if not well screw up Anywai that So go to be us thi algorithm call the Naiv Bay algorithm So thi is a type of classifi go to us the actual just the Bay algorithm which work on veri strong independ assumpt for each featur and actual where it get it naiv name from basic Sometim peopl call it the stupid Bay becaus it not realli I mean like logic we can think through thi algorithm and we can see all of the flaw that ar contain within it but we can actual find that we have pretti good result with it and becaus the algorithm is so basic a realli short algorithm and it requir a lot of comput we can scale thi algorithm to like massiv proport becaus it take much process So the Bay algorithm is pretti simpl go to have basic the posterior and just like you know the likelihood equal and go to be prior occurr time likelihood and then all of that is go to be divid by the current evid and that will be the likelihood of someth to be posit and then you would do the same thing for someth to be neg And basic all done So a quick algorithm If you want to know more about that algorithm I highli suggest you just go Googl it and read like I know the Wikipedia page and mayb some articl on it and figur out more how it work but it realli is actual quit the basic algorithm as far as learn algorithm ar concern But again not necessarili the best algorithm it is just scalabl and realli easi to understand So onc gotten to thi point now readi to actual creat the classifi that we want to us becaus got a train set got a test set and now readi So first of all we have NLTK alreadi import so just continu us that and go to sai our classifi classifi is go to equal NLTK and go to us the naiv Bay classifi algorithm go to dot train and go to train against our train set just copi that and past it And now first of all what we can do is we can do a coupl of thing First of all we can check accuraci right awai So train it now we can test it So we can just sai print and sai naiv Bay algo accuraci colon comma and then now we can us NLTK dot classifi dot accuraci and we can run that and what you the paramet that you pass through here ar go to be what classifi ar you us and the set that you want to test against test set and then just multipli the answer by so in a percentag and even denot thi as accuraci percent Okai so then so we can do that but also add on more thing to it go to sai classifi dot show underscor most underscor inform underscor featur and show the most inform featur and go ahead and save and run that Let me just check to make sure I need major syntax error I think so so save and run it and talk about thi as wait So thi will print we need to print thi anymor stop that from happen So go to hog up a bunch of space real quick but anywai now it So thi train it thi run the accuraci and what thi is go to do is tell us like the most popular word on both side and whether or not thei tend to be posit or tend to be neg So it look like probabl updat now Right so first of all our Naiv Bay algorithm accuraci percent given here is Now I will add that depend on your sampl and actual go check and make sure we screw anyth up Thi is actual a higher number than I normal see with the Bay algorithm You might find that your is as low as like or mayb even lower Sometim seen it hit and stuff while I wa set thi up So just understand that thi is highli volatil and be like asham if you hit You probabl do anyth wrong Anywai so now we can look down here and we can see okai what ar the most common word I believ is on of them actual kind of shock I want to go check and make sure It is a littl eeri that we us here but that make ani differ at all Anywai so we can see here on of the more import the most import on is thi word slip and it appear time more often than it doe in a neg review is a neg on try to think of like why would be on Anywai then got symbol seamless idiot make a lot of sens Thi is an unimagin Okai these on make a lot of sens Idiotic appear time more often than it doe in a posit review Unimagin same thing and so on So we can actual end up us these later on down the road to chang the algorithm slightli but as you can see none of these contain grammar and why grammar just matter We have such a larg data set We have most common word that grammar is of cours not on here becaus it an inform featur So we can actual kind of us that to our advantag and sinc talk rerun thi on more time befor I cut thi tutori off be realli surpris if we get anoth high return so hopefulli get someth a littl more reason becaus just uncommon that go to hit that much accuraci but anywai so just wait for thi and then talk about thi So ye not like I said not as accur as is unlik Anywai talk about how to improv the accuraci of thi or at least improv the reliabl becaus on of those two is realli import nice to have both but we definit want to have reliabl over random high accuraci point So anywai it for thi tutori If you have ani question or comment pleas feel free to leav them below Otherwis as alwai thank for watch Thank for all the support and subscript and until 
